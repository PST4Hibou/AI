{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "YOLO11 Tutorial",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train and Predict Drones"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Abstract\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "### Introduction\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üß† 1. Setup\n",
    "\n",
    "pip install `ultralytics` and [dependencies](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) and check software and hardware.\n",
    "\n",
    "[![PyPI - Version](https://img.shields.io/pypi/v/ultralytics?logo=pypi&logoColor=white)](https://pypi.org/project/ultralytics/) [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/ultralytics?logo=python&logoColor=gold)](https://pypi.org/project/ultralytics/)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.0 Importing Libraries"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install ultralytics markdown rich wrapt pandas mlflow huggingface_hub opencv-python -q",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from IPython.display import display, Image as IPyImage\n",
    "from mlflow import log_param, log_metric, log_artifact\n",
    "from ultralytics import YOLO, settings\n",
    "from collections import defaultdict\n",
    "from huggingface_hub import login\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import ultralytics\n",
    "import numpy as np\n",
    "import datetime\n",
    "import hashlib\n",
    "import random\n",
    "import mlflow\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "\n",
    "ultralytics.checks()"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.221 üöÄ Python-3.13.9 torch-2.7.0+cu126 CPU (11th Gen Intel Core i7-1165G7 @ 2.80GHz)\n",
      "Setup complete ‚úÖ (8 CPUs, 23.3 GB RAM, 428.4/464.2 GB disk)\n"
     ]
    }
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T14:11:41.348794Z",
     "start_time": "2025-11-27T14:11:41.171480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATASET_NAME_DIR = './datasets/new_dataset/'\n",
    "TRAINING_DATASET_DIRECTORY = DATASET_NAME_DIR + 'training/'\n",
    "VALIDATION_DATASET_DIRECTORY = DATASET_NAME_DIR + 'valid/'\n",
    "MODELS_DIRECTORY = './models/'\n",
    "RUNS_DIRECTORY = \"./runs\""
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "567413b693054e89941a85fed870834f"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    ### 1.1 Global Definitions"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# YOLO settings\n",
    "settings.update({\"mlflow\": True})\n",
    "\n",
    "# MLflow settings\n",
    "mlflow.set_tracking_uri(\"https://hibou.pythonanywhere.com/\")\n",
    "mlflow.set_experiment(experiment_id=\"869246232133219410\")\n",
    "\n",
    "# Hugging Face login\n",
    "login()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# üìÇ 2. Dataset"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.1 Acquire Dataset\n",
    "\n",
    "Download dataset from hugging face."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.2 Data Structure Check\n",
    "\n",
    "##### Directory path validation"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_image_names = [f for f in os.listdir(TRAINING_DATASET_DIRECTORY) if f.endswith(\".jpg\") or f.endswith(\".JPEG\")]\n",
    "val_image_names = [f for f in os.listdir(VALIDATION_DATASET_DIRECTORY) if f.endswith(\".jpg\") or f.endswith(\".JPEG\")]\n",
    "\n",
    "print(f\"Number of images in training folder: {len(train_image_names)}\")\n",
    "print(f\"Number of images in valid folder: {len(val_image_names)}\")\n",
    "\n",
    "train_txt_names = [f for f in os.listdir(TRAINING_DATASET_DIRECTORY) if f.endswith(\".txt\")]\n",
    "val_txt_names = [f for f in os.listdir(VALIDATION_DATASET_DIRECTORY) if f.endswith(\".txt\")]\n",
    "\n",
    "print(f\"\\nNumber of TXT files in train labels folder: {len(train_txt_names)}\")\n",
    "print(f\"Number of TXT files in val labels folder: {len(val_txt_names)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Duplicate files\n",
    "\n",
    "Check for duplicate images and labels."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "paths_to_check = [\n",
    "    [os.path.join(TRAINING_DATASET_DIRECTORY, f) for f in train_image_names],\n",
    "    [os.path.join(VALIDATION_DATASET_DIRECTORY, f) for f in val_image_names],\n",
    "    [os.path.join(TRAINING_DATASET_DIRECTORY, f) for f in train_txt_names],\n",
    "    [os.path.join(VALIDATION_DATASET_DIRECTORY, f) for f in val_txt_names],\n",
    "]\n",
    "\n",
    "\n",
    "def sha3_file(path, chunk_size=8192):\n",
    "    hash_sha3 = hashlib.sha3_256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
    "            hash_sha3.update(chunk)\n",
    "    return hash_sha3.hexdigest()\n",
    "\n",
    "\n",
    "duplicates_array = []\n",
    "\n",
    "for path in paths_to_check:\n",
    "\n",
    "    # Build list of (hash, path)\n",
    "    pairs = [\n",
    "        (sha3_file(f), f)\n",
    "        for f in path\n",
    "    ]\n",
    "\n",
    "    # Group paths by hash\n",
    "    hash_map = defaultdict(list)\n",
    "    for h, p in pairs:\n",
    "        hash_map[h].append(p)\n",
    "\n",
    "    # Extract only duplicates\n",
    "    duplicates = {h: paths for h, paths in hash_map.items() if len(paths) > 1}\n",
    "    duplicates_array.append(duplicates)\n",
    "\n",
    "    # Print results\n",
    "    if duplicates:\n",
    "        print(\"Duplicate files found:\")\n",
    "        for h, paths in duplicates.items():\n",
    "            print(f\"Hash: {h}\")\n",
    "            for p in paths:\n",
    "                print(f\"  - {p}\")\n",
    "    else:\n",
    "        print(f\"No duplicate files found in the scanned directories.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### ‚ö†Ô∏èDelete duplicate files ‚ö†Ô∏è\n",
    "\n",
    "Delete for duplicate images and labels."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "try:\n",
    "    if input(\"Files are going to be deleted. Write yes to continue.\") == 'yes':\n",
    "        for duplicates in duplicates_array:\n",
    "            for paths in duplicates.values():\n",
    "                for path in paths:\n",
    "                    os.remove(path)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Deletion aborted.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Check for orphelins txt file"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "paths_to_check = [\n",
    "    [os.path.join(TRAINING_DATASET_DIRECTORY, f) for f in train_txt_names],\n",
    "    [os.path.join(VALIDATION_DATASET_DIRECTORY, f) for f in val_txt_names],\n",
    "]\n",
    "\n",
    "orphelins_label = []\n",
    "\n",
    "for path_list in paths_to_check:\n",
    "    for p in path_list:\n",
    "\n",
    "        root, _ = os.path.splitext(p)\n",
    "\n",
    "        candidates = [\n",
    "            root + \".jpg\",\n",
    "            root + \".JPG\",\n",
    "            root + \".jpeg\",\n",
    "            root + \".JPEG\",\n",
    "            root + \".png\",\n",
    "            root + \".PNG\",\n",
    "        ]\n",
    "\n",
    "        if not any(os.path.isfile(c) for c in candidates):\n",
    "            orphelins_label.append(p)\n",
    "            print(f\"TXT file {p} does not belong to any image.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### ‚ö†Ô∏èDelete orphelin labels files ‚ö†Ô∏è\n",
    "\n",
    "Delete labels that don't belong to any images."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "try:\n",
    "    if input(\"Files are going to be deleted. Write yes to continue.\") == 'yes':\n",
    "        for orphelin_path in orphelins_label:\n",
    "            os.remove(orphelin_path)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Deletion aborted.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Rename files\n",
    "\n",
    "Rename images and labels from 0 to n images."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "TRAIN = TRAINING_DATASET_DIRECTORY\n",
    "VAL = VALIDATION_DATASET_DIRECTORY\n",
    "\n",
    "jpeg_exts = [\".jpg\", \".jpeg\", \".JPG\", \".JPEG\"]\n",
    "\n",
    "tmp = os.path.join(DATASET_NAME_DIR, \"tmp\")\n",
    "\n",
    "\n",
    "def numeric_key(name):\n",
    "    nums = re.findall(r'\\d+', name)\n",
    "    return int(nums[0]) if nums else float('inf')\n",
    "\n",
    "\n",
    "def collect_images(folder):\n",
    "    return sorted(\n",
    "        [f for f in os.listdir(folder) if f.endswith(tuple(jpeg_exts))],\n",
    "        key=numeric_key\n",
    "    )\n",
    "\n",
    "\n",
    "def process(folder, start_i, out_dir):\n",
    "    files = collect_images(folder)\n",
    "\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.mkdir(out_dir)\n",
    "\n",
    "    for f in files:\n",
    "        root, ext = os.path.splitext(f)\n",
    "\n",
    "        # find real existing image\n",
    "        image_path = None\n",
    "        for e in jpeg_exts:\n",
    "            p = os.path.join(folder, root + e)\n",
    "            if os.path.isfile(p):\n",
    "                image_path = p\n",
    "                break\n",
    "\n",
    "        if image_path is None:\n",
    "            print(\"Missing image for:\", f)\n",
    "            continue\n",
    "\n",
    "        label_path = os.path.join(folder, root + \".txt\")\n",
    "        if not os.path.isfile(label_path):\n",
    "            os.rename(image_path, os.path.join(out_dir, f\"{start_i}.jpg\"))\n",
    "            start_i += 1\n",
    "            continue\n",
    "\n",
    "        new_image = os.path.join(out_dir, f\"{start_i}.jpg\")\n",
    "        new_label = os.path.join(out_dir, f\"{start_i}.txt\")\n",
    "\n",
    "        # Move safely\n",
    "        os.rename(image_path, new_image)\n",
    "        os.rename(label_path, new_label)\n",
    "\n",
    "        start_i += 1\n",
    "\n",
    "    return start_i\n",
    "\n",
    "\n",
    "# === RUN ===\n",
    "\n",
    "i = 0\n",
    "i = process(TRAIN, i, tmp + '_train')\n",
    "i = process(VAL, i, tmp + '_valid')\n",
    "\n",
    "print(f\"DONE ‚Äî All files renamed safely into {DATASET_NAME_DIR}tmp_NAME\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Set labels"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install torch==2.7.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126 -q\n",
    "!pip install sam3 scikit-learn requests ipywidgets -q"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sam3.model.sam3_image_processor import Sam3Processor\n",
    "from sam3.visualization_utils import plot_results\n",
    "from sam3 import build_sam3_image_model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bpe_name = \"bpe_simple_vocab_16e6.txt.gz\"",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Load model"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = build_sam3_image_model(bpe_path=os.path.join(MODELS_DIRECTORY, bpe_name))\n",
    "processor = Sam3Processor(model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image_path = f\"datasets/new_dataset/training/633.jpg\"\n",
    "prompt = \"drone\"\n",
    "confidence_threshold = 0.5\n",
    "\n",
    "image = Image.open(image_path)\n",
    "width, height = image.size\n",
    "processor = Sam3Processor(model, confidence_threshold=confidence_threshold)\n",
    "inference_state = processor.set_image(image)\n",
    "\n",
    "processor.reset_all_prompts(inference_state)\n",
    "inference_state = processor.set_text_prompt(state=inference_state, prompt=prompt)\n",
    "img0 = Image.open(image_path)\n",
    "plot_results(img0, inference_state)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Settings"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "labelize_overwrite_label: bool = True\n",
    "labelize_prompt: str = 'drone'\n",
    "labelize_confidence_threshold: float = 0.62\n",
    "labelize_drone_class = 0\n",
    "labelize_output_label_file_directory = os.path.join(DATASET_NAME_DIR, 'new_labels')\n",
    "labelize_output_label_file_errors = os.path.join(labelize_output_label_file_directory, 'errors.txt')\n",
    "labelize_output_label_file_success = os.path.join(labelize_output_label_file_directory, 'success.txt')\n",
    "\n",
    "paths_to_labelize = [\n",
    "    [os.path.join(TRAINING_DATASET_DIRECTORY, f) for f in train_image_names],\n",
    "    [os.path.join(VALIDATION_DATASET_DIRECTORY, f) for f in val_image_names],\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "os.makedirs(labelize_output_label_file_directory, exist_ok=True)\n",
    "\n",
    "image_failed_labelize = []\n",
    "\n",
    "for paths in paths_to_labelize:\n",
    "    for image_path in paths:\n",
    "        root, ext = os.path.splitext(image_path)\n",
    "        root_label_name = root.split(\"/\")[-1]\n",
    "        current_label_path = root + \".txt\"\n",
    "        saved_label_path = os.path.join(labelize_output_label_file_directory, root_label_name) + \".txt\"\n",
    "\n",
    "        if os.path.isfile(current_label_path) and not labelize_overwrite_label:\n",
    "            continue\n",
    "\n",
    "        if os.path.isfile(saved_label_path) and not labelize_overwrite_label:\n",
    "            continue\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(image_path)\n",
    "        if image.mode != \"RGB\":\n",
    "            with open(labelize_output_label_file_errors, \"a\") as f:\n",
    "                image_failed_labelize.append(image_path)\n",
    "                f.write(f\"{image_path} is not RGB.\\n\")\n",
    "            continue\n",
    "        width, height = image.size\n",
    "\n",
    "        # Run SAM3 processor\n",
    "        processor = Sam3Processor(model, confidence_threshold=labelize_confidence_threshold)\n",
    "        inference_state = processor.set_image(image)\n",
    "        processor.reset_all_prompts(inference_state)\n",
    "        inference_state = processor.set_text_prompt(state=inference_state, prompt=labelize_prompt)\n",
    "\n",
    "        # --- Save YOLO labels ---\n",
    "        bboxes = inference_state.get(\"boxes\", None)  # fixed\n",
    "\n",
    "        if bboxes is None or len(bboxes) == 0:\n",
    "            # print(f\"No detections for {image_path}\")\n",
    "            image_failed_labelize.append(image_path)\n",
    "            with open(labelize_output_label_file_errors, \"a\") as f:\n",
    "                f.write(f\"No detections for {image_path}\\n\")\n",
    "            continue\n",
    "\n",
    "        yolo_lines = []\n",
    "        for box in bboxes:\n",
    "            x_min, y_min, x_max, y_max = box  # SAM3 gives XYXY\n",
    "\n",
    "            # Convert to YOLO format\n",
    "            bbox_width = x_max - x_min\n",
    "            bbox_height = y_max - y_min\n",
    "\n",
    "            cx = (x_min + bbox_width / 2) / width\n",
    "            cy = (y_min + bbox_height / 2) / height\n",
    "\n",
    "            nw = bbox_width / width\n",
    "            nh = bbox_height / height\n",
    "\n",
    "            yolo_lines.append(\n",
    "                f\"{labelize_drone_class} {cx:.6f} {cy:.6f} {nw:.6f} {nh:.6f}\"\n",
    "            )\n",
    "\n",
    "        with open(saved_label_path, \"w\") as f:\n",
    "            f.write(\"\\n\".join(yolo_lines))\n",
    "        with open(labelize_output_label_file_success, \"a\") as f:\n",
    "            f.write(f\"Saved YOLO labels: {saved_label_path}\\n\")\n",
    "\n",
    "    with open(labelize_output_label_file_success, 'r') as fp:\n",
    "        nb_lines = len(fp.readlines())\n",
    "        print(f\"Successfully procedded files: {nb_lines} out of {len(paths)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "try:\n",
    "    if input(\"Files are going to be deleted. Write yes to continue.\") == 'yes':\n",
    "        for image_path in image_failed_labelize:\n",
    "            os.remove(image_path)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Deletion aborted.\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.3 Check\n",
    "\n",
    "Load N random images and show rectangle round the drone."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_image_with_boxes(image_path, label_path):\n",
    "    \"\"\"Returns a PIL image with YOLO bounding boxes drawn.\"\"\"\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    w, h = img.size\n",
    "\n",
    "    if os.path.exists(label_path):\n",
    "        with open(label_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            cls, xc, yc, bw, bh = map(float, line.split())\n",
    "\n",
    "            # YOLO normalized ‚Üí pixel coordinates\n",
    "            x_center = xc * w\n",
    "            y_center = yc * h\n",
    "            box_width = bw * w\n",
    "            box_height = bh * h\n",
    "\n",
    "            x1 = x_center - box_width / 2\n",
    "            y1 = y_center - box_height / 2\n",
    "            x2 = x_center + box_width / 2\n",
    "            y2 = y_center + box_height / 2\n",
    "\n",
    "            draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=3)\n",
    "            draw.text((x1, y1), f\"{int(cls)}\", fill=\"red\")\n",
    "\n",
    "    return img"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "N = 48  # number of random samples\n",
    "cols = 4\n",
    "\n",
    "# list all images\n",
    "image_paths = [p for p in os.listdir(TRAINING_DATASET_DIRECTORY) if p.lower().endswith((\".jpg\", \".png\"))]\n",
    "sampled_images = random.sample(image_paths, min(N, len(image_paths)))\n",
    "\n",
    "# -------- DISPLAY GRID -------- #\n",
    "\n",
    "rows = math.ceil(len(sampled_images) / cols)\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(cols * 3, rows * 2))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, img_name in zip(axes, sampled_images):\n",
    "    img_path = os.path.join(TRAINING_DATASET_DIRECTORY, img_name)\n",
    "    label_path = os.path.join(TRAINING_DATASET_DIRECTORY, img_name.rsplit(\".\", 1)[0] + \".txt\")\n",
    "\n",
    "    img = load_image_with_boxes(img_path, label_path)\n",
    "\n",
    "    ax.imshow(img)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(img_name, fontsize=8)\n",
    "\n",
    "# turn off unused axes\n",
    "for ax in axes[len(sampled_images):]:\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ‚öôÔ∏è 3. Training\n",
    "\n",
    "Purpose: Handle model setup and training configuration."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.1 Model Selection"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "YOLO_MODELS = {\n",
    "    \"nano\": \"yolo11n.pt\",\n",
    "    \"small\": \"yolo11s.pt\",\n",
    "    \"medium\": \"yolo11m.pt\",\n",
    "    \"large\": \"yolo11l.pt\",\n",
    "    \"xlarge\": \"yolo11x.pt\",\n",
    "}\n",
    "\n",
    "selected_size = \"nano\"\n",
    "model = YOLO(MODELS_DIRECTORY + YOLO_MODELS[selected_size])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.2 Training Configuration\n",
    "Define hyperparameters (epochs, batch size, image size)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T14:17:06.778239Z",
     "start_time": "2025-11-27T14:17:06.747544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import yaml\n",
    "\n",
    "data_yaml = dict(\n",
    "    train=os.path.join('../../', TRAINING_DATASET_DIRECTORY),\n",
    "    val=os.path.join('../../', VALIDATION_DATASET_DIRECTORY),\n",
    "    nc=1,\n",
    "    names=['drone']\n",
    ")\n",
    "\n",
    "data_config_path = os.path.join(DATASET_NAME_DIR, 'data.yaml')\n",
    "\n",
    "with open(data_config_path, 'w') as outfile:\n",
    "    yaml.dump(data_yaml, outfile, default_flow_style=True)\n",
    "\n",
    "%cat \"$data_config_path\""
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TRAINING_DATASET_DIRECTORY' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01myaml\u001B[39;00m\n\u001B[32m      3\u001B[39m data_yaml = \u001B[38;5;28mdict\u001B[39m(\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m     train=os.path.join(\u001B[33m'\u001B[39m\u001B[33m../../\u001B[39m\u001B[33m'\u001B[39m, \u001B[43mTRAINING_DATASET_DIRECTORY\u001B[49m),\n\u001B[32m      5\u001B[39m     val=os.path.join(\u001B[33m'\u001B[39m\u001B[33m../../\u001B[39m\u001B[33m'\u001B[39m, VALIDATION_DATASET_DIRECTORY),\n\u001B[32m      6\u001B[39m     nc=\u001B[32m1\u001B[39m,\n\u001B[32m      7\u001B[39m     names=[\u001B[33m'\u001B[39m\u001B[33mdrone\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m      8\u001B[39m )\n\u001B[32m     10\u001B[39m data_config_path = os.path.join(DATASET_NAME_DIR, \u001B[33m'\u001B[39m\u001B[33mdata.yaml\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m     12\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(data_config_path, \u001B[33m'\u001B[39m\u001B[33mw\u001B[39m\u001B[33m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m outfile:\n",
      "\u001B[31mNameError\u001B[39m: name 'TRAINING_DATASET_DIRECTORY' is not defined"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_config = {\n",
    "    'data': data_config_path,\n",
    "    'epochs': 50,\n",
    "    'imgsz': 640,\n",
    "    'batch': 16,\n",
    "    'lr0': 0.001,\n",
    "    'optimizer': 'SGD',\n",
    "    'project': RUNS_DIRECTORY,\n",
    "    'name': \"computer_vision\"\n",
    "}\n",
    "\n",
    "train_config"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Parameter breakdown:**\n",
    "\n",
    "- **train**: Executes the YOLOv11x training pipeline.\n",
    "- **model**=yolov11x.pt: Uses pre-trained YOLOv11x weights as initialization.\n",
    "- **data**=/content/data.yaml: Specifies the dataset configuration file.\n",
    "- **imgsz**=640: Sets input resolution to enhance small-object detection.\n",
    "- **lr0**=0.001: Sets the learing rate for each training step.\n",
    "- **epochs**=32: Defines the number of training cycles over the dataset.\n",
    "- **batch**=16: Sets the batch size for each training step.\n",
    "- **device**=0: Allocates GPU device 0 for training.\n",
    "- **optimizer**=AdamW: As default, AdamW optimization algorithm utilized."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.3 Run Training"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# results = model.train(**train_config)\n",
    "# plot_results(os.path.join(RUNS_DIRECTORY, \"/train/results.csv\"))\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "\n",
    "    # Log model + training configuration\n",
    "    log_param(\"model_size\", selected_size)\n",
    "\n",
    "    # Log each training config parameter individually (better for MLflow UI)\n",
    "    for k, v in train_config.items():\n",
    "        log_param(k, v)\n",
    "\n",
    "    # Train YOLOv11\n",
    "    results = model.train(**train_config)\n",
    "\n",
    "    # Log YOLO training metrics (loss, mAP, P, R, etc.)\n",
    "    for key, value in results.results_dict.items():\n",
    "        mlflow.log_metric(key, float(value))\n",
    "\n",
    "    # Log training artifacts (weights, plots, results.csv)\n",
    "    # Path YOLO uses for this run:\n",
    "    run_path = os.path.join(train_config[\"project\"], train_config[\"name\"])\n",
    "\n",
    "    # Log all important artifacts\n",
    "    artifacts_to_log = [\n",
    "        \"weights/best.pt\",\n",
    "        \"weights/last.pt\",\n",
    "        \"results.csv\",\n",
    "        \"results.png\",\n",
    "        \"confusion_matrix.png\",\n",
    "        \"F1_curve.png\",\n",
    "        \"PR_curve.png\",\n",
    "        \"P_curve.png\",\n",
    "        \"R_curve.png\"\n",
    "    ]\n",
    "\n",
    "    for fname in artifacts_to_log:\n",
    "        full_path = os.path.join(run_path, fname)\n",
    "        if os.path.exists(full_path):\n",
    "            mlflow.log_artifact(full_path)\n",
    "\n",
    "    # Log full run directory (optional but useful)\n",
    "    mlflow.log_artifact(run_path)\n",
    "\n",
    "    print(\"Logged to external MLflow run:\", run.info.run_id)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.4 Result of training"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "display(IPyImage(filename=os.path.join(RUNS_DIRECTORY, \"/train/results.png\")))",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results_paths = []\n",
    "\n",
    "for root, _, files in os.walk(os.path.join(RUNS_DIRECTORY, \"train\")):\n",
    "    for f in files:\n",
    "        if f.lower().endswith((\".jpg\")):\n",
    "            results_paths.append(os.path.join(root, f))\n",
    "results_paths = sorted(results_paths)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%matplotlib inline\n",
    "\n",
    "for path in results_paths:\n",
    "    image = np.array(Image.open(path))\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.5 Evaluate Model\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "metrics = model.val()\n",
    "metrics"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.6 Compare with previous trains"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "saved_train_path = \"./saved_trains\"\n",
    "\n",
    "# List all folders in the saved_train_path\n",
    "folders_results = [dir for dir in os.listdir(saved_train_path) if os.path.isdir(os.path.join(saved_train_path, dir))]\n",
    "\n",
    "# Convert folder names to datetime objects for sorting\n",
    "folders_results_sorted = sorted(\n",
    "    folders_results,\n",
    "    key=lambda x: datetime.strptime(x, \"%d-%m-%Y_%H:%M:%S\"),\n",
    "    reverse=True  # latest first\n",
    ")\n",
    "\n",
    "# Take the last 2 trainings by date\n",
    "last_two_folders = folders_results_sorted[:2]\n",
    "\n",
    "# Dictionary to store the last row of each folder\n",
    "results_data = {}\n",
    "\n",
    "for folder in last_two_folders:\n",
    "    path = os.path.join(saved_train_path, folder)\n",
    "    results_csv = os.path.join(path, \"results.csv\")\n",
    "\n",
    "    if os.path.exists(results_csv):\n",
    "        df = pd.read_csv(results_csv)\n",
    "        # Get the last row as a dictionary\n",
    "        last_row = df.iloc[-1].to_dict()\n",
    "\n",
    "        # Compute a custom YOLO train score\n",
    "        score = (\n",
    "            last_row['metrics/mAP50-95(B)'] * 0.7 +\n",
    "            last_row['metrics/recall(B)'] * 0.2 -\n",
    "            last_row['val/box_loss'] * 0.1\n",
    "        )\n",
    "\n",
    "        # Add the score to the dictionary\n",
    "        last_row['train_score'] = score\n",
    "\n",
    "        results_data[folder] = last_row\n",
    "\n",
    "# Convert the dictionary to a pandas DataFrame\n",
    "summary_df = pd.DataFrame.from_dict(results_data, orient='index')\n",
    "summary_df.reset_index(inplace=True)\n",
    "summary_df.rename(columns={'index': 'folder'}, inplace=True)\n",
    "\n",
    "# Sort by train_score if desired\n",
    "best_train = summary_df.sort_values(by='train_score', ascending=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JnkELT0cIJg"
   },
   "source": [
    "# üîç 4. Prediction & Visualization\n",
    "\n",
    "Purpose: Evaluate results qualitatively and quantitatively.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.1 Run Predictions"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "best_path = os.path.join(RUNS_DIRECTORY, \"train/weights/best.pt\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "custom_model = YOLO(best_path)\n",
    "!rm -rf ./runs/predict\n",
    "preds = custom_model.predict(VALIDATION_DATASET_DIRECTORY, save=True, project=RUNS_DIRECTORY)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.2 Result of Prediction"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = custom_model.predict(VALIDATION_DATASET_DIRECTORY, conf=0.2)\n",
    "print(len(results))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.3 Visualize Predictions"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "predictions_paths = []\n",
    "\n",
    "for root, _, files in os.walk(os.path.join(RUNS_DIRECTORY, \"predict\")):\n",
    "    for f in files:\n",
    "        if f.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "            predictions_paths.append(os.path.join(root, f))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cols = 4\n",
    "max_images_preview = -1\n",
    "rows = math.ceil(len(predictions_paths) / cols)\n",
    "\n",
    "plt.figure(figsize=(cols * 3, rows * 2))  # dynamic size\n",
    "for i, path in enumerate(predictions_paths):\n",
    "    if max_images_preview != -1 and max_images_preview < len(predictions_paths):\n",
    "        break\n",
    "    img = Image.open(path)\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üìä 5. Analysis & Improvements\n",
    "Purpose: Interpret and iterate."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.1 Metrics Review"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "metrics.box.map, metrics.box.map50, metrics.box.map75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.2 Error Analysis"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "metrics.confusion_matrix.plot()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.3 Compare with previous trains"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ‚úÖ 6. Export & Deployment"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6.1 Save results"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# source = os.path.join(RUNS_DIRECTORY, \"train\")\n",
    "# saved_success_file_name = \".saved.success\"\n",
    "#\n",
    "# check_for_file = os.path.isfile(os.path.join(source, saved_success_file_name))\n",
    "#\n",
    "# if check_for_file:\n",
    "#     print(\"Training already saved. If you want to save it again, delete the .saved.success file.\")\n",
    "# else:\n",
    "#     now = datetime.now().strftime('%d-%m-%Y_%H:%M:%S')\n",
    "#     destination = \"./saved_trains/\" + now\n",
    "#\n",
    "#     shutil.copytree(source, destination, dirs_exist_ok=True)\n",
    "#\n",
    "#     train_config_path = os.path.join(destination, 'train_config.json')\n",
    "#     with open(train_config_path, 'w') as outfile:\n",
    "#         yaml.dump(json.dumps(train_config), outfile, default_flow_style=True)\n",
    "#\n",
    "#     saved_file = os.path.join(source, '.saved.success')\n",
    "#\n",
    "#     with open(saved_file, \"a\") as f:\n",
    "#         f.write(now)\n",
    "#     print(\"Training saved successfully.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T14:14:17.412167Z",
     "start_time": "2025-11-27T14:13:51.337455Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5.4MB 4.9MB/s 1.1s.0s<0.0s1.2s\n",
      "New https://pypi.org/project/ultralytics/8.3.233 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.221 üöÄ Python-3.13.9 torch-2.7.0+cu126 CPU (11th Gen Intel Core i7-1165G7 @ 2.80GHz)\n",
      "\u001B[34m\u001B[1mengine/trainer: \u001B[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=coco128.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=5, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=exp_external_mlflow, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/train, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/adrien/Projects/Python/AI/notebooks/computer_vision/runs/train/exp_external_mlflow, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\n",
      "WARNING ‚ö†Ô∏è Dataset 'coco128.yaml' images not found, missing path '/home/adrien/Projects/Python/datasets/coco128/images/train2017'\n",
      "\u001B[KDownloading https://ultralytics.com/assets/coco128.zip to '/home/adrien/Projects/Python/datasets/coco128.zip': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6.7MB 5.3MB/s 1.3s.2s<0.0s1.6s\n",
      "\u001B[KUnzipping /home/adrien/Projects/Python/datasets/coco128.zip to /home/adrien/Projects/Python/datasets/coco128...: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 263/263 5.2Kfiles/s 0.1s\n",
      "Dataset download success ‚úÖ (1.6s), saved to \u001B[1m/home/adrien/Projects/Python/datasets\u001B[0m\n",
      "\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    464912  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
      "YOLO11n summary: 181 layers, 2,624,080 parameters, 2,624,064 gradients, 6.6 GFLOPs\n",
      "\n",
      "Transferred 499/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 2104.3¬±842.7 MB/s, size: 46.0 KB)\n",
      "\u001B[K\u001B[34m\u001B[1mtrain: \u001B[0mScanning /home/adrien/Projects/Python/datasets/coco128/labels/train2017... 126 images, 2 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 128/128 2.5Kit/s 0.1s\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mNew cache created: /home/adrien/Projects/Python/datasets/coco128/labels/train2017.cache\n",
      "\u001B[34m\u001B[1mval: \u001B[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 2219.5¬±959.7 MB/s, size: 56.4 KB)\n",
      "\u001B[K\u001B[34m\u001B[1mval: \u001B[0mScanning /home/adrien/Projects/Python/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 128/128 376.2Kit/s 0.0s\n",
      "Plotting labels to /home/adrien/Projects/Python/AI/notebooks/computer_vision/runs/train/exp_external_mlflow/labels.jpg... \n",
      "\u001B[34m\u001B[1moptimizer:\u001B[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001B[34m\u001B[1moptimizer:\u001B[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/27 15:13:57 INFO mlflow.tracking.fluent: Experiment with name 'runs/train' does not exist. Creating a new experiment.\n",
      "2025/11/27 15:13:57 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/11/27 15:13:57 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2025/11/27 15:13:57 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mMLflow: \u001B[0mlogging run_id(171fab3f08504799b8c95571fe872f60) to https://hibou.pythonanywhere.com/\n",
      "\u001B[34m\u001B[1mMLflow: \u001B[0mdisable with 'yolo settings mlflow=False'\n",
      "üèÉ View run unleashed-slug-501 at: https://hibou.pythonanywhere.com/#/experiments/869246232133219410/runs/171fab3f08504799b8c95571fe872f60\n",
      "üß™ View experiment at: https://hibou.pythonanywhere.com/#/experiments/869246232133219410\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 13\u001B[39m\n\u001B[32m      8\u001B[39m \u001B[38;5;66;03m# ---------------------------------------------------\u001B[39;00m\n\u001B[32m      9\u001B[39m \u001B[38;5;66;03m# 3. Train YOLOv11\u001B[39;00m\n\u001B[32m     10\u001B[39m \u001B[38;5;66;03m# ---------------------------------------------------\u001B[39;00m\n\u001B[32m     11\u001B[39m model = YOLO(\u001B[33m\"\u001B[39m\u001B[33myolo11n.pt\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m results = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcoco128.yaml\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     15\u001B[39m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[43m    \u001B[49m\u001B[43mimgsz\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m640\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     17\u001B[39m \u001B[43m    \u001B[49m\u001B[43mproject\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mruns/train\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     18\u001B[39m \u001B[43m    \u001B[49m\u001B[43mname\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mexp_external_mlflow\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\n\u001B[32m     19\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     21\u001B[39m \u001B[38;5;66;03m# ---------------------------------------------------\u001B[39;00m\n\u001B[32m     22\u001B[39m \u001B[38;5;66;03m# 4. Log YOLO training metrics\u001B[39;00m\n\u001B[32m     23\u001B[39m \u001B[38;5;66;03m# ---------------------------------------------------\u001B[39;00m\n\u001B[32m     24\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m results.results_dict.items():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/lib/python3.13/site-packages/ultralytics/engine/model.py:800\u001B[39m, in \u001B[36mModel.train\u001B[39m\u001B[34m(self, trainer, **kwargs)\u001B[39m\n\u001B[32m    797\u001B[39m     \u001B[38;5;28mself\u001B[39m.trainer.model = \u001B[38;5;28mself\u001B[39m.trainer.get_model(weights=\u001B[38;5;28mself\u001B[39m.model \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.ckpt \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, cfg=\u001B[38;5;28mself\u001B[39m.model.yaml)\n\u001B[32m    798\u001B[39m     \u001B[38;5;28mself\u001B[39m.model = \u001B[38;5;28mself\u001B[39m.trainer.model\n\u001B[32m--> \u001B[39m\u001B[32m800\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    801\u001B[39m \u001B[38;5;66;03m# Update model and cfg after training\u001B[39;00m\n\u001B[32m    802\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m RANK \u001B[38;5;129;01min\u001B[39;00m {-\u001B[32m1\u001B[39m, \u001B[32m0\u001B[39m}:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/lib/python3.13/site-packages/ultralytics/engine/trainer.py:240\u001B[39m, in \u001B[36mBaseTrainer.train\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    237\u001B[39m         ddp_cleanup(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mstr\u001B[39m(file))\n\u001B[32m    239\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m240\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_do_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/lib/python3.13/site-packages/ultralytics/engine/trainer.py:361\u001B[39m, in \u001B[36mBaseTrainer._do_train\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    359\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.world_size > \u001B[32m1\u001B[39m:\n\u001B[32m    360\u001B[39m     \u001B[38;5;28mself\u001B[39m._setup_ddp()\n\u001B[32m--> \u001B[39m\u001B[32m361\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_setup_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    363\u001B[39m nb = \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m.train_loader)  \u001B[38;5;66;03m# number of batches\u001B[39;00m\n\u001B[32m    364\u001B[39m nw = \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;28mround\u001B[39m(\u001B[38;5;28mself\u001B[39m.args.warmup_epochs * nb), \u001B[32m100\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.args.warmup_epochs > \u001B[32m0\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m -\u001B[32m1\u001B[39m  \u001B[38;5;66;03m# warmup iterations\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/lib/python3.13/site-packages/ultralytics/engine/trainer.py:355\u001B[39m, in \u001B[36mBaseTrainer._setup_train\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    353\u001B[39m \u001B[38;5;28mself\u001B[39m.resume_training(ckpt)\n\u001B[32m    354\u001B[39m \u001B[38;5;28mself\u001B[39m.scheduler.last_epoch = \u001B[38;5;28mself\u001B[39m.start_epoch - \u001B[32m1\u001B[39m  \u001B[38;5;66;03m# do not move\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m355\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrun_callbacks\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mon_pretrain_routine_end\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/lib/python3.13/site-packages/ultralytics/engine/trainer.py:213\u001B[39m, in \u001B[36mBaseTrainer.run_callbacks\u001B[39m\u001B[34m(self, event)\u001B[39m\n\u001B[32m    211\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Run all existing callbacks associated with a particular event.\"\"\"\u001B[39;00m\n\u001B[32m    212\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m callback \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.callbacks.get(event, []):\n\u001B[32m--> \u001B[39m\u001B[32m213\u001B[39m     \u001B[43mcallback\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/lib/python3.13/site-packages/ultralytics/utils/callbacks/mlflow.py:82\u001B[39m, in \u001B[36mon_pretrain_routine_end\u001B[39m\u001B[34m(trainer)\u001B[39m\n\u001B[32m     80\u001B[39m         LOGGER.info(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mPREFIX\u001B[38;5;132;01m}\u001B[39;00m\u001B[33mview at http://127.0.0.1:5000 with \u001B[39m\u001B[33m'\u001B[39m\u001B[33mmlflow server --backend-store-uri \u001B[39m\u001B[38;5;132;01m{\u001B[39;00muri\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     81\u001B[39m     LOGGER.info(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mPREFIX\u001B[38;5;132;01m}\u001B[39;00m\u001B[33mdisable with \u001B[39m\u001B[33m'\u001B[39m\u001B[33myolo settings mlflow=False\u001B[39m\u001B[33m'\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m82\u001B[39m     \u001B[43mmlflow\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlog_params\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mdict\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     83\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m     84\u001B[39m     LOGGER.warning(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mPREFIX\u001B[38;5;132;01m}\u001B[39;00m\u001B[33mFailed to initialize: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/AI/lib/python3.13/site-packages/mlflow/tracking/fluent.py:1190\u001B[39m, in \u001B[36mlog_params\u001B[39m\u001B[34m(params, synchronous, run_id)\u001B[39m\n\u001B[32m   1188\u001B[39m params_arr = [Param(key, \u001B[38;5;28mstr\u001B[39m(value)) \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m params.items()]\n\u001B[32m   1189\u001B[39m synchronous = synchronous \u001B[38;5;28;01mif\u001B[39;00m synchronous \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m MLFLOW_ENABLE_ASYNC_LOGGING.get()\n\u001B[32m-> \u001B[39m\u001B[32m1190\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mMlflowClient\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlog_batch\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1191\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrun_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetrics\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m=\u001B[49m\u001B[43mparams_arr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtags\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msynchronous\u001B[49m\u001B[43m=\u001B[49m\u001B[43msynchronous\u001B[49m\n\u001B[32m   1192\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/AI/lib/python3.13/site-packages/mlflow/tracking/client.py:2395\u001B[39m, in \u001B[36mMlflowClient.log_batch\u001B[39m\u001B[34m(self, run_id, metrics, params, tags, synchronous)\u001B[39m\n\u001B[32m   2392\u001B[39m \u001B[38;5;66;03m# Stringify the values of the params\u001B[39;00m\n\u001B[32m   2393\u001B[39m params = [Param(key=param.key, value=\u001B[38;5;28mstr\u001B[39m(param.value)) \u001B[38;5;28;01mfor\u001B[39;00m param \u001B[38;5;129;01min\u001B[39;00m params]\n\u001B[32m-> \u001B[39m\u001B[32m2395\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_tracking_client\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlog_batch\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2396\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrun_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetrics\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtags\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msynchronous\u001B[49m\u001B[43m=\u001B[49m\u001B[43msynchronous\u001B[49m\n\u001B[32m   2397\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/AI/lib/python3.13/site-packages/mlflow/telemetry/track.py:23\u001B[39m, in \u001B[36mrecord_usage_event.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m     20\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m     21\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mwrapper\u001B[39m(*args: P.args, **kwargs: P.kwargs) -> R:\n\u001B[32m     22\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m is_telemetry_disabled() \u001B[38;5;129;01mor\u001B[39;00m _is_telemetry_disabled_for_event(event):\n\u001B[32m---> \u001B[39m\u001B[32m23\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     25\u001B[39m     success = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m     26\u001B[39m     result = \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/AI/lib/python3.13/site-packages/mlflow/tracking/_tracking_service/client.py:556\u001B[39m, in \u001B[36mTrackingServiceClient.log_batch\u001B[39m\u001B[34m(self, run_id, metrics, params, tags, synchronous)\u001B[39m\n\u001B[32m    553\u001B[39m metrics = metrics[metrics_batch_size:]\n\u001B[32m    555\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m synchronous:\n\u001B[32m--> \u001B[39m\u001B[32m556\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstore\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlog_batch\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    557\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrun_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetrics\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmetrics_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m=\u001B[49m\u001B[43mparams_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtags\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtags_batch\u001B[49m\n\u001B[32m    558\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    559\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    560\u001B[39m     run_operations_list.append(\n\u001B[32m    561\u001B[39m         \u001B[38;5;28mself\u001B[39m.store.log_batch_async(\n\u001B[32m    562\u001B[39m             run_id=run_id,\n\u001B[32m   (...)\u001B[39m\u001B[32m    566\u001B[39m         )\n\u001B[32m    567\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/AI/lib/python3.13/site-packages/mlflow/store/tracking/rest_store.py:774\u001B[39m, in \u001B[36mRestStore.log_batch\u001B[39m\u001B[34m(self, run_id, metrics, params, tags)\u001B[39m\n\u001B[32m    770\u001B[39m tag_protos = [tag.to_proto() \u001B[38;5;28;01mfor\u001B[39;00m tag \u001B[38;5;129;01min\u001B[39;00m tags]\n\u001B[32m    771\u001B[39m req_body = message_to_json(\n\u001B[32m    772\u001B[39m     LogBatch(metrics=metric_protos, params=param_protos, tags=tag_protos, run_id=run_id)\n\u001B[32m    773\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m774\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_endpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mLogBatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq_body\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/AI/lib/python3.13/site-packages/mlflow/store/tracking/rest_store.py:134\u001B[39m, in \u001B[36mRestStore._call_endpoint\u001B[39m\u001B[34m(self, api, json_body, endpoint, retry_timeout_seconds)\u001B[39m\n\u001B[32m    132\u001B[39m     endpoint, method = _METHOD_TO_INFO[api]\n\u001B[32m    133\u001B[39m response_proto = api.Response()\n\u001B[32m--> \u001B[39m\u001B[32m134\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcall_endpoint\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    135\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_host_creds\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    136\u001B[39m \u001B[43m    \u001B[49m\u001B[43mendpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    137\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    138\u001B[39m \u001B[43m    \u001B[49m\u001B[43mjson_body\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    139\u001B[39m \u001B[43m    \u001B[49m\u001B[43mresponse_proto\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    140\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretry_timeout_seconds\u001B[49m\u001B[43m=\u001B[49m\u001B[43mretry_timeout_seconds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    141\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/AI/lib/python3.13/site-packages/mlflow/utils/rest_utils.py:552\u001B[39m, in \u001B[36mcall_endpoint\u001B[39m\u001B[34m(host_creds, endpoint, method, json_body, response_proto, extra_headers, retry_timeout_seconds)\u001B[39m\n\u001B[32m    550\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    551\u001B[39m     call_kwargs[\u001B[33m\"\u001B[39m\u001B[33mjson\u001B[39m\u001B[33m\"\u001B[39m] = json_body\n\u001B[32m--> \u001B[39m\u001B[32m552\u001B[39m     response = \u001B[43mhttp_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mcall_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    554\u001B[39m response = verify_rest_response(response, endpoint)\n\u001B[32m    555\u001B[39m response_to_parse = response.text\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/AI/lib/python3.13/site-packages/mlflow/utils/rest_utils.py:230\u001B[39m, in \u001B[36mhttp_request\u001B[39m\u001B[34m(host_creds, endpoint, method, max_retries, backoff_factor, backoff_jitter, extra_headers, retry_codes, timeout, raise_on_status, respect_retry_after_header, retry_timeout_seconds, **kwargs)\u001B[39m\n\u001B[32m    227\u001B[39m     kwargs[\u001B[33m\"\u001B[39m\u001B[33mauth\u001B[39m\u001B[33m\"\u001B[39m] = fetch_auth(host_creds.auth)\n\u001B[32m    229\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m230\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_get_http_response_with_retries\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    231\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    232\u001B[39m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    233\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    234\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbackoff_factor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    235\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbackoff_jitter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    236\u001B[39m \u001B[43m        \u001B[49m\u001B[43mretry_codes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    237\u001B[39m \u001B[43m        \u001B[49m\u001B[43mraise_on_status\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    238\u001B[39m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    239\u001B[39m \u001B[43m        \u001B[49m\u001B[43mverify\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhost_creds\u001B[49m\u001B[43m.\u001B[49m\u001B[43mverify\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    240\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    241\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrespect_retry_after_header\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrespect_retry_after_header\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    242\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    243\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    244\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m requests.exceptions.Timeout \u001B[38;5;28;01mas\u001B[39;00m to:\n\u001B[32m    245\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m MlflowException(\n\u001B[32m    246\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mAPI request to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m failed with timeout exception \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mto\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    247\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m To increase the timeout, set the environment variable \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    248\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mMLFLOW_HTTP_REQUEST_TIMEOUT\u001B[38;5;132;01m!s}\u001B[39;00m\u001B[33m to a larger value.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    249\u001B[39m     ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mto\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/AI/lib/python3.13/site-packages/mlflow/utils/request_utils.py:237\u001B[39m, in \u001B[36m_get_http_response_with_retries\u001B[39m\u001B[34m(method, url, max_retries, backoff_factor, backoff_jitter, retry_codes, raise_on_status, allow_redirects, respect_retry_after_header, **kwargs)\u001B[39m\n\u001B[32m    234\u001B[39m env_value = os.getenv(\u001B[33m\"\u001B[39m\u001B[33mMLFLOW_ALLOW_HTTP_REDIRECTS\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mtrue\u001B[39m\u001B[33m\"\u001B[39m).lower() \u001B[38;5;129;01min\u001B[39;00m [\u001B[33m\"\u001B[39m\u001B[33mtrue\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33m1\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m    235\u001B[39m allow_redirects = env_value \u001B[38;5;28;01mif\u001B[39;00m allow_redirects \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m allow_redirects\n\u001B[32m--> \u001B[39m\u001B[32m237\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msession\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallow_redirects\u001B[49m\u001B[43m=\u001B[49m\u001B[43mallow_redirects\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/lib/python3.13/site-packages/requests/sessions.py:589\u001B[39m, in \u001B[36mSession.request\u001B[39m\u001B[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[39m\n\u001B[32m    584\u001B[39m send_kwargs = {\n\u001B[32m    585\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mtimeout\u001B[39m\u001B[33m\"\u001B[39m: timeout,\n\u001B[32m    586\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mallow_redirects\u001B[39m\u001B[33m\"\u001B[39m: allow_redirects,\n\u001B[32m    587\u001B[39m }\n\u001B[32m    588\u001B[39m send_kwargs.update(settings)\n\u001B[32m--> \u001B[39m\u001B[32m589\u001B[39m resp = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43msend_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    591\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/lib/python3.13/site-packages/requests/sessions.py:703\u001B[39m, in \u001B[36mSession.send\u001B[39m\u001B[34m(self, request, **kwargs)\u001B[39m\n\u001B[32m    700\u001B[39m start = preferred_clock()\n\u001B[32m    702\u001B[39m \u001B[38;5;66;03m# Send the request\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m703\u001B[39m r = \u001B[43madapter\u001B[49m\u001B[43m.\u001B[49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    705\u001B[39m \u001B[38;5;66;03m# Total elapsed time of the request (approximately)\u001B[39;00m\n\u001B[32m    706\u001B[39m elapsed = preferred_clock() - start\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/lib/python3.13/site-packages/requests/adapters.py:644\u001B[39m, in \u001B[36mHTTPAdapter.send\u001B[39m\u001B[34m(self, request, stream, timeout, verify, cert, proxies)\u001B[39m\n\u001B[32m    641\u001B[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001B[32m    643\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m644\u001B[39m     resp = \u001B[43mconn\u001B[49m\u001B[43m.\u001B[49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    645\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    646\u001B[39m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m=\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    647\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    648\u001B[39m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m.\u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    649\u001B[39m \u001B[43m        \u001B[49m\u001B[43mredirect\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    650\u001B[39m \u001B[43m        \u001B[49m\u001B[43massert_same_host\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    651\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    652\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    653\u001B[39m \u001B[43m        \u001B[49m\u001B[43mretries\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    654\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    655\u001B[39m \u001B[43m        \u001B[49m\u001B[43mchunked\u001B[49m\u001B[43m=\u001B[49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    656\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    658\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m (ProtocolError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[32m    659\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(err, request=request)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/AI/lib/python3.13/site-packages/urllib3/connectionpool.py:942\u001B[39m, in \u001B[36mHTTPConnectionPool.urlopen\u001B[39m\u001B[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[39m\n\u001B[32m    940\u001B[39m     retries.sleep(response)\n\u001B[32m    941\u001B[39m     log.debug(\u001B[33m\"\u001B[39m\u001B[33mRetry: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m\"\u001B[39m, url)\n\u001B[32m--> \u001B[39m\u001B[32m942\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    943\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    944\u001B[39m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    945\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    946\u001B[39m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    947\u001B[39m \u001B[43m        \u001B[49m\u001B[43mretries\u001B[49m\u001B[43m=\u001B[49m\u001B[43mretries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    948\u001B[39m \u001B[43m        \u001B[49m\u001B[43mredirect\u001B[49m\u001B[43m=\u001B[49m\u001B[43mredirect\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    949\u001B[39m \u001B[43m        \u001B[49m\u001B[43massert_same_host\u001B[49m\u001B[43m=\u001B[49m\u001B[43massert_same_host\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    950\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    951\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpool_timeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpool_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    952\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrelease_conn\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrelease_conn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    953\u001B[39m \u001B[43m        \u001B[49m\u001B[43mchunked\u001B[49m\u001B[43m=\u001B[49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    954\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbody_pos\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbody_pos\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    955\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    956\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    957\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mresponse_kw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    958\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    960\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/AI/lib/python3.13/site-packages/urllib3/connectionpool.py:942\u001B[39m, in \u001B[36mHTTPConnectionPool.urlopen\u001B[39m\u001B[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[39m\n\u001B[32m    940\u001B[39m     retries.sleep(response)\n\u001B[32m    941\u001B[39m     log.debug(\u001B[33m\"\u001B[39m\u001B[33mRetry: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m\"\u001B[39m, url)\n\u001B[32m--> \u001B[39m\u001B[32m942\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    943\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    944\u001B[39m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    945\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    946\u001B[39m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    947\u001B[39m \u001B[43m        \u001B[49m\u001B[43mretries\u001B[49m\u001B[43m=\u001B[49m\u001B[43mretries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    948\u001B[39m \u001B[43m        \u001B[49m\u001B[43mredirect\u001B[49m\u001B[43m=\u001B[49m\u001B[43mredirect\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    949\u001B[39m \u001B[43m        \u001B[49m\u001B[43massert_same_host\u001B[49m\u001B[43m=\u001B[49m\u001B[43massert_same_host\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    950\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    951\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpool_timeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpool_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    952\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrelease_conn\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrelease_conn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    953\u001B[39m \u001B[43m        \u001B[49m\u001B[43mchunked\u001B[49m\u001B[43m=\u001B[49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    954\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbody_pos\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbody_pos\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    955\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    956\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    957\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mresponse_kw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    958\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    960\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/AI/lib/python3.13/site-packages/urllib3/connectionpool.py:942\u001B[39m, in \u001B[36mHTTPConnectionPool.urlopen\u001B[39m\u001B[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[39m\n\u001B[32m    940\u001B[39m     retries.sleep(response)\n\u001B[32m    941\u001B[39m     log.debug(\u001B[33m\"\u001B[39m\u001B[33mRetry: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m\"\u001B[39m, url)\n\u001B[32m--> \u001B[39m\u001B[32m942\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    943\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    944\u001B[39m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    945\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    946\u001B[39m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    947\u001B[39m \u001B[43m        \u001B[49m\u001B[43mretries\u001B[49m\u001B[43m=\u001B[49m\u001B[43mretries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    948\u001B[39m \u001B[43m        \u001B[49m\u001B[43mredirect\u001B[49m\u001B[43m=\u001B[49m\u001B[43mredirect\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    949\u001B[39m \u001B[43m        \u001B[49m\u001B[43massert_same_host\u001B[49m\u001B[43m=\u001B[49m\u001B[43massert_same_host\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    950\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    951\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpool_timeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpool_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    952\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrelease_conn\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrelease_conn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    953\u001B[39m \u001B[43m        \u001B[49m\u001B[43mchunked\u001B[49m\u001B[43m=\u001B[49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    954\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbody_pos\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbody_pos\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    955\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    956\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    957\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mresponse_kw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    958\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    960\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/AI/lib/python3.13/site-packages/urllib3/connectionpool.py:940\u001B[39m, in \u001B[36mHTTPConnectionPool.urlopen\u001B[39m\u001B[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[39m\n\u001B[32m    937\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m response\n\u001B[32m    939\u001B[39m response.drain_conn()\n\u001B[32m--> \u001B[39m\u001B[32m940\u001B[39m \u001B[43mretries\u001B[49m\u001B[43m.\u001B[49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    941\u001B[39m log.debug(\u001B[33m\"\u001B[39m\u001B[33mRetry: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m\"\u001B[39m, url)\n\u001B[32m    942\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.urlopen(\n\u001B[32m    943\u001B[39m     method,\n\u001B[32m    944\u001B[39m     url,\n\u001B[32m   (...)\u001B[39m\u001B[32m    957\u001B[39m     **response_kw,\n\u001B[32m    958\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/AI/lib/python3.13/site-packages/urllib3/util/retry.py:363\u001B[39m, in \u001B[36mRetry.sleep\u001B[39m\u001B[34m(self, response)\u001B[39m\n\u001B[32m    360\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m slept:\n\u001B[32m    361\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m363\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sleep_backoff\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/AI/lib/python3.13/site-packages/urllib3/util/retry.py:347\u001B[39m, in \u001B[36mRetry._sleep_backoff\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    345\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m backoff <= \u001B[32m0\u001B[39m:\n\u001B[32m    346\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m347\u001B[39m \u001B[43mtime\u001B[49m\u001B[43m.\u001B[49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbackoff\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
