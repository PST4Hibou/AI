{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-18T12:42:17.293203Z",
     "start_time": "2025-11-18T12:42:14.325142Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import ClassLabel, Audio, DatasetDict\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Audio as DisplayAudio\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "NUM_PROC = 24\n",
    "SAMPLING_RATE = 16000\n",
    "CHUNK_DURATION = 0.5\n",
    "BATCH_SIZE = 32\n",
    "THRESHOLD_AUGMENTATION = 1"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pierre/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T12:55:51.202256Z",
     "start_time": "2025-11-18T12:55:48.269398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Loading DS\")\n",
    "# Load dataset\n",
    "DS = load_dataset(\"n1coc4cola/maotouying\")\n",
    "print(len(DS))\n",
    "DS_train = DS[\"train\"]"
   ],
   "id": "f50c8d27f32ebe05",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DS\n",
      "1\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T12:55:51.770649Z",
     "start_time": "2025-11-18T12:55:51.745932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Shuffling\")\n",
    "# Take only n% of the dataset\n",
    "n = 1.0\n",
    "DS_train_shuffled = DS_train.shuffle(seed=SEED).select(range(int(n * len(DS_train))))"
   ],
   "id": "9ebeb9471a5bda96",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T12:55:52.548207Z",
     "start_time": "2025-11-18T12:55:52.416995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_test = DS_train_shuffled.train_test_split(test_size=0.2, seed=SEED)\n",
    "test_val = train_test[\"test\"].train_test_split(test_size=0.5, seed=SEED)\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_test[\"train\"],\n",
    "    \"val\": test_val[\"train\"],\n",
    "    \"test\": test_val[\"test\"],\n",
    "})"
   ],
   "id": "739e03a487aaf0c1",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T12:55:53.164176Z",
     "start_time": "2025-11-18T12:55:53.112754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cast to 16khz\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].cast_column(\"audio\", Audio(sampling_rate=SAMPLING_RATE))"
   ],
   "id": "84d363c93d163c1f",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T12:55:53.639117Z",
     "start_time": "2025-11-18T12:55:53.633878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def noise_injection(data, noise_factor):\n",
    "    noise = np.random.randn(len(data))\n",
    "    augmented_data = data + noise_factor * noise\n",
    "    # Cast back to the same data type\n",
    "    augmented_data = augmented_data.astype(type(data[0]))\n",
    "    return augmented_data\n",
    "\n",
    "def change_pitch(data, sampling_rate, pitch_factor):\n",
    "    return librosa.effects.pitch_shift(data, sr=sampling_rate, n_steps=pitch_factor)\n",
    "\n",
    "def time_stretch(data, stretch_factor):\n",
    "    return librosa.effects.time_stretch(data, rate=stretch_factor)\n",
    "\n",
    "def frequency_mask(data, mask_width=10):\n",
    "    stft = librosa.stft(data)\n",
    "    freq_bins = stft.shape[0]\n",
    "    f0 = np.random.randint(0, freq_bins - mask_width)\n",
    "    stft[f0:f0+mask_width, :] = 0\n",
    "    return librosa.istft(stft)\n",
    "\n",
    "def time_mask(data, mask_width=10):\n",
    "    d = data.copy()\n",
    "    t0 = np.random.randint(0, len(d) - mask_width)\n",
    "    d[t0:t0+mask_width] = 0\n",
    "    return d\n",
    "\n",
    "def dynamic_range_compression(data, threshold, ratio):\n",
    "    # Simple compression: reduce amplitude above threshold\n",
    "    compressed = data.copy()\n",
    "    compressed[np.abs(data) > threshold] = threshold + (compressed[np.abs(data) > threshold] - threshold) / ratio\n",
    "    return compressed"
   ],
   "id": "e81b9794161c9fe",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T12:55:54.162230Z",
     "start_time": "2025-11-18T12:55:54.159350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Size of dataset: {len(dataset)}\")\n",
    "print(f\"Size of splits: train={len(dataset['train'])}, val={len(dataset['val'])}, test={len(dataset['test'])}\")"
   ],
   "id": "88683268461f5e1c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 3\n",
      "Size of splits: train=168771, val=21096, test=21097\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T12:59:41.098364Z",
     "start_time": "2025-11-18T12:55:54.947502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def split_audio_into_chunks(audio_array, chunk_duration=CHUNK_DURATION, sampling_rate=SAMPLING_RATE):\n",
    "    samples_per_chunk = int(chunk_duration * sampling_rate)\n",
    "    num_chunks = audio_array.shape[-1] // samples_per_chunk\n",
    "    # Only split into full chunks, no padding\n",
    "    chunks = [audio_array[i * samples_per_chunk:(i + 1) * samples_per_chunk]\n",
    "              for i in range(num_chunks)]\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def chunk_audio_batch(batch):\n",
    "    # Process by batch to allow multi processing\n",
    "    all_audios = []\n",
    "    all_sampling_rates = []\n",
    "    all_labels = []\n",
    "\n",
    "    for audio, label in zip(batch[\"audio\"], batch[\"label\"]):\n",
    "        audio_array = audio[\"array\"]\n",
    "        sampling_rate = audio[\"sampling_rate\"]\n",
    "        audio_array = torch.tensor(audio_array).float()\n",
    "        chunks = split_audio_into_chunks(audio_array)\n",
    "\n",
    "        all_audios.extend([chunk.numpy() for chunk in chunks])\n",
    "        all_sampling_rates.extend([sampling_rate] * len(chunks))\n",
    "        all_labels.extend([label] * len(chunks))\n",
    "\n",
    "    return {\n",
    "        \"audio\": all_audios,\n",
    "        \"label\": all_labels,\n",
    "    }\n",
    "\n",
    "\n",
    "chunked_dataset = DatasetDict()\n",
    "for split in dataset.keys():\n",
    "    print(f\"Chunking split: {split}, original length: {len(dataset[split])}\")\n",
    "    chunked_split = dataset[split].map(\n",
    "        chunk_audio_batch,\n",
    "        batched=True,\n",
    "        num_proc=NUM_PROC,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        remove_columns=dataset[split].column_names,\n",
    "    )\n",
    "    print(f\"Generated split {split} length, {len(chunked_split)}\")\n",
    "    chunked_dataset[split] = chunked_split"
   ],
   "id": "46067d4009f91aca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking split: train, original length: 168771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=24): 100%|██████████| 168771/168771 [02:55<00:00, 959.41 examples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated split train length, 617500\n",
      "Chunking split: val, original length: 21096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=24): 100%|██████████| 21096/21096 [00:26<00:00, 804.50 examples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated split val length, 78726\n",
      "Chunking split: test, original length: 21097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=24): 100%|██████████| 21097/21097 [00:22<00:00, 924.16 examples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated split test length, 78483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T12:42:26.227689Z",
     "start_time": "2025-11-18T12:42:25.977633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# def augmente_batch(batch):\n",
    "#     rng = np.random.default_rng()\n",
    "#     all_audios = []\n",
    "#     all_labels = []\n",
    "#\n",
    "#     for audio, label in zip(batch[\"audio\"], batch[\"label\"]):\n",
    "#         all_audios.append(audio)\n",
    "#         all_labels.append(label)\n",
    "#\n",
    "#         augmented_audio = None\n",
    "#         if rng.random() > THRESHOLD_AUGMENTATION:\n",
    "#             r = rng.random()\n",
    "#             data = np.array(audio)\n",
    "#             if r < 0.20:\n",
    "#                 augmented_audio = noise_injection(data, noise_factor=0.05)\n",
    "#             elif r < 0.40:\n",
    "#                 augmented_audio = change_pitch(data, sampling_rate=SAMPLING_RATE, pitch_factor=2)\n",
    "#             elif r < 0.60:\n",
    "#                 augmented_audio = time_stretch(data, stretch_factor=1.1)\n",
    "#             elif r < 0.80:\n",
    "#                 augmented_audio = dynamic_range_compression(data, threshold=0.05, ratio=1.0)\n",
    "#             else:\n",
    "#                 augmented_audio = frequency_mask(data, mask_width=20)\n",
    "#\n",
    "#             all_audios.append(augmented_audio)\n",
    "#             all_labels.append(label)\n",
    "#\n",
    "#\n",
    "#     return {\n",
    "#         \"audio\": all_audios,\n",
    "#         \"label\": all_labels,\n",
    "#     }\n",
    "#\n",
    "# augmented_dataset = DatasetDict()\n",
    "# for split in chunked_dataset.keys():\n",
    "#     print(f\"Augmenting split: {split}, length before: {len(chunked_dataset[split])}\")\n",
    "#     augmented_split = chunked_dataset[split].map(\n",
    "#         augmente_batch,\n",
    "#         batched=True,\n",
    "#         num_proc=NUM_PROC,\n",
    "#         batch_size=BATCH_SIZE,\n",
    "#         remove_columns=chunked_dataset[split].column_names,\n",
    "#     )\n",
    "#     print(f\"Length after: {len(augmented_split)}\")\n",
    "#     augmented_dataset[split] = augmented_split\n"
   ],
   "id": "5dcd316ee675e42a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting split: train, length before: 61543\n",
      "Length after: 61543\n",
      "Augmenting split: val, length before: 7222\n",
      "Length after: 7222\n",
      "Augmenting split: test, length before: 7220\n",
      "Length after: 7220\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T13:09:16.893352Z",
     "start_time": "2025-11-18T13:01:58.360794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------\n",
    "# Audio transforms for linear spectrogram\n",
    "# -----------------------------\n",
    "def convert_to_linear_spectrogram(batch):\n",
    "    all_linear_db = []\n",
    "    all_labels = []\n",
    "\n",
    "    for audio, label in zip(batch[\"audio\"], batch[\"label\"]):\n",
    "        data = np.array(audio)\n",
    "        # Compute STFT\n",
    "        stft = librosa.stft(data, n_fft=2048, hop_length=512)\n",
    "\n",
    "        # Compute magnitude\n",
    "        magnitude = np.abs(stft)\n",
    "\n",
    "        # Convert to dB\n",
    "        linear_db = librosa.amplitude_to_db(magnitude, ref=np.max)\n",
    "        all_linear_db.append(torch.tensor(linear_db))\n",
    "        all_labels.append(torch.tensor(label))\n",
    "\n",
    "    return {\n",
    "        # Convert to torch tensors\n",
    "        \"audio\": all_linear_db,\n",
    "        \"label\": all_labels,\n",
    "    }\n",
    "\n",
    "spectrogram_dataset = DatasetDict()\n",
    "for split in chunked_dataset.keys():\n",
    "    print(f\"Converting to spectrogram split: {split}\")\n",
    "    spectrogram_split = chunked_dataset[split].map(\n",
    "        convert_to_linear_spectrogram,\n",
    "        batched=True,\n",
    "        num_proc=NUM_PROC,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        remove_columns=chunked_dataset[split].column_names,\n",
    "    )\n",
    "    spectrogram_dataset[split] = spectrogram_split\n"
   ],
   "id": "284f5be619efd41f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to spectrogram split: train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=24): 100%|██████████| 617500/617500 [05:57<00:00, 1728.11 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to spectrogram split: val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=24): 100%|██████████| 78726/78726 [00:38<00:00, 2042.70 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to spectrogram split: test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=24): 100%|██████████| 78483/78483 [00:42<00:00, 1849.99 examples/s]\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T13:14:59.970664Z",
     "start_time": "2025-11-18T13:09:43.576971Z"
    }
   },
   "cell_type": "code",
   "source": "spectrogram_dataset.save_to_disk(\"./ds_2_noaugment_big.hf\")",
   "id": "ea75e1f1149f2f8a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (168/168 shards): 100%|██████████| 617500/617500 [03:35<00:00, 2859.63 examples/s]\n",
      "Saving the dataset (22/22 shards): 100%|██████████| 78726/78726 [00:21<00:00, 3696.12 examples/s]\n",
      "Saving the dataset (22/22 shards): 100%|██████████| 78483/78483 [00:22<00:00, 3457.75 examples/s]\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a7c1a8b2e08a3a72"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
