{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-24T21:18:17.612713Z",
     "start_time": "2025-10-24T21:18:15.247961Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import ClassLabel, Audio\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Audio as DisplayAudio\n",
    "import torch\n",
    "\n",
    "labels = [\"other\", \"drone\"]\n",
    "SEED = 42\n",
    "NUM_PROC = 23\n",
    "SAMPLING_RATE = 16000\n",
    "CHUNK_DURATION = 0.5\n",
    "BATCH_SIZE = 32\n",
    "THRESHOLD_AUGMENTATION = 0.5"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pierre/Documents/Projects/PST4/AI/.venv/lib64/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T21:19:42.723403Z",
     "start_time": "2025-10-24T21:19:39.541197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load dataset\n",
    "DS = load_dataset(\"geronimobasso/drone-audio-detection-samples\")"
   ],
   "id": "f50c8d27f32ebe05",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T21:19:51.165735Z",
     "start_time": "2025-10-24T21:19:50.584328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DS = DS[\"train\"]\n",
    "# Cast labels\n",
    "DS = DS.cast_column(\"label\", ClassLabel(names=labels))"
   ],
   "id": "29e4e5c2911370d6",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T21:19:56.330480Z",
     "start_time": "2025-10-24T21:19:56.323975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Take only n% of the dataset\n",
    "n = 1\n",
    "dataset = DS.shuffle(seed=SEED).select(range(int(n * len(DS))))"
   ],
   "id": "9ebeb9471a5bda96",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T21:19:57.183412Z",
     "start_time": "2025-10-24T21:19:57.180107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cast to 16khz\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ],
   "id": "84d363c93d163c1f",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T21:19:57.624462Z",
     "start_time": "2025-10-24T21:19:57.621301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def noise_injection(data, noise_factor):\n",
    "    noise = np.random.randn(len(data))\n",
    "    augmented_data = data + noise_factor * noise\n",
    "    # Cast back to the same data type\n",
    "    augmented_data = augmented_data.astype(type(data[0]))\n",
    "    return augmented_data\n",
    "\n",
    "def change_pitch(data, sampling_rate, pitch_factor):\n",
    "    return librosa.effects.pitch_shift(data, sr=sampling_rate, n_steps=pitch_factor)\n",
    "\n",
    "def time_stretch(data, stretch_factor):\n",
    "    return librosa.effects.time_stretch(data, rate=stretch_factor)\n",
    "\n",
    "def frequency_mask(data, mask_width=10):\n",
    "    stft = librosa.stft(data)\n",
    "    freq_bins = stft.shape[0]\n",
    "    f0 = np.random.randint(0, freq_bins - mask_width)\n",
    "    stft[f0:f0+mask_width, :] = 0\n",
    "    return librosa.istft(stft)\n",
    "\n",
    "def time_mask(data, mask_width=10):\n",
    "    d = data.copy()\n",
    "    t0 = np.random.randint(0, len(d) - mask_width)\n",
    "    d[t0:t0+mask_width] = 0\n",
    "    return d\n",
    "\n",
    "def dynamic_range_compression(data, threshold, ratio):\n",
    "    # Simple compression: reduce amplitude above threshold\n",
    "    compressed = data.copy()\n",
    "    compressed[np.abs(data) > threshold] = threshold + (compressed[np.abs(data) > threshold] - threshold) / ratio\n",
    "    return compressed\n"
   ],
   "id": "e81b9794161c9fe",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T21:19:58.101419Z",
     "start_time": "2025-10-24T21:19:58.098892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_spectrogram(audio, title, sr=16000):\n",
    "    D = librosa.amplitude_to_db(\n",
    "        librosa.stft(audio), ref=np.max\n",
    "    )\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(D, y_axis='log', x_axis='time', sr=sr)\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def plot_waveform(audio, title, sr=16000):\n",
    "    time = np.linspace(0, len(audio) / sr, num=len(audio))\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(time, audio, alpha=0.7)\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ],
   "id": "b2b6d3b7be3cc0a2",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T21:19:58.791602Z",
     "start_time": "2025-10-24T21:19:58.789719Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "88683268461f5e1c",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T21:19:59.370696Z",
     "start_time": "2025-10-24T21:19:59.368581Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Size of dataset: {len(dataset)}\")",
   "id": "54d8fb8bbc60515b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 1803\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T21:20:00.589514Z",
     "start_time": "2025-10-24T21:20:00.440967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def split_audio_into_chunks(audio_array, chunk_duration=CHUNK_DURATION, sampling_rate=SAMPLING_RATE):\n",
    "    samples_per_chunk = int(chunk_duration * sampling_rate)\n",
    "    num_chunks = audio_array.shape[-1] // samples_per_chunk\n",
    "\n",
    "    # Only split into full chunks, no padding\n",
    "    chunks = [audio_array[i * samples_per_chunk:(i + 1) * samples_per_chunk]\n",
    "              for i in range(num_chunks)]\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def chunk_audio_batch(batch):\n",
    "    # Process by batch to allow multi processing\n",
    "    all_audios = []\n",
    "    all_sampling_rates = []\n",
    "    all_labels = []\n",
    "\n",
    "    for audio, label in zip(batch[\"audio\"], batch[\"label\"]):\n",
    "        audio_array = audio[\"array\"]\n",
    "        sampling_rate = audio[\"sampling_rate\"]\n",
    "        audio_array = torch.tensor(audio_array).float()\n",
    "        chunks = split_audio_into_chunks(audio_array)\n",
    "\n",
    "        all_audios.extend([chunk.numpy() for chunk in chunks])\n",
    "        all_sampling_rates.extend([sampling_rate] * len(chunks))\n",
    "        all_labels.extend([label] * len(chunks))\n",
    "\n",
    "    return {\n",
    "        \"audio\": all_audios,\n",
    "        \"label\": all_labels,\n",
    "    }\n",
    "\n",
    "chunked_dataset = dataset.map(\n",
    "    chunk_audio_batch,\n",
    "    batched=True,\n",
    "    num_proc=NUM_PROC,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    remove_columns=dataset.column_names,\n",
    ")\n",
    "len(chunked_dataset)"
   ],
   "id": "46067d4009f91aca",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4235"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T21:20:09.300458Z",
     "start_time": "2025-10-24T21:20:01.206012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def augmente_batch(batch):\n",
    "    rng = np.random.default_rng()\n",
    "    all_audios = []\n",
    "    all_labels = []\n",
    "\n",
    "    for audio, label in zip(batch[\"audio\"], batch[\"label\"]):\n",
    "        all_audios.append(audio)\n",
    "        all_labels.append(label)\n",
    "\n",
    "        augmented_audio = None\n",
    "        if rng.random() > THRESHOLD_AUGMENTATION:\n",
    "            r = rng.random()\n",
    "            data = np.array(audio)\n",
    "            if r < 0.20:\n",
    "                augmented_audio = noise_injection(data, noise_factor=0.05)\n",
    "            elif r < 0.40:\n",
    "                augmented_audio = change_pitch(data, sampling_rate=SAMPLING_RATE, pitch_factor=2)\n",
    "            elif r < 0.60:\n",
    "                augmented_audio = time_stretch(data, stretch_factor=1.1)\n",
    "            elif r < 0.80:\n",
    "                augmented_audio = dynamic_range_compression(data, threshold=0.05, ratio=1.0)\n",
    "            else:\n",
    "                augmented_audio = frequency_mask(data, mask_width=20)\n",
    "\n",
    "            all_audios.append(augmented_audio)\n",
    "            all_labels.append(label)\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"audio\": all_audios,\n",
    "        \"label\": all_labels,\n",
    "    }\n",
    "\n",
    "augmented_dataset = chunked_dataset.map(augmente_batch, batched=True, num_proc=NUM_PROC, batch_size=BATCH_SIZE, remove_columns=dataset.column_names)\n",
    "len(augmented_dataset)"
   ],
   "id": "5dcd316ee675e42a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=23): 100%|██████████| 4235/4235 [00:08<00:00, 526.85 examples/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6352"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T21:20:10.919200Z",
     "start_time": "2025-10-24T21:20:10.917248Z"
    }
   },
   "cell_type": "code",
   "source": "print(len(augmented_dataset))",
   "id": "c3a2de6d2fd0a186",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6352\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T21:25:05.628946Z",
     "start_time": "2025-10-24T21:24:56.281973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# Audio transforms for linear spectrogram\n",
    "# -----------------------------\n",
    "def convert_to_linear_spectrogram(batch):\n",
    "    all_linear_db = []\n",
    "    all_labels = []\n",
    "\n",
    "    for audio, label in zip(batch[\"audio\"], batch[\"label\"]):\n",
    "        data = np.array(audio)\n",
    "        # Compute STFT\n",
    "        stft = librosa.stft(data, n_fft=2048, hop_length=512)\n",
    "\n",
    "        # Compute magnitude\n",
    "        magnitude = np.abs(stft)\n",
    "\n",
    "        # Convert to dB\n",
    "        linear_db = librosa.amplitude_to_db(magnitude, ref=np.max)\n",
    "\n",
    "        all_linear_db.append(linear_db)\n",
    "        all_labels.append(label)\n",
    "\n",
    "    return {\n",
    "        \"audio\": all_linear_db,\n",
    "        \"label\": all_labels,\n",
    "    }\n",
    "\n",
    "# Apply to dataset\n",
    "spectrogram_dataset = augmented_dataset.map(\n",
    "    convert_to_linear_spectrogram,\n",
    "    batched=True,\n",
    "    num_proc=NUM_PROC,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    remove_columns=augmented_dataset.column_names\n",
    ")\n",
    "len(spectrogram_dataset)\n"
   ],
   "id": "284f5be619efd41f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=23): 100%|██████████| 6352/6352 [00:09<00:00, 682.83 examples/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6352"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T21:27:07.613692Z",
     "start_time": "2025-10-24T21:27:07.450197Z"
    }
   },
   "cell_type": "code",
   "source": "augmented_dataset.save_to_disk(\"../../data/datasets/1_augmented_spectogram_test\")\n",
   "id": "3ca8952fe3b91855",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 6352/6352 [00:00<00:00, 40195.86 examples/s]\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# import torchaudio\n",
    "#\n",
    "# # -----------------------------\n",
    "# # Audio transforms\n",
    "# # -----------------------------\n",
    "# mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "#     sample_rate=SAMPLING_RATE,\n",
    "#     n_fft=2048,\n",
    "#     hop_length=512,\n",
    "#     n_mels=128,\n",
    "# )\n",
    "# db_transform = torchaudio.transforms.AmplitudeToDB()\n",
    "#\n",
    "# def convert_to_mel_spectogram(batch):\n",
    "#     all_mel_db = []\n",
    "#     all_labels = []\n",
    "#\n",
    "#     for audio, label in zip(batch[\"audio\"], batch[\"label\"]):\n",
    "#         mel = mel_transform(audio)\n",
    "#         mel_db = db_transform(mel)\n",
    "#\n",
    "#         all_mel_db.append(mel_db)\n",
    "#         all_labels.append(label)\n",
    "#\n",
    "#\n",
    "#     return {\n",
    "#         \"audio\": all_mel_db,\n",
    "#         \"label\": all_labels,\n",
    "#     }\n",
    "#\n",
    "# spectogram_dataset = chunked_dataset.map(augmente_batch, batched=True, num_proc=NUM_PROC, batch_size=BATCH_SIZE, remove_columns=dataset.column_names)\n",
    "# len(spectogram_dataset)"
   ],
   "id": "c8dbbe80bcbcdb0f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T19:45:09.967212Z",
     "start_time": "2025-10-24T19:44:43.029378Z"
    }
   },
   "cell_type": "code",
   "source": "augmented_dataset.save_to_disk(\"../../data/datasets/1_augmented_mel_spectogram\")",
   "id": "ea75e1f1149f2f8a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (42/42 shards): 100%|██████████| 654521/654521 [00:26<00:00, 24417.11 examples/s]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "142cb2fdbaa55c20",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
