{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T15:49:44.332766Z",
     "start_time": "2025-11-20T15:49:31.265705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "#################################### For Image ####################################\n",
    "from PIL import Image\n",
    "from sam3.model_builder import build_sam3_image_model\n",
    "from sam3.model.sam3_image_processor import Sam3Processor\n",
    "# Load the model\n",
    "model = build_sam3_image_model()\n",
    "processor = Sam3Processor(model)\n",
    "# Load an image\n",
    "image = Image.open(\"/home/pierre/Downloads/proxy-image.jpg\")\n",
    "inference_state = processor.set_image(image)\n",
    "# Prompt the model with text\n",
    "output = processor.set_text_prompt(state=inference_state, prompt=\"drone\")\n",
    "\n",
    "# Get the masks, bounding boxes, and scores\n",
    "masks, boxes, scores = output[\"masks\"], output[\"boxes\"], output[\"scores\"]\n"
   ],
   "id": "8a820cf72ac230d9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pierre/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/pierre/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  self.setter(val)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T16:19:41.027098Z",
     "start_time": "2025-11-20T16:19:37.757551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Sam3TrackerVideoModel, Sam3TrackerVideoProcessor, Sam3VideoProcessor\n",
    "from accelerate import Accelerator\n",
    "import torch"
   ],
   "id": "f5deb4151ca9be2d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pierre/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T16:20:00.211253Z",
     "start_time": "2025-11-20T16:19:54.056282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = Accelerator().device\n",
    "model = Sam3TrackerVideoModel.from_pretrained(\"facebook/sam3\").to(device, dtype=torch.bfloat16)\n",
    "processor = Sam3VideoProcessor.from_pretrained(\"facebook/sam3\")"
   ],
   "id": "f737fecbcae113a6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 1494it [00:01, 1530.08it/s, Materializing param=detector_model.detr_encoder.layers.2.cross_attn.q_proj.bias]                                               IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T16:20:04.266541Z",
     "start_time": "2025-11-20T16:20:03.147442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers.video_utils import load_video\n",
    "video_url = \"https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/bedroom.mp4\"\n",
    "video_frames, _ = load_video(video_url)"
   ],
   "id": "9e5df31e6060dcc",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T16:20:23.932279Z",
     "start_time": "2025-11-20T16:20:23.930310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inference_session = processor.init_video_session(\n",
    "    inference_device=device,\n",
    "    dtype=torch.bfloat16,\n",
    ")"
   ],
   "id": "2d8bd5e9d47be9bc",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T16:21:36.992243Z",
     "start_time": "2025-11-20T16:21:36.855612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for frame_idx, frame in enumerate(video_frames[:10]):  # Process first 10 frames\n",
    "    inputs = processor(images=frame, device=device, return_tensors=\"pt\", text=\"bed\")\n",
    "\n",
    "    # sam3_tracker_video_output = model(inference_session=inference_session, frame=inputs.pixel_values[0])\n",
    "    # video_res_masks = processor.post_process_masks(\n",
    "    #     [sam3_tracker_video_output.pred_masks], original_sizes=inputs.original_sizes, binarize=False\n",
    "    # )[0]\n",
    "    # print(f\"Frame {frame_idx}: mask shape {video_res_masks.shape}\")\n"
   ],
   "id": "1ce7e40b366adfd2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused or unrecognized kwargs: text.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Sam3FastImageProcessorKwargs.__init__() got an unexpected keyword argument 'text'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m frame_idx, frame \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(video_frames[:\u001B[32m10\u001B[39m]):  \u001B[38;5;66;03m# Process first 10 frames\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m     inputs = \u001B[43mprocessor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m=\u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mpt\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mbed\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      4\u001B[39m     \u001B[38;5;66;03m# sam3_tracker_video_output = model(inference_session=inference_session, frame=inputs.pixel_values[0])\u001B[39;00m\n\u001B[32m      5\u001B[39m     \u001B[38;5;66;03m# video_res_masks = processor.post_process_masks(\u001B[39;00m\n\u001B[32m      6\u001B[39m     \u001B[38;5;66;03m#     [sam3_tracker_video_output.pred_masks], original_sizes=inputs.original_sizes, binarize=False\u001B[39;00m\n\u001B[32m      7\u001B[39m     \u001B[38;5;66;03m# )[0]\u001B[39;00m\n\u001B[32m      8\u001B[39m     \u001B[38;5;66;03m# print(f\"Frame {frame_idx}: mask shape {video_res_masks.shape}\")\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/transformers/models/sam3_video/processing_sam3_video.py:90\u001B[39m, in \u001B[36mSam3VideoProcessor.__call__\u001B[39m\u001B[34m(self, images, segmentation_maps, original_sizes, return_tensors, **kwargs)\u001B[39m\n\u001B[32m     68\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     69\u001B[39m \u001B[33;03mThis method uses [`Sam3VideoImageProcessorFast.__call__`] method to prepare image(s) for the model.\u001B[39;00m\n\u001B[32m     70\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m     87\u001B[39m \u001B[33;03m    - `labels` (`torch.Tensor`, *optional*): The processed segmentation maps (if provided).\u001B[39;00m\n\u001B[32m     88\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     89\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m images \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m90\u001B[39m     encoding_image_processor = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mimage_processor\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     91\u001B[39m \u001B[43m        \u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     92\u001B[39m \u001B[43m        \u001B[49m\u001B[43msegmentation_maps\u001B[49m\u001B[43m=\u001B[49m\u001B[43msegmentation_maps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     93\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     94\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     95\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     96\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m original_sizes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m     97\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(original_sizes, torch.Tensor):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/transformers/image_processing_utils.py:54\u001B[39m, in \u001B[36mBaseImageProcessor.__call__\u001B[39m\u001B[34m(self, images, *args, **kwargs)\u001B[39m\n\u001B[32m     52\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, images: ImageInput, *args, **kwargs: Unpack[ImagesKwargs]) -> BatchFeature:\n\u001B[32m     53\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m54\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpreprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/transformers/models/sam3/image_processing_sam3_fast.py:476\u001B[39m, in \u001B[36mSam3ImageProcessorFast.preprocess\u001B[39m\u001B[34m(self, images, segmentation_maps, **kwargs)\u001B[39m\n\u001B[32m    465\u001B[39m \u001B[38;5;129m@auto_docstring\u001B[39m\n\u001B[32m    466\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpreprocess\u001B[39m(\n\u001B[32m    467\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    470\u001B[39m     **kwargs: Unpack[Sam3FastImageProcessorKwargs],\n\u001B[32m    471\u001B[39m ) -> BatchFeature:\n\u001B[32m    472\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    473\u001B[39m \u001B[33;03m    segmentation_maps (`ImageInput`, *optional*):\u001B[39;00m\n\u001B[32m    474\u001B[39m \u001B[33;03m        The segmentation maps to preprocess.\u001B[39;00m\n\u001B[32m    475\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m476\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpreprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msegmentation_maps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/transformers/image_processing_utils_fast.py:722\u001B[39m, in \u001B[36mBaseImageProcessorFast.preprocess\u001B[39m\u001B[34m(self, images, *args, **kwargs)\u001B[39m\n\u001B[32m    719\u001B[39m validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=\u001B[38;5;28mself\u001B[39m._valid_kwargs_names)\n\u001B[32m    721\u001B[39m \u001B[38;5;66;03m# Perform type validation on received kwargs\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m722\u001B[39m \u001B[43mvalidate_typed_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mvalid_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    724\u001B[39m \u001B[38;5;66;03m# Set default kwargs from self. This ensures that if a kwarg is not provided\u001B[39;00m\n\u001B[32m    725\u001B[39m \u001B[38;5;66;03m# by the user, it gets its default value from the instance, or is set to None.\u001B[39;00m\n\u001B[32m    726\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m kwarg_name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._valid_kwargs_names:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/huggingface_hub/dataclasses.py:302\u001B[39m, in \u001B[36mvalidate_typed_dict\u001B[39m\u001B[34m(schema, data)\u001B[39m\n\u001B[32m    299\u001B[39m strict_cls = _build_strict_cls_from_typed_dict(schema)\n\u001B[32m    301\u001B[39m \u001B[38;5;66;03m# Validate the data by instantiating the strict dataclass\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m302\u001B[39m \u001B[43mstrict_cls\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/huggingface_hub/dataclasses.py:244\u001B[39m, in \u001B[36mstrict.<locals>.wrap.<locals>.init_with_validate\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    241\u001B[39m \u001B[38;5;129m@wraps\u001B[39m(initial_init)\n\u001B[32m    242\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34minit_with_validate\u001B[39m(\u001B[38;5;28mself\u001B[39m, *args, **kwargs) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    243\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Run class validators after initialization.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m244\u001B[39m     \u001B[43minitial_init\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore [call-arg]\u001B[39;00m\n\u001B[32m    245\u001B[39m     \u001B[38;5;28mcls\u001B[39m.validate(\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[31mTypeError\u001B[39m: Sam3FastImageProcessorKwargs.__init__() got an unexpected keyword argument 'text'"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T16:24:41.574149Z",
     "start_time": "2025-11-20T16:24:31.796867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "from sam3.model_builder import build_sam3_image_model\n",
    "from sam3.model.sam3_image_processor import Sam3Processor\n",
    "\n",
    "model = build_sam3_image_model()\n",
    "processor = Sam3Processor(model)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "fps = 30\n",
    "interval_frames = int(fps * 0.5)\n",
    "\n",
    "frame_index = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    print(\"\")\n",
    "    if frame_index % interval_frames == 0:\n",
    "        pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        state = processor.set_image(pil_image)\n",
    "        out = processor.set_text_prompt(state=state, prompt=\"drone\")\n",
    "        masks, boxes, scores = out[\"masks\"], out[\"boxes\"], out[\"scores\"]\n",
    "        print(\"Hello\")\n",
    "    frame_index += 1\n",
    "\n",
    "cap.release()\n"
   ],
   "id": "90bcdd5fa883db1c",
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 33.69 MiB is free. Process 35858 has 61.97 MiB memory in use. Including non-PyTorch memory, this process has 6.34 GiB memory in use. Of the allocated memory 6.01 GiB is allocated by PyTorch, and 179.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mOutOfMemoryError\u001B[39m                          Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msam3\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmodel_builder\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m build_sam3_image_model\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msam3\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmodel\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01msam3_image_processor\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Sam3Processor\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m model = \u001B[43mbuild_sam3_image_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      7\u001B[39m processor = Sam3Processor(model)\n\u001B[32m      9\u001B[39m cap = cv2.VideoCapture(\u001B[32m0\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/sam3/model_builder.py:635\u001B[39m, in \u001B[36mbuild_sam3_image_model\u001B[39m\u001B[34m(bpe_path, device, eval_mode, checkpoint_path, load_from_HF, enable_segmentation, enable_inst_interactivity, compile)\u001B[39m\n\u001B[32m    632\u001B[39m     _load_checkpoint(model, checkpoint_path)\n\u001B[32m    634\u001B[39m \u001B[38;5;66;03m# Setup device and mode\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m635\u001B[39m model = \u001B[43m_setup_device_and_mode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meval_mode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    637\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/sam3/model_builder.py:551\u001B[39m, in \u001B[36m_setup_device_and_mode\u001B[39m\u001B[34m(model, device, eval_mode)\u001B[39m\n\u001B[32m    549\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Setup model device and evaluation mode.\"\"\"\u001B[39;00m\n\u001B[32m    550\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m device == \u001B[33m\"\u001B[39m\u001B[33mcuda\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m551\u001B[39m     model = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    552\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m eval_mode:\n\u001B[32m    553\u001B[39m     model.eval()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1084\u001B[39m, in \u001B[36mModule.cuda\u001B[39m\u001B[34m(self, device)\u001B[39m\n\u001B[32m   1067\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcuda\u001B[39m(\u001B[38;5;28mself\u001B[39m, device: Optional[Union[\u001B[38;5;28mint\u001B[39m, device]] = \u001B[38;5;28;01mNone\u001B[39;00m) -> Self:\n\u001B[32m   1068\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"Move all model parameters and buffers to the GPU.\u001B[39;00m\n\u001B[32m   1069\u001B[39m \n\u001B[32m   1070\u001B[39m \u001B[33;03m    This also makes associated parameters and buffers different objects. So\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1082\u001B[39m \u001B[33;03m        Module: self\u001B[39;00m\n\u001B[32m   1083\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1084\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:930\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    928\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[32m    929\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.children():\n\u001B[32m--> \u001B[39m\u001B[32m930\u001B[39m         \u001B[43mmodule\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    932\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied) -> \u001B[38;5;28mbool\u001B[39m:\n\u001B[32m    933\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[32m    934\u001B[39m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[32m    935\u001B[39m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    940\u001B[39m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[32m    941\u001B[39m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:930\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    928\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[32m    929\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.children():\n\u001B[32m--> \u001B[39m\u001B[32m930\u001B[39m         \u001B[43mmodule\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    932\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied) -> \u001B[38;5;28mbool\u001B[39m:\n\u001B[32m    933\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[32m    934\u001B[39m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[32m    935\u001B[39m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    940\u001B[39m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[32m    941\u001B[39m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "    \u001B[31m[... skipping similar frames: Module._apply at line 930 (5 times)]\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:930\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    928\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[32m    929\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.children():\n\u001B[32m--> \u001B[39m\u001B[32m930\u001B[39m         \u001B[43mmodule\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    932\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied) -> \u001B[38;5;28mbool\u001B[39m:\n\u001B[32m    933\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[32m    934\u001B[39m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[32m    935\u001B[39m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    940\u001B[39m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[32m    941\u001B[39m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:957\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    953\u001B[39m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[32m    954\u001B[39m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[32m    955\u001B[39m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[32m    956\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m--> \u001B[39m\u001B[32m957\u001B[39m     param_applied = \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    958\u001B[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001B[32m    960\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_subclasses\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mfake_tensor\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m FakeTensor\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1084\u001B[39m, in \u001B[36mModule.cuda.<locals>.<lambda>\u001B[39m\u001B[34m(t)\u001B[39m\n\u001B[32m   1067\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcuda\u001B[39m(\u001B[38;5;28mself\u001B[39m, device: Optional[Union[\u001B[38;5;28mint\u001B[39m, device]] = \u001B[38;5;28;01mNone\u001B[39;00m) -> Self:\n\u001B[32m   1068\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"Move all model parameters and buffers to the GPU.\u001B[39;00m\n\u001B[32m   1069\u001B[39m \n\u001B[32m   1070\u001B[39m \u001B[33;03m    This also makes associated parameters and buffers different objects. So\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1082\u001B[39m \u001B[33;03m        Module: self\u001B[39;00m\n\u001B[32m   1083\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1084\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._apply(\u001B[38;5;28;01mlambda\u001B[39;00m t: \u001B[43mt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[31mOutOfMemoryError\u001B[39m: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 33.69 MiB is free. Process 35858 has 61.97 MiB memory in use. Including non-PyTorch memory, this process has 6.34 GiB memory in use. Of the allocated memory 6.01 GiB is allocated by PyTorch, and 179.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "dd9f0ba4553780c3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
