{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-20T15:13:42.841001Z",
     "start_time": "2025-11-20T15:13:28.398579Z"
    }
   },
   "source": [
    "from sam3.model_builder import build_sam3_video_predictor\n",
    "import torch\n",
    "import sam3\n",
    "import os\n",
    "gpus_to_use = [torch.cuda.current_device()]\n",
    "predictor = build_sam3_video_predictor(gpus_to_use=gpus_to_use)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pierre/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/pierre/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  self.setter(val)\n",
      "\u001B[0m\u001B[32mINFO 2025-11-20 16:13:31,633 9922 sam3_video_predictor.py: 299:\u001B[0m using the following GPU IDs: [0]\n",
      "\u001B[0m\u001B[32mINFO 2025-11-20 16:13:31,764 9922 sam3_video_predictor.py: 315:\u001B[0m \n",
      "\n",
      "\n",
      "\t*** START loading model on all ranks ***\n",
      "\n",
      "\n",
      "\u001B[0m\u001B[32mINFO 2025-11-20 16:13:31,764 9922 sam3_video_predictor.py: 317:\u001B[0m loading model on rank=0 with world_size=1 -- this could take a while ...\n",
      "\u001B[0m\u001B[32mINFO 2025-11-20 16:13:38,571 9922 sam3_video_base.py: 124:\u001B[0m setting max_num_objects=10000 and num_obj_for_compile=16\n",
      "\u001B[0m\u001B[32mINFO 2025-11-20 16:13:42,839 9922 sam3_video_predictor.py: 319:\u001B[0m loading model on rank=0 with world_size=1 -- DONE locally\n",
      "\u001B[0m\u001B[32mINFO 2025-11-20 16:13:42,839 9922 sam3_video_predictor.py: 330:\u001B[0m \n",
      "\n",
      "\n",
      "\t*** DONE loading model on all ranks ***\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T15:13:59.901355Z",
     "start_time": "2025-11-20T15:13:59.234111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sam3.visualization_utils import (\n",
    "    load_frame,\n",
    "    prepare_masks_for_visualization,\n",
    "    visualize_formatted_frame_output,\n",
    ")\n",
    "\n",
    "# font size for axes titles\n",
    "plt.rcParams[\"axes.titlesize\"] = 12\n",
    "plt.rcParams[\"figure.titlesize\"] = 12\n",
    "\n",
    "\n",
    "def propagate_in_video(predictor, session_id):\n",
    "    # we will just propagate from frame 0 to the end of the video\n",
    "    outputs_per_frame = {}\n",
    "    for response in predictor.handle_stream_request(\n",
    "        request=dict(\n",
    "            type=\"propagate_in_video\",\n",
    "            session_id=session_id,\n",
    "        )\n",
    "    ):\n",
    "        outputs_per_frame[response[\"frame_index\"]] = response[\"outputs\"]\n",
    "\n",
    "    return outputs_per_frame\n",
    "\n",
    "\n",
    "def abs_to_rel_coords(coords, IMG_WIDTH, IMG_HEIGHT, coord_type=\"point\"):\n",
    "    \"\"\"Convert absolute coordinates to relative coordinates (0-1 range)\n",
    "\n",
    "    Args:\n",
    "        coords: List of coordinates\n",
    "        coord_type: 'point' for [x, y] or 'box' for [x, y, w, h]\n",
    "    \"\"\"\n",
    "    if coord_type == \"point\":\n",
    "        return [[x / IMG_WIDTH, y / IMG_HEIGHT] for x, y in coords]\n",
    "    elif coord_type == \"box\":\n",
    "        return [\n",
    "            [x / IMG_WIDTH, y / IMG_HEIGHT, w / IMG_WIDTH, h / IMG_HEIGHT]\n",
    "            for x, y, w, h in coords\n",
    "        ]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown coord_type: {coord_type}\")"
   ],
   "id": "5acd511233089e42",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T15:14:01.595011Z",
     "start_time": "2025-11-20T15:14:01.592550Z"
    }
   },
   "cell_type": "code",
   "source": "video_path = f\"/home/pierre/Videos/Screencasts/Screencast From 2025-11-20 15-32-03.mp4\"",
   "id": "4f40d40f86c2fd0c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T15:14:04.089893Z",
     "start_time": "2025-11-20T15:14:02.560819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if isinstance(video_path, str) and video_path.endswith(\".mp4\"):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    video_frames_for_vis = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        video_frames_for_vis.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    cap.release()\n",
    "else:\n",
    "    video_frames_for_vis = glob.glob(os.path.join(video_path, \"*.jpg\"))\n",
    "    try:\n",
    "        # integer sort instead of string sort (so that e.g. \"2.jpg\" is before \"11.jpg\")\n",
    "        video_frames_for_vis.sort(\n",
    "            key=lambda p: int(os.path.splitext(os.path.basename(p))[0])\n",
    "        )\n",
    "    except ValueError:\n",
    "        # fallback to lexicographic sort if the format is not \"<frame_index>.jpg\"\n",
    "        print(\n",
    "            f'frame names are not in \"<frame_index>.jpg\" format: {video_frames_for_vis[:5]=}, '\n",
    "            f\"falling back to lexicographic sort.\"\n",
    "        )\n",
    "        video_frames_for_vis.sort()"
   ],
   "id": "e7ef0e233cc7b16b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T15:14:10.507270Z",
     "start_time": "2025-11-20T15:14:05.028539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = predictor.handle_request(\n",
    "    request=dict(\n",
    "        type=\"start_session\",\n",
    "        resource_path=video_path,\n",
    "    )\n",
    ")\n",
    "session_id = response[\"session_id\"]"
   ],
   "id": "690f88fa2c9ebc0f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame loading (OpenCV) [rank=0]: 100%|█████████▉| 510/511 [00:01<00:00, 316.27it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 5.79 GiB. GPU 0 has a total capacity of 7.60 GiB of which 2.95 GiB is free. Including non-PyTorch memory, this process has 3.60 GiB memory in use. Of the allocated memory 3.36 GiB is allocated by PyTorch, and 104.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mOutOfMemoryError\u001B[39m                          Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m response = \u001B[43mpredictor\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      2\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mdict\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mtype\u001B[39;49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstart_session\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresource_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvideo_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m      7\u001B[39m session_id = response[\u001B[33m\"\u001B[39m\u001B[33msession_id\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    117\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    119\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/sam3/model/sam3_video_predictor.py:349\u001B[39m, in \u001B[36mSam3VideoPredictorMultiGPU.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    346\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m rank \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[32m1\u001B[39m, \u001B[38;5;28mself\u001B[39m.world_size):\n\u001B[32m    347\u001B[39m         \u001B[38;5;28mself\u001B[39m.command_queues[rank].put((request, \u001B[38;5;28;01mFalse\u001B[39;00m))\n\u001B[32m--> \u001B[39m\u001B[32m349\u001B[39m response = \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    351\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.world_size > \u001B[32m1\u001B[39m:\n\u001B[32m    352\u001B[39m     torch.distributed.barrier()  \u001B[38;5;66;03m# wait for all ranks to finish\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    117\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    119\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/sam3/model/sam3_video_predictor.py:60\u001B[39m, in \u001B[36mSam3VideoPredictor.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m     58\u001B[39m request_type = request[\u001B[33m\"\u001B[39m\u001B[33mtype\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m     59\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m request_type == \u001B[33m\"\u001B[39m\u001B[33mstart_session\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m60\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstart_session\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     61\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresource_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mresource_path\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     62\u001B[39m \u001B[43m        \u001B[49m\u001B[43msession_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43msession_id\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     63\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     64\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m request_type == \u001B[33m\"\u001B[39m\u001B[33madd_prompt\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m     65\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.add_prompt(\n\u001B[32m     66\u001B[39m         session_id=request[\u001B[33m\"\u001B[39m\u001B[33msession_id\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m     67\u001B[39m         frame_idx=request[\u001B[33m\"\u001B[39m\u001B[33mframe_index\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m   (...)\u001B[39m\u001B[32m     73\u001B[39m         obj_id=request.get(\u001B[33m\"\u001B[39m\u001B[33mobj_id\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[32m     74\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/sam3/model/sam3_video_predictor.py:113\u001B[39m, in \u001B[36mSam3VideoPredictor.start_session\u001B[39m\u001B[34m(self, resource_path, session_id)\u001B[39m\n\u001B[32m    103\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    104\u001B[39m \u001B[33;03mStart a new inference session on an image or a video. Here `resource_path`\u001B[39;00m\n\u001B[32m    105\u001B[39m \u001B[33;03mcan be either a path to an image file (for image inference) or an MP4 file\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    110\u001B[39m \u001B[33;03ma session id and return it.\u001B[39;00m\n\u001B[32m    111\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    112\u001B[39m \u001B[38;5;66;03m# get an initial inference_state from the model\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m113\u001B[39m inference_state = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43minit_state\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    114\u001B[39m \u001B[43m    \u001B[49m\u001B[43mresource_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresource_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    115\u001B[39m \u001B[43m    \u001B[49m\u001B[43masync_loading_frames\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43masync_loading_frames\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    116\u001B[39m \u001B[43m    \u001B[49m\u001B[43mvideo_loader_type\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mvideo_loader_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    117\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m session_id:\n\u001B[32m    119\u001B[39m     session_id = \u001B[38;5;28mstr\u001B[39m(uuid.uuid4())\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    117\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    119\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/sam3/model/sam3_video_inference.py:62\u001B[39m, in \u001B[36mSam3VideoInference.init_state\u001B[39m\u001B[34m(self, resource_path, offload_video_to_cpu, async_loading_frames, video_loader_type)\u001B[39m\n\u001B[32m     53\u001B[39m \u001B[38;5;129m@torch\u001B[39m.inference_mode()\n\u001B[32m     54\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34minit_state\u001B[39m(\n\u001B[32m     55\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m     59\u001B[39m     video_loader_type=\u001B[33m\"\u001B[39m\u001B[33mcv2\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     60\u001B[39m ):\n\u001B[32m     61\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Initialize an inference state from `resource_path` (an image or a video).\"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m62\u001B[39m     images, orig_height, orig_width = \u001B[43mload_resource_as_video_frames\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     63\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresource_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresource_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     64\u001B[39m \u001B[43m        \u001B[49m\u001B[43mimage_size\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mimage_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     65\u001B[39m \u001B[43m        \u001B[49m\u001B[43moffload_video_to_cpu\u001B[49m\u001B[43m=\u001B[49m\u001B[43moffload_video_to_cpu\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     66\u001B[39m \u001B[43m        \u001B[49m\u001B[43mimg_mean\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mimage_mean\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     67\u001B[39m \u001B[43m        \u001B[49m\u001B[43mimg_std\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mimage_std\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     68\u001B[39m \u001B[43m        \u001B[49m\u001B[43masync_loading_frames\u001B[49m\u001B[43m=\u001B[49m\u001B[43masync_loading_frames\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     69\u001B[39m \u001B[43m        \u001B[49m\u001B[43mvideo_loader_type\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvideo_loader_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     70\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     71\u001B[39m     inference_state = {}\n\u001B[32m     72\u001B[39m     inference_state[\u001B[33m\"\u001B[39m\u001B[33mimage_size\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28mself\u001B[39m.image_size\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/sam3/model/io_utils.py:82\u001B[39m, in \u001B[36mload_resource_as_video_frames\u001B[39m\u001B[34m(resource_path, image_size, offload_video_to_cpu, img_mean, img_std, async_loading_frames, video_loader_type)\u001B[39m\n\u001B[32m     74\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m load_image_as_single_frame_video(\n\u001B[32m     75\u001B[39m         image_path=resource_path,\n\u001B[32m     76\u001B[39m         image_size=image_size,\n\u001B[32m   (...)\u001B[39m\u001B[32m     79\u001B[39m         img_std=img_std,\n\u001B[32m     80\u001B[39m     )\n\u001B[32m     81\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m82\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mload_video_frames\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     83\u001B[39m \u001B[43m        \u001B[49m\u001B[43mvideo_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresource_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     84\u001B[39m \u001B[43m        \u001B[49m\u001B[43mimage_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mimage_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     85\u001B[39m \u001B[43m        \u001B[49m\u001B[43moffload_video_to_cpu\u001B[49m\u001B[43m=\u001B[49m\u001B[43moffload_video_to_cpu\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     86\u001B[39m \u001B[43m        \u001B[49m\u001B[43mimg_mean\u001B[49m\u001B[43m=\u001B[49m\u001B[43mimg_mean\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     87\u001B[39m \u001B[43m        \u001B[49m\u001B[43mimg_std\u001B[49m\u001B[43m=\u001B[49m\u001B[43mimg_std\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     88\u001B[39m \u001B[43m        \u001B[49m\u001B[43masync_loading_frames\u001B[49m\u001B[43m=\u001B[49m\u001B[43masync_loading_frames\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     89\u001B[39m \u001B[43m        \u001B[49m\u001B[43mvideo_loader_type\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvideo_loader_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     90\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/sam3/model/io_utils.py:145\u001B[39m, in \u001B[36mload_video_frames\u001B[39m\u001B[34m(video_path, image_size, offload_video_to_cpu, img_mean, img_std, async_loading_frames, video_loader_type)\u001B[39m\n\u001B[32m    136\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m load_video_frames_from_image_folder(\n\u001B[32m    137\u001B[39m         image_folder=video_path,\n\u001B[32m    138\u001B[39m         image_size=image_size,\n\u001B[32m   (...)\u001B[39m\u001B[32m    142\u001B[39m         async_loading_frames=async_loading_frames,\n\u001B[32m    143\u001B[39m     )\n\u001B[32m    144\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m os.path.splitext(video_path)[-\u001B[32m1\u001B[39m].lower() \u001B[38;5;129;01min\u001B[39;00m VIDEO_EXTS:\n\u001B[32m--> \u001B[39m\u001B[32m145\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mload_video_frames_from_video_file\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    146\u001B[39m \u001B[43m        \u001B[49m\u001B[43mvideo_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvideo_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    147\u001B[39m \u001B[43m        \u001B[49m\u001B[43mimage_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mimage_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    148\u001B[39m \u001B[43m        \u001B[49m\u001B[43moffload_video_to_cpu\u001B[49m\u001B[43m=\u001B[49m\u001B[43moffload_video_to_cpu\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    149\u001B[39m \u001B[43m        \u001B[49m\u001B[43mimg_mean\u001B[49m\u001B[43m=\u001B[49m\u001B[43mimg_mean\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    150\u001B[39m \u001B[43m        \u001B[49m\u001B[43mimg_std\u001B[49m\u001B[43m=\u001B[49m\u001B[43mimg_std\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    151\u001B[39m \u001B[43m        \u001B[49m\u001B[43masync_loading_frames\u001B[49m\u001B[43m=\u001B[49m\u001B[43masync_loading_frames\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    152\u001B[39m \u001B[43m        \u001B[49m\u001B[43mvideo_loader_type\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvideo_loader_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    153\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    154\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    155\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mOnly video files and image folders are supported\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/sam3/model/io_utils.py:226\u001B[39m, in \u001B[36mload_video_frames_from_video_file\u001B[39m\u001B[34m(video_path, image_size, offload_video_to_cpu, img_mean, img_std, async_loading_frames, gpu_acceleration, gpu_device, video_loader_type)\u001B[39m\n\u001B[32m    224\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Load the video frames from a video file.\"\"\"\u001B[39;00m\n\u001B[32m    225\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m video_loader_type == \u001B[33m\"\u001B[39m\u001B[33mcv2\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m226\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mload_video_frames_from_video_file_using_cv2\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    227\u001B[39m \u001B[43m        \u001B[49m\u001B[43mvideo_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvideo_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    228\u001B[39m \u001B[43m        \u001B[49m\u001B[43mimage_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mimage_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    229\u001B[39m \u001B[43m        \u001B[49m\u001B[43mimg_mean\u001B[49m\u001B[43m=\u001B[49m\u001B[43mimg_mean\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    230\u001B[39m \u001B[43m        \u001B[49m\u001B[43mimg_std\u001B[49m\u001B[43m=\u001B[49m\u001B[43mimg_std\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    231\u001B[39m \u001B[43m        \u001B[49m\u001B[43moffload_video_to_cpu\u001B[49m\u001B[43m=\u001B[49m\u001B[43moffload_video_to_cpu\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    232\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    233\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m video_loader_type == \u001B[33m\"\u001B[39m\u001B[33mtorchcodec\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    234\u001B[39m     logger.info(\u001B[33m\"\u001B[39m\u001B[33mUsing torchcodec to load video file\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/sam3/model/io_utils.py:310\u001B[39m, in \u001B[36mload_video_frames_from_video_file_using_cv2\u001B[39m\u001B[34m(video_path, image_size, img_mean, img_std, offload_video_to_cpu)\u001B[39m\n\u001B[32m    308\u001B[39m img_std = torch.tensor(img_std, dtype=torch.float16).view(\u001B[32m1\u001B[39m, \u001B[32m3\u001B[39m, \u001B[32m1\u001B[39m, \u001B[32m1\u001B[39m)\n\u001B[32m    309\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m offload_video_to_cpu:\n\u001B[32m--> \u001B[39m\u001B[32m310\u001B[39m     video_tensor = \u001B[43mvideo_tensor\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    311\u001B[39m     img_mean = img_mean.cuda()\n\u001B[32m    312\u001B[39m     img_std = img_std.cuda()\n",
      "\u001B[31mOutOfMemoryError\u001B[39m: CUDA out of memory. Tried to allocate 5.79 GiB. GPU 0 has a total capacity of 7.60 GiB of which 2.95 GiB is free. Including non-PyTorch memory, this process has 3.60 GiB memory in use. Of the allocated memory 3.36 GiB is allocated by PyTorch, and 104.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7c8a44725f33439e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
