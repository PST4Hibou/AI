{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-19T13:59:00.554551Z",
     "start_time": "2025-11-19T13:59:00.550665Z"
    }
   },
   "source": [
    "# -----------------------------\n",
    "# 1. Imports\n",
    "# -----------------------------\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, load_from_disk  #\n",
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification #\n",
    "from functools import partial #\n",
    "import torchaudio.transforms as T\n",
    "import torchmetrics\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Configuration & Setup\n",
    "# -----------------------------\n",
    "SEED = 42\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 24\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LEARNING_RATE = 5e-5\n",
    "WEIGHT_DECAY = 1e-4\n",
    "MODEL_SAVE_PATH = \"./best_finetuned_model.pt\"\n",
    "EPOCHS = 10\n",
    "\n",
    "MODEL_CHECKPOINT = \"MIT/ast-finetuned-speech-commands-v2\"\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Number of workers: {NUM_WORKERS}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Number of workers: 24\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T14:01:01.666820Z",
     "start_time": "2025-11-19T13:59:01.138006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Loading feature extractor for: {MODEL_CHECKPOINT}\")\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "print(f\"Loading model for: {MODEL_CHECKPOINT}\")\n",
    "# We set num_labels=1 to match your original BCEWithLogitsLoss setup\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    MODEL_CHECKPOINT,\n",
    "    num_labels=1,\n",
    "    ignore_mismatched_sizes=True # Allow replacing the head\n",
    ").to(DEVICE)"
   ],
   "id": "9d031aac3c86637b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading feature extractor for: MIT/ast-finetuned-speech-commands-v2\n",
      "Loading model for: MIT/ast-finetuned-speech-commands-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-speech-commands-v2 and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.bias: found shape torch.Size([35]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([35, 768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T14:03:32.368919Z",
     "start_time": "2025-11-19T14:03:31.169132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------\n",
    "# 3. Load Dataset\n",
    "# -----------------------------\n",
    "# Assuming the dataset is in a directory relative to the notebook\n",
    "# Using os.path.join for better cross-platform compatibility\n",
    "# DATASET_PATH = \"../dataset/ds_2_noaugment_test.hf\"\n",
    "DATASET_PATH = \"../../dataset/ds_3_raw_chunked.hf\"\n",
    "ONLINE_PATH = None\n",
    "\n",
    "if ONLINE_PATH is None:\n",
    "    if not os.path.exists(DATASET_PATH):\n",
    "        print(f\"Error: Dataset path not found at {DATASET_PATH}\")\n",
    "        print(\"Please update the DATASET_PATH variable to point to your dataset directory.\")\n",
    "    else:\n",
    "        dataset = load_from_disk(DATASET_PATH)\n",
    "        print(\"Dataset loaded successfully.\")\n",
    "\n",
    "        # Set format to PyTorch\n",
    "        dataset = dataset.with_format(\"torch\", columns=[\"audio\", \"label\"])\n",
    "\n",
    "        print(\"\\nDataset splits:\")\n",
    "        print({k: v.shape for k, v in dataset.items()})\n",
    "\n",
    "        print(\"\\nDataset features:\")\n",
    "        print(dataset[\"train\"].features)"
   ],
   "id": "bbe1e6a50003fbdc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "\n",
      "Dataset splits:\n",
      "{'train': (617500, 2), 'val': (78726, 2), 'test': (78483, 2)}\n",
      "\n",
      "Dataset features:\n",
      "{'audio': List(Value('float32')), 'label': ClassLabel(names=['other', 'drone'])}\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T14:27:10.989857Z",
     "start_time": "2025-11-19T14:27:10.987018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spec_augmentations = nn.Sequential(\n",
    "    T.FrequencyMasking(freq_mask_param=5),\n",
    "    T.TimeMasking(time_mask_param=5)\n",
    ").to(DEVICE)\n",
    "\n",
    "def collate_fn_finetune(batch, augment=False):\n",
    "    # 1. Extract audio arrays and labels from the batch\n",
    "    xs = [b[\"audio\"] for b in batch]\n",
    "    ys = [b[\"label\"] for b in batch]\n",
    "\n",
    "    inputs = feature_extractor(\n",
    "        xs,\n",
    "        sampling_rate=feature_extractor.sampling_rate,\n",
    "        padding=\"max_length\", # Pad to model's max (e.g., 10s)\n",
    "        truncation=True,\n",
    "        # max_length=16000 * 10, # Explicitly set to 10s\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # 'input_values' are the spectrograms: (B, N_MELS, N_FRAMES)\n",
    "    # e.g., (32, 128, 1024) for AST\n",
    "    input_values = inputs[\"input_values\"]\n",
    "\n",
    "    labels = torch.tensor(ys, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    return input_values, labels\n",
    "\n",
    "# --- Create partial functions for train/validation ---\n",
    "train_collate_fn = partial(collate_fn_finetune, augment=True)\n",
    "val_collate_fn = partial(collate_fn_finetune, augment=False)"
   ],
   "id": "9a803be5ea3a74fe",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T14:27:12.433028Z",
     "start_time": "2025-11-19T14:27:11.609224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(dataset[\"train\"], batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, collate_fn=train_collate_fn, pin_memory=True)\n",
    "valid_loader = DataLoader(dataset[\"val\"], batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, collate_fn=val_collate_fn, pin_memory=True)\n",
    "test_loader = DataLoader(dataset[\"test\"], batch_size=BATCH_SIZE, shuffle=False,\n",
    "                         num_workers=NUM_WORKERS, collate_fn=val_collate_fn, pin_memory=True)\n",
    "\n",
    "print(f\"\\nCreated DataLoaders with Batch Size: {BATCH_SIZE}\")\n",
    "\n",
    "# Check a sample batch\n",
    "try:\n",
    "    sample_x, sample_y = next(iter(train_loader))\n",
    "    print(f\"Sample batch shape - X (Spectrograms): {sample_x.shape}, Y (Labels): {sample_y.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load a sample batch: {e}\")\n",
    "    print(\"Check your NUM_WORKERS setting or collate_fn.\")"
   ],
   "id": "e13ca3ec6f17593d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created DataLoaders with Batch Size: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f7541d30d60>Exception ignored in: \n",
      "Traceback (most recent call last):\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7f7541d30d60>\n",
      "  File \"/home/pierre/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pierre/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
      "        self._shutdown_workers()self._shutdown_workers()\n",
      "\n",
      "  File \"/home/pierre/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py\", line 1637, in _shutdown_workers\n",
      "  File \"/home/pierre/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py\", line 1637, in _shutdown_workers\n",
      "        if w.is_alive():if w.is_alive():\n",
      "\n",
      "  File \"/home/pierre/Applications/miniconda3/lib/python3.13/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/home/pierre/Applications/miniconda3/lib/python3.13/multiprocessing/process.py\", line 160, in is_alive\n",
      "        assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      "AssertionErrorAssertionError: : can only test a child processcan only test a child process\n",
      "\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f7541d30d60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pierre/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/pierre/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py\", line 1637, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/pierre/Applications/miniconda3/lib/python3.13/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f7541d30d60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pierre/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/pierre/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py\", line 1637, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/pierre/Applications/miniconda3/lib/python3.13/multiprocessing/process.py\", line 160, in is_alive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load a sample batch: Caught AssertionError in DataLoader worker process 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/pierre/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n",
      "    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n",
      "  File \"/home/pierre/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n",
      "    return self.collate_fn(data)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^\n",
      "  File \"/tmp/ipykernel_52595/1621439837.py\", line 11, in collate_fn_finetune\n",
      "    inputs = feature_extractor(\n",
      "        xs,\n",
      "    ...<4 lines>...\n",
      "        return_tensors=\"pt\"\n",
      "    )\n",
      "  File \"/home/pierre/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/transformers/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.py\", line 219, in __call__\n",
      "    features = [self._extract_fbank_features(waveform, max_length=self.max_length) for waveform in raw_speech]\n",
      "                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pierre/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/transformers/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.py\", line 119, in _extract_fbank_features\n",
      "    fbank = ta_kaldi.fbank(\n",
      "        waveform,\n",
      "    ...<2 lines>...\n",
      "        num_mel_bins=self.num_mel_bins,\n",
      "    )\n",
      "  File \"/home/pierre/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torchaudio/compliance/kaldi.py\", line 591, in fbank\n",
      "    waveform, window_shift, window_size, padded_window_size = _get_waveform_and_window_properties(\n",
      "                                                              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        waveform, channel, sample_frequency, frame_shift, frame_length, round_to_power_of_two, preemphasis_coefficient\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/pierre/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torchaudio/compliance/kaldi.py\", line 142, in _get_waveform_and_window_properties\n",
      "    assert 2 <= window_size <= len(waveform), \"choose a window size {} that is [2, {}]\".format(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: choose a window size 400 that is [2, 32]\n",
      "\n",
      "Check your NUM_WORKERS setting or collate_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f7541d30d60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pierre/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/pierre/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py\", line 1637, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/pierre/Applications/miniconda3/lib/python3.13/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f7541d30d60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pierre/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/pierre/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py\", line 1637, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/pierre/Applications/miniconda3/lib/python3.13/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f7541d30d60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pierre/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/pierre/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py\", line 1637, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/pierre/Applications/miniconda3/lib/python3.13/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "90abc715da6f5b27"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
