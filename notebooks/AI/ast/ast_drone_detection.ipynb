{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T09:49:48.622850Z",
     "start_time": "2025-11-24T09:49:48.618991Z"
    }
   },
   "source": [
    "from IPython.lib.display import Audio as AudioDisplay\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    ASTFeatureExtractor,\n",
    "    ASTConfig,\n",
    "    ASTForAudioClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "# Set style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"All imports successful\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T09:26:09.508305Z",
     "start_time": "2025-11-24T09:26:09.504623Z"
    }
   },
   "source": [
    "# ============================\n",
    "# CONFIGURATION\n",
    "# ============================\n",
    "\n",
    "# Paths\n",
    "DATASET_PATH = \"../../dataset/ds_3_raw_chunked.hf\"\n",
    "OUTPUT_DIR = \"./runs/ast_drone_detection\"\n",
    "MODEL_SAVE_PATH = \"./best_ast_drone_model.pt\"\n",
    "\n",
    "# Model\n",
    "PRETRAINED_MODEL = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "\n",
    "# Training hyperparameters\n",
    "SEED = 42\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 5e-5\n",
    "WEIGHT_DECAY = 1e-4\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "# System\n",
    "NUM_WORKERS = 24\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Create output directory\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Output directory: ./runs/ast_drone_detection\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set Random Seeds\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T09:26:12.946501Z",
     "start_time": "2025-11-24T09:26:12.941805Z"
    }
   },
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(SEED)\n",
    "print(f\"Random seed set to {SEED}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 42\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T09:57:30.411013Z",
     "start_time": "2025-11-24T09:57:30.363225Z"
    }
   },
   "source": [
    "# Load the preprocessed dataset\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    raise FileNotFoundError(f\"Dataset not found at {DATASET_PATH}\")\n",
    "\n",
    "dataset = load_from_disk(DATASET_PATH)\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"\\nDataset structure:\")\n",
    "labels = dataset[\"train\"].features[\"label\"]\n",
    "print(f\"\\nDataset splits:\")\n",
    "for split in dataset.keys():\n",
    "    print(f\"\\t{split}: {len(dataset[split])} samples\")\n",
    "\n",
    "print(f\"\\nFeatures:\")\n",
    "print(dataset[\"train\"].features)\n",
    "\n",
    "# Check a sample\n",
    "sample = dataset[\"train\"][0]\n",
    "print(f\"\\nSample data:\")\n",
    "print(f\"\\tAudio shape: {np.array(sample['audio']).shape}\")\n",
    "print(f\"\\tLabel: {sample['label']} ({CLASS_NAMES[sample['label']]})\")\n",
    "\n",
    "AudioDisplay(sample['audio'], rate=16000)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "\n",
      "Dataset structure:\n",
      "\n",
      "Dataset splits:\n",
      "\ttrain: 617500 samples\n",
      "\tval: 78726 samples\n",
      "\ttest: 78483 samples\n",
      "\n",
      "Features:\n",
      "{'audio': List(Value('float32')), 'label': ClassLabel(names=['other', 'drone'])}\n",
      "\n",
      "Sample data:\n",
      "\tAudio shape: (8000,)\n",
      "\tLabel: 0 (other)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ],
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T09:58:19.466317Z",
     "start_time": "2025-11-24T09:58:14.259243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Count the number of items per class\n",
    "from collections import Counter\n",
    "label_counts = Counter(dataset[\"train\"][\"label\"])\n",
    "print(label_counts)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 441442, 1: 176058})\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Feature Extractor and Model\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T09:19:45.171959Z",
     "start_time": "2025-11-24T09:19:44.829988Z"
    }
   },
   "source": [
    "# Load feature extractor\n",
    "print(f\"Loading feature extractor from {PRETRAINED_MODEL}...\")\n",
    "feature_extractor = ASTFeatureExtractor.from_pretrained(PRETRAINED_MODEL)\n",
    "\n",
    "SAMPLING_RATE = feature_extractor.sampling_rate\n",
    "MODEL_INPUT_NAME = feature_extractor.model_input_names[0]\n",
    "\n",
    "print(f\"✅ Feature extractor loaded\")\n",
    "print(f\"  Sampling rate: {SAMPLING_RATE} Hz\")\n",
    "print(f\"  Model input name: {MODEL_INPUT_NAME}\")\n",
    "print(f\"  Max length: {feature_extractor.max_length}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading feature extractor from MIT/ast-finetuned-audioset-10-10-0.4593...\n",
      "✅ Feature extractor loaded\n",
      "  Sampling rate: 16000 Hz\n",
      "  Model input name: input_values\n",
      "  Max length: 1024\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T22:16:37.760448Z",
     "start_time": "2025-11-23T22:16:37.199587Z"
    }
   },
   "source": [
    "# Load and configure model\n",
    "print(f\"\\nLoading model from {PRETRAINED_MODEL}...\")\n",
    "\n",
    "config = ASTConfig.from_pretrained(PRETRAINED_MODEL)\n",
    "config.num_labels = NUM_LABELS\n",
    "config.label2id = LABEL2ID\n",
    "config.id2label = ID2LABEL\n",
    "\n",
    "model = ASTForAudioClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL,\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True  # Replaces the classification head\n",
    ")\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "print(f\"✅ Model loaded and moved to {DEVICE}\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model from MIT/ast-finetuned-audioset-10-10-0.4593...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 203/203 [00:00<00:00, 1194.61it/s, Materializing param=audio_spectrogram_transformer.embeddings.cls_token]                            \n",
      "ASTForAudioClassification LOAD REPORT from: MIT/ast-finetuned-audioset-10-10-0.4593\n",
      "Key                     | Status   |                                                                                       \n",
      "------------------------+----------+---------------------------------------------------------------------------------------\n",
      "classifier.dense.bias   | MISMATCH | Reinit due to size mismatch ckpt: torch.Size([527]) vs model:torch.Size([2])          \n",
      "classifier.dense.weight | MISMATCH | Reinit due to size mismatch ckpt: torch.Size([527, 768]) vs model:torch.Size([2, 768])\n",
      "classifier.dense.weight | MISC     | 'Linear' object has no attribute 'param_name'\n",
      "Error when processing parameter cl      \n",
      "classifier.dense.bias   | MISC     | 'Linear' object has no attribute 'param_name'\n",
      "Error when processing parameter cl      \n",
      "\n",
      "Notes:\n",
      "- MISMATCH\t:ckpt weights were loaded, but they did not match the original empty weight.\n",
      "- MISC\t:originate from the conversion scheme\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded and moved to cuda\n",
      "  Total parameters: 86,190,338\n",
      "  Trainable parameters: 86,190,338\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Preprocessing\n",
    "\n",
    "We need to convert spectrograms back to waveforms for AST processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T22:16:38.849754Z",
     "start_time": "2025-11-23T22:16:38.845942Z"
    }
   },
   "source": [
    "def spectrogram_to_waveform(spectrogram, sr=16000, n_fft=2048, hop_length=512):\n",
    "    \"\"\"\n",
    "    Convert a magnitude spectrogram back to a waveform using Griffin-Lim.\n",
    "    \n",
    "    Args:\n",
    "        spectrogram: 2D array (freq_bins, time_frames) in dB scale\n",
    "        sr: sampling rate\n",
    "        n_fft: FFT window size\n",
    "        hop_length: hop length for STFT\n",
    "    \n",
    "    Returns:\n",
    "        waveform: 1D array\n",
    "    \"\"\"\n",
    "    # Convert from dB to linear magnitude\n",
    "    magnitude = librosa.db_to_amplitude(spectrogram)\n",
    "    \n",
    "    # Use Griffin-Lim to reconstruct phase and waveform\n",
    "    waveform = librosa.griffinlim(\n",
    "        magnitude,\n",
    "        n_iter=32,\n",
    "        hop_length=hop_length,\n",
    "        n_fft=n_fft\n",
    "    )\n",
    "    \n",
    "    return waveform\n",
    "\n",
    "def preprocess_function(batch):\n",
    "    print(batch)\n",
    "    \"\"\"\n",
    "    Preprocess batch of spectrograms for AST model.\n",
    "    Converts spectrograms to waveforms and extracts features.\n",
    "    \"\"\"\n",
    "    waveforms = []\n",
    "    \n",
    "    for spec in batch[\"audio\"]:\n",
    "        # Convert list to numpy array\n",
    "        spec_array = np.array(spec, dtype=np.float32)\n",
    "        \n",
    "        # Convert spectrogram to waveform\n",
    "        waveform = spectrogram_to_waveform(spec_array, sr=SAMPLING_RATE)\n",
    "        \n",
    "        waveforms.append(waveform)\n",
    "    \n",
    "    # Extract features using AST feature extractor\n",
    "    inputs = feature_extractor(\n",
    "        waveforms,\n",
    "        sampling_rate=SAMPLING_RATE,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=SAMPLING_RATE * 10  # 10 seconds max\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        MODEL_INPUT_NAME: inputs[MODEL_INPUT_NAME],\n",
    "        \"labels\": batch[\"label\"]\n",
    "    }\n",
    "\n",
    "print(\"✅ Preprocessing function defined\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessing function defined\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T22:16:39.427294Z",
     "start_time": "2025-11-23T22:16:39.412122Z"
    }
   },
   "source": [
    "# Use a smaller subset for faster iteration during development\n",
    "USE_SUBSET = True  # Set to False for full dataset\n",
    "SUBSET_SIZE = 5000 if USE_SUBSET else None\n",
    "\n",
    "if USE_SUBSET:\n",
    "    print(f\"⚠️  Using subset of {SUBSET_SIZE} samples for faster development\")\n",
    "    dataset[\"train\"] = dataset[\"train\"].select(range(min(SUBSET_SIZE, len(dataset[\"train\"]))))\n",
    "    dataset[\"val\"] = dataset[\"val\"].select(range(min(1000, len(dataset[\"val\"]))))\n",
    "    dataset[\"test\"] = dataset[\"test\"].select(range(min(1000, len(dataset[\"test\"]))))\n",
    "\n",
    "# NO set_transform! Preprocessing will be done in the collator\n",
    "print(\"\\n✅ Dataset subsets selected\")\n",
    "print(f\"\\nFinal dataset sizes:\")\n",
    "for split in dataset.keys():\n",
    "    print(f\"  {split}: {len(dataset[split])} samples\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Using subset of 5000 samples for faster development\n",
      "\n",
      "✅ Dataset subsets selected\n",
      "\n",
      "Final dataset sizes:\n",
      "  train: 5000 samples\n",
      "  val: 1000 samples\n",
      "  test: 1000 samples\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Define Data Collator\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T22:16:40.646955Z",
     "start_time": "2025-11-23T22:16:40.642223Z"
    }
   },
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class ASTDataCollatorWithPreprocessing:\n",
    "    \"\"\"Custom data collator that preprocesses spectrograms on-the-fly.\"\"\"\n",
    "    \n",
    "    feature_extractor: ASTFeatureExtractor\n",
    "    \n",
    "    def __call__(self, features: List[Dict[str, any]]) -> Dict[str, torch.Tensor]:\n",
    "        # Extract raw spectrograms and labels\n",
    "        spectrograms = [f[\"audio\"] for f in features]\n",
    "        labels = [f[\"label\"] for f in features]\n",
    "        \n",
    "        # Convert spectrograms to waveforms using Griffin-Lim\n",
    "        waveforms = []\n",
    "        for spec in spectrograms:\n",
    "            # Convert to numpy array\n",
    "            spec_array = np.array(spec, dtype=np.float32)\n",
    "            \n",
    "            # Convert from dB to linear magnitude\n",
    "            magnitude = librosa.db_to_amplitude(spec_array)\n",
    "            \n",
    "            # Use Griffin-Lim to reconstruct waveform\n",
    "            waveform = librosa.griffinlim(\n",
    "                magnitude,\n",
    "                n_iter=32,\n",
    "                hop_length=512,\n",
    "                n_fft=2048\n",
    "            )\n",
    "            \n",
    "            waveforms.append(waveform)\n",
    "        \n",
    "        # Extract AST features\n",
    "        inputs = self.feature_extractor(\n",
    "            waveforms,\n",
    "            sampling_rate=SAMPLING_RATE,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=SAMPLING_RATE * 10  # 10 seconds max\n",
    "        )\n",
    "        \n",
    "        # Prepare batch\n",
    "        batch = {\n",
    "            MODEL_INPUT_NAME: inputs[MODEL_INPUT_NAME],\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "        return batch\n",
    "\n",
    "# Create the new collator\n",
    "collator = ASTDataCollatorWithPreprocessing(feature_extractor=feature_extractor)\n",
    "print(\"✅ Custom data collator with preprocessing defined\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Custom data collator with preprocessing defined\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Define Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T22:16:44.407927Z",
     "start_time": "2025-11-23T22:16:41.735793Z"
    }
   },
   "source": [
    "# Load metrics\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute metrics for evaluation.\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    \n",
    "    metrics = {}\n",
    "    metrics.update(accuracy_metric.compute(predictions=predictions, references=labels))\n",
    "    metrics.update(precision_metric.compute(predictions=predictions, references=labels, average=\"binary\"))\n",
    "    metrics.update(recall_metric.compute(predictions=predictions, references=labels, average=\"binary\"))\n",
    "    metrics.update(f1_metric.compute(predictions=predictions, references=labels, average=\"binary\"))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"✅ Metrics defined\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Metrics defined\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Configure Training\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T22:16:45.889664Z",
     "start_time": "2025-11-23T22:16:45.860831Z"
    }
   },
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    # Training\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # Logging\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",  # Disable tensorboard (install tensorboard if you want logging)\n",
    "    \n",
    "    # System\n",
    "    dataloader_num_workers=NUM_WORKERS,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_pin_memory=True if NUM_WORKERS > 0 else False,\n",
    "    \n",
    "    # Saving\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # Seed\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(\"✅ Training arguments configured\")\n",
    "print(f\"\\nTraining configuration:\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"  Warmup ratio: {WARMUP_RATIO}\")\n",
    "print(f\"  FP16: {training_args.fp16}\")\n",
    "print(f\"  Workers: {NUM_WORKERS}\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training arguments configured\n",
      "\n",
      "Training configuration:\n",
      "  Epochs: 10\n",
      "  Batch size: 32\n",
      "  Learning rate: 5e-05\n",
      "  Weight decay: 0.0001\n",
      "  Warmup ratio: 0.1\n",
      "  FP16: True\n",
      "  Workers: 24\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Initialize Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T22:16:48.371669Z",
     "start_time": "2025-11-23T22:16:48.358289Z"
    }
   },
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"val\"],\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"✅ Trainer initialized\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Trainer initialized\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T22:16:50.905939Z",
     "start_time": "2025-11-23T22:16:49.644649Z"
    }
   },
   "source": [
    "print(\"Starting training...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Training complete!\\n\")\n",
    "\n",
    "# Save final model\n",
    "trainer.save_model(OUTPUT_DIR + \"/final_model\")\n",
    "feature_extractor.save_pretrained(OUTPUT_DIR + \"/final_model\")\n",
    "\n",
    "# Save training metrics\n",
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "\n",
    "print(f\"✅ Model saved to {OUTPUT_DIR}/final_model\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/pierre/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/pierre/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ~~~~~~~~~~~~~~~^^^^^^\n  File \"/tmp/ipykernel_74369/2595779619.py\", line 12, in __call__\n    spectrograms = [f[\"audio\"] for f in features]\n                    ~^^^^^^^^^\nKeyError: 'audio'\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[13]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mStarting training...\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m      2\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m=\u001B[39m\u001B[33m\"\u001B[39m * \u001B[32m80\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m train_result = \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m + \u001B[33m\"\u001B[39m\u001B[33m=\u001B[39m\u001B[33m\"\u001B[39m * \u001B[32m80\u001B[39m)\n\u001B[32m      7\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mTraining complete!\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/transformers/trainer.py:2155\u001B[39m, in \u001B[36mTrainer.train\u001B[39m\u001B[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[39m\n\u001B[32m   2153\u001B[39m         hf_hub_utils.enable_progress_bars()\n\u001B[32m   2154\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2155\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2156\u001B[39m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2157\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2158\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2159\u001B[39m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2160\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/transformers/trainer.py:2430\u001B[39m, in \u001B[36mTrainer._inner_training_loop\u001B[39m\u001B[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[39m\n\u001B[32m   2428\u001B[39m update_step += \u001B[32m1\u001B[39m\n\u001B[32m   2429\u001B[39m num_batches = args.gradient_accumulation_steps \u001B[38;5;28;01mif\u001B[39;00m update_step != (total_updates - \u001B[32m1\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m remainder\n\u001B[32m-> \u001B[39m\u001B[32m2430\u001B[39m batch_samples, num_items_in_batch = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_batch_samples\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch_iterator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_batches\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2431\u001B[39m \u001B[38;5;66;03m# Store the number of batches for current gradient accumulation\u001B[39;00m\n\u001B[32m   2432\u001B[39m \u001B[38;5;66;03m# This is used to correctly scale the loss when the last accumulation step has fewer batches\u001B[39;00m\n\u001B[32m   2433\u001B[39m \u001B[38;5;28mself\u001B[39m.current_gradient_accumulation_steps = \u001B[38;5;28mlen\u001B[39m(batch_samples)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/transformers/trainer.py:5154\u001B[39m, in \u001B[36mTrainer.get_batch_samples\u001B[39m\u001B[34m(self, epoch_iterator, num_batches, device)\u001B[39m\n\u001B[32m   5152\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_batches):\n\u001B[32m   5153\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m5154\u001B[39m         batch_samples.append(\u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mepoch_iterator\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m   5155\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[32m   5156\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/accelerate/data_loader.py:567\u001B[39m, in \u001B[36mDataLoaderShard.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    565\u001B[39m \u001B[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001B[39;00m\n\u001B[32m    566\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m567\u001B[39m     current_batch = \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdataloader_iter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    568\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[32m    569\u001B[39m     \u001B[38;5;28mself\u001B[39m.end()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:732\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    729\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    730\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    731\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m732\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    733\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    734\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    735\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    736\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    737\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    738\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1506\u001B[39m, in \u001B[36m_MultiProcessingDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1504\u001B[39m worker_id = \u001B[38;5;28mself\u001B[39m._task_info.pop(idx)[\u001B[32m0\u001B[39m]\n\u001B[32m   1505\u001B[39m \u001B[38;5;28mself\u001B[39m._rcvd_idx += \u001B[32m1\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1506\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_process_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mworker_id\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1541\u001B[39m, in \u001B[36m_MultiProcessingDataLoaderIter._process_data\u001B[39m\u001B[34m(self, data, worker_idx)\u001B[39m\n\u001B[32m   1539\u001B[39m \u001B[38;5;28mself\u001B[39m._try_put_index()\n\u001B[32m   1540\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, ExceptionWrapper):\n\u001B[32m-> \u001B[39m\u001B[32m1541\u001B[39m     \u001B[43mdata\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1542\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/_utils.py:769\u001B[39m, in \u001B[36mExceptionWrapper.reraise\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    765\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m    766\u001B[39m     \u001B[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001B[39;00m\n\u001B[32m    767\u001B[39m     \u001B[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001B[39;00m\n\u001B[32m    768\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m769\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m exception\n",
      "\u001B[31mKeyError\u001B[39m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/pierre/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/pierre/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ~~~~~~~~~~~~~~~^^^^^^\n  File \"/tmp/ipykernel_74369/2595779619.py\", line 12, in __call__\n    spectrograms = [f[\"audio\"] for f in features]\n                    ~^^^^^^^^^\nKeyError: 'audio'\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Evaluate on Validation Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating on validation set...\\n\")\n",
    "\n",
    "eval_metrics = trainer.evaluate(eval_dataset=dataset[\"val\"])\n",
    "\n",
    "trainer.log_metrics(\"eval\", eval_metrics)\n",
    "trainer.save_metrics(\"eval\", eval_metrics)\n",
    "\n",
    "print(\"\\nValidation Metrics:\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in eval_metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        metric_name = key.replace(\"eval_\", \"\")\n",
    "        print(f\"{metric_name:20s}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Test Set Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating on test set...\\n\")\n",
    "\n",
    "# Get predictions\n",
    "predictions_output = trainer.predict(dataset[\"test\"])\n",
    "predictions = np.argmax(predictions_output.predictions, axis=1)\n",
    "labels = predictions_output.label_ids\n",
    "\n",
    "# Compute metrics\n",
    "test_accuracy = accuracy_score(labels, predictions)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    labels, predictions, average=\"binary\"\n",
    ")\n",
    "\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Accuracy:  {test_accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Detailed Classification Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(\"=\" * 80)\n",
    "print(classification_report(\n",
    "    labels,\n",
    "    predictions,\n",
    "    target_names=CLASS_NAMES,\n",
    "    digits=4\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(labels, predictions)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=CLASS_NAMES,\n",
    "    yticklabels=CLASS_NAMES,\n",
    "    cbar_kws={'label': 'Count'}\n",
    ")\n",
    "plt.title('Confusion Matrix - Test Set', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/confusion_matrix.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✅ Confusion matrix saved to {OUTPUT_DIR}/confusion_matrix.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Training History Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training history from logs\n",
    "import json\n",
    "\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Extract metrics\n",
    "train_loss = []\n",
    "eval_loss = []\n",
    "eval_accuracy = []\n",
    "eval_f1 = []\n",
    "epochs = []\n",
    "\n",
    "for entry in log_history:\n",
    "    if 'loss' in entry and 'epoch' in entry:\n",
    "        train_loss.append(entry['loss'])\n",
    "    if 'eval_loss' in entry:\n",
    "        eval_loss.append(entry['eval_loss'])\n",
    "        eval_accuracy.append(entry.get('eval_accuracy', 0))\n",
    "        eval_f1.append(entry.get('eval_f1', 0))\n",
    "        epochs.append(entry['epoch'])\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Loss plot\n",
    "if eval_loss:\n",
    "    axes[0].plot(epochs, eval_loss, marker='o', linewidth=2, label='Validation Loss')\n",
    "    axes[0].set_title('Loss vs. Epochs', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Metrics plot\n",
    "if eval_accuracy and eval_f1:\n",
    "    axes[1].plot(epochs, eval_accuracy, marker='o', linewidth=2, label='Accuracy')\n",
    "    axes[1].plot(epochs, eval_f1, marker='s', linewidth=2, label='F1 Score')\n",
    "    axes[1].set_title('Metrics vs. Epochs', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('Score', fontsize=12)\n",
    "    axes[1].legend(fontsize=10)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/training_history.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✅ Training history saved to {OUTPUT_DIR}/training_history.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Per-Class Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class metrics\n",
    "precision_per_class, recall_per_class, f1_per_class, support = precision_recall_fscore_support(\n",
    "    labels, predictions, average=None\n",
    ")\n",
    "\n",
    "print(\"\\nPer-Class Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Class':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}\")\n",
    "print(\"-\" * 80)\n",
    "for i, class_name in enumerate(CLASS_NAMES):\n",
    "    print(f\"{class_name:<15} {precision_per_class[i]:<12.4f} {recall_per_class[i]:<12.4f} \"\n",
    "          f\"{f1_per_class[i]:<12.4f} {support[i]:<10}\")\n",
    "\n",
    "# Visualize per-class metrics\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(CLASS_NAMES))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar(x - width, precision_per_class, width, label='Precision', alpha=0.8)\n",
    "ax.bar(x, recall_per_class, width, label='Recall', alpha=0.8)\n",
    "ax.bar(x + width, f1_per_class, width, label='F1-Score', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Class', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Per-Class Metrics', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(CLASS_NAMES)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_ylim([0, 1.1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/per_class_metrics.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✅ Per-class metrics saved to {OUTPUT_DIR}/per_class_metrics.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Prediction Confidence Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction probabilities\n",
    "import torch.nn.functional as F\n",
    "\n",
    "logits = predictions_output.predictions\n",
    "probabilities = F.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "\n",
    "# Get max probability (confidence) for each prediction\n",
    "confidences = np.max(probabilities, axis=1)\n",
    "\n",
    "# Separate correct and incorrect predictions\n",
    "correct_mask = predictions == labels\n",
    "correct_confidences = confidences[correct_mask]\n",
    "incorrect_confidences = confidences[~correct_mask]\n",
    "\n",
    "print(f\"\\nPrediction Confidence Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Correct predictions:   {len(correct_confidences)} (avg confidence: {correct_confidences.mean():.4f})\")\n",
    "print(f\"Incorrect predictions: {len(incorrect_confidences)} (avg confidence: {incorrect_confidences.mean() if len(incorrect_confidences) > 0 else 0:.4f})\")\n",
    "\n",
    "# Plot confidence distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Histogram\n",
    "if len(correct_confidences) > 0:\n",
    "    axes[0].hist(correct_confidences, bins=30, alpha=0.7, label='Correct', color='green')\n",
    "if len(incorrect_confidences) > 0:\n",
    "    axes[0].hist(incorrect_confidences, bins=30, alpha=0.7, label='Incorrect', color='red')\n",
    "axes[0].set_xlabel('Confidence', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_title('Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "box_data = [correct_confidences]\n",
    "box_labels = ['Correct']\n",
    "if len(incorrect_confidences) > 0:\n",
    "    box_data.append(incorrect_confidences)\n",
    "    box_labels.append('Incorrect')\n",
    "\n",
    "axes[1].boxplot(\n",
    "    box_data,\n",
    "    labels=box_labels,\n",
    "    patch_artist=True,\n",
    "    boxprops=dict(facecolor='lightblue', alpha=0.7)\n",
    ")\n",
    "axes[1].set_ylabel('Confidence', fontsize=12)\n",
    "axes[1].set_title('Confidence Box Plot', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/confidence_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✅ Confidence analysis saved to {OUTPUT_DIR}/confidence_analysis.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Save Final Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model state dict\n",
    "torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "print(f\"✅ Model state dict saved to {MODEL_SAVE_PATH}\")\n",
    "\n",
    "# Save complete model info\n",
    "model_info = {\n",
    "    'model_name': PRETRAINED_MODEL,\n",
    "    'num_labels': NUM_LABELS,\n",
    "    'class_names': CLASS_NAMES,\n",
    "    'label2id': LABEL2ID,\n",
    "    'id2label': ID2LABEL,\n",
    "    'test_accuracy': float(test_accuracy),\n",
    "    'test_precision': float(precision),\n",
    "    'test_recall': float(recall),\n",
    "    'test_f1': float(f1),\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(f\"{OUTPUT_DIR}/model_info.json\", 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print(f\"✅ Model info saved to {OUTPUT_DIR}/model_info.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nModel: {PRETRAINED_MODEL}\")\n",
    "print(f\"Task: Binary Classification (Drone Detection)\")\n",
    "print(f\"Classes: {CLASS_NAMES}\")\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Train samples: {len(dataset['train'])}\")\n",
    "print(f\"  Val samples:   {len(dataset['val'])}\")\n",
    "print(f\"  Test samples:  {len(dataset['test'])}\")\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Epochs:        {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size:    {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Weight decay:  {WEIGHT_DECAY}\")\n",
    "print(f\"\\nFinal Test Set Performance:\")\n",
    "print(f\"  Accuracy:  {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1 Score:  {f1:.4f}\")\n",
    "print(f\"\\nOutput Files:\")\n",
    "print(f\"  Model directory:     {OUTPUT_DIR}/final_model\")\n",
    "print(f\"  Model state dict:    {MODEL_SAVE_PATH}\")\n",
    "print(f\"  Confusion matrix:    {OUTPUT_DIR}/confusion_matrix.png\")\n",
    "print(f\"  Training history:    {OUTPUT_DIR}/training_history.png\")\n",
    "print(f\"  Per-class metrics:   {OUTPUT_DIR}/per_class_metrics.png\")\n",
    "print(f\"  Confidence analysis: {OUTPUT_DIR}/confidence_analysis.png\")\n",
    "print(f\"  Model info:          {OUTPUT_DIR}/model_info.json\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✅ TRAINING COMPLETE!\")\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
