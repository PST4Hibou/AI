{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-19T22:12:45.482976Z",
     "start_time": "2025-11-19T22:12:42.266116Z"
    }
   },
   "source": [
    "import os\n",
    "import math\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "from datasets import load_dataset, Audio, ClassLabel\n",
    "from transformers import (\n",
    "ASTFeatureExtractor,\n",
    "ASTConfig,\n",
    "ASTForAudioClassification,\n",
    "TrainingArguments,\n",
    "Trainer,\n",
    ")\n",
    "\n",
    "\n",
    "import evaluate\n",
    "from audiomentations import Compose, AddGaussianSNR, GainTransition, Gain, ClippingDistortion, TimeStretch, PitchShift\n",
    "\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "\n",
    "# Where to save outputs\n",
    "OUT_DIR = Path(\"./runs/ast_esc50\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pierre/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T22:12:46.969473Z",
     "start_time": "2025-11-19T22:12:45.622861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Loading dataset ashraq/esc50 from the Hub...\")\n",
    "dataset = load_dataset(\"ashraq/esc50\", split=\"train\")\n",
    "print(dataset)"
   ],
   "id": "4cd347ce5494ecf4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset ashraq/esc50 from the Hub...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['filename', 'fold', 'target', 'category', 'esc10', 'src_file', 'take', 'audio'],\n",
      "    num_rows: 2000\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T22:12:47.829534Z",
     "start_time": "2025-11-19T22:12:47.823654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# fetch mapping from the dataset\n",
    "# create class names ordered by label id\n",
    "df = dataset.select_columns([\"target\", \"category\"]).to_pandas()\n",
    "unique_idx = np.unique(df[\"target\"], return_index=True)[1]\n",
    "class_names = df.iloc[unique_idx][\"category\"].tolist()"
   ],
   "id": "f8a401179721e098",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T22:12:48.931314Z",
     "start_time": "2025-11-19T22:12:48.917363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_labels = len(class_names)\n",
    "print(f\"Found {num_labels} classes\")\n",
    "\n",
    "\n",
    "from datasets import Features, Value\n",
    "\n",
    "\n",
    "# rename & cast\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "dataset = dataset.rename_column(\"target\", \"labels\")\n",
    "\n",
    "\n",
    "# Convert labels to ClassLabel so Trainer can understand\n",
    "label_feature = ClassLabel(names=class_names)\n",
    "dataset = dataset.cast_column(\"labels\", label_feature)\n",
    "\n",
    "\n",
    "# Build label2id and id2label\n",
    "label2id = {name: i for i, name in enumerate(class_names)}\n",
    "id2label = {i: name for name, i in label2id.items()} # note: label_feature already has mapping\n",
    "\n",
    "\n",
    "print(label2id)"
   ],
   "id": "2f9d9c0c94e4159",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 classes\n",
      "{'dog': 0, 'rooster': 1, 'pig': 2, 'cow': 3, 'frog': 4, 'cat': 5, 'hen': 6, 'insects': 7, 'sheep': 8, 'crow': 9, 'rain': 10, 'sea_waves': 11, 'crackling_fire': 12, 'crickets': 13, 'chirping_birds': 14, 'water_drops': 15, 'wind': 16, 'pouring_water': 17, 'toilet_flush': 18, 'thunderstorm': 19, 'crying_baby': 20, 'sneezing': 21, 'clapping': 22, 'breathing': 23, 'coughing': 24, 'footsteps': 25, 'laughing': 26, 'brushing_teeth': 27, 'snoring': 28, 'drinking_sipping': 29, 'door_wood_knock': 30, 'mouse_click': 31, 'keyboard_typing': 32, 'door_wood_creaks': 33, 'can_opening': 34, 'washing_machine': 35, 'vacuum_cleaner': 36, 'clock_alarm': 37, 'clock_tick': 38, 'glass_breaking': 39, 'helicopter': 40, 'chainsaw': 41, 'siren': 42, 'car_horn': 43, 'engine': 44, 'train': 45, 'church_bells': 46, 'airplane': 47, 'fireworks': 48, 'hand_saw': 49}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T22:12:50.064835Z",
     "start_time": "2025-11-19T22:12:49.826848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pretrained_model = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "feature_extractor = ASTFeatureExtractor.from_pretrained(pretrained_model)\n",
    "model_input_name = feature_extractor.model_input_names[0] # usually 'input_values'\n",
    "SAMPLING_RATE = feature_extractor.sampling_rate\n",
    "print(\"Feature extractor sampling rate:\", SAMPLING_RATE)"
   ],
   "id": "2406c0be599bcda",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extractor sampling rate: 16000\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T22:12:54.385235Z",
     "start_time": "2025-11-19T22:12:50.731345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 5) Optional: compute dataset mean/std for normalization\n",
    "# This can be slow. We sample or process full train set depending on resource availability.\n",
    "\n",
    "\n",
    "def estimate_mean_std(ds, num_samples=None):\n",
    "    \"\"\"Estimate mean and std of spectrogram inputs produced by feature_extractor.\n",
    "    We process raw waveforms through the feature_extractor WITHOUT normalization and compute mean/std of resulting tensors.\n",
    "    \"\"\"\n",
    "    print(\"Estimating mean/std on dataset...\")\n",
    "    # Temporarily disable normalization in the extractor\n",
    "    original_do_normalize = feature_extractor.do_normalize\n",
    "    feature_extractor.do_normalize = False\n",
    "\n",
    "\n",
    "    indices = list(range(len(ds))) if num_samples is None else np.linspace(0, len(ds)-1, num=num_samples, dtype=int).tolist()\n",
    "\n",
    "\n",
    "    means = []\n",
    "    stds = []\n",
    "    for idx in indices:\n",
    "        item = ds[int(idx)]\n",
    "        wav = item[\"audio\"][\"array\"]\n",
    "        # feature_extractor accepts python lists of arrays\n",
    "        inputs = feature_extractor([wav], sampling_rate=SAMPLING_RATE, return_tensors=\"pt\")\n",
    "        # model input tensor: shape (1, n_mels, time) or similar; flatten to compute global mean/std\n",
    "        arr = inputs.get(model_input_name).squeeze().numpy()\n",
    "        means.append(np.mean(arr))\n",
    "        stds.append(np.std(arr))\n",
    "\n",
    "\n",
    "    feature_extractor.do_normalize = original_do_normalize\n",
    "    mean = float(np.mean(means))\n",
    "    std = float(np.mean(stds))\n",
    "    print(f\"Estimated mean={mean:.6f}, std={std:.6f}\")\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "# Estimate on a subset for speed (e.g., 200 samples). Increase for better estimate.\n",
    "mean, std = estimate_mean_std(dataset, num_samples=200)\n",
    "feature_extractor.mean = mean\n",
    "feature_extractor.std = std\n",
    "feature_extractor.do_normalize = True"
   ],
   "id": "75fee7b02d38aa20",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating mean/std on dataset...\n",
      "Estimated mean=-3.365088, std=4.383699\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T22:14:02.689788Z",
     "start_time": "2025-11-19T22:14:02.687343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def preprocess_audio(batch):\n",
    "    # batch is a dictionary with 'audio' and 'labels'\n",
    "    wavs = [a[\"array\"] for a in batch[\"audio\"]]\n",
    "    inputs = feature_extractor(wavs, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\")\n",
    "    # return as numpy to keep datasets happy; Trainer/Collator will convert to tensors\n",
    "    return {model_input_name: inputs.get(model_input_name), \"labels\": batch[\"labels\"]}"
   ],
   "id": "8a734c1360a9b187",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T22:42:28.464718Z",
     "start_time": "2025-11-19T22:42:28.461153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "audio_augmentations = Compose([\n",
    "    AddGaussianSNR(min_snr_db=10, max_snr_db=20, p=0.5),\n",
    "    Gain(min_gain_db=-6, max_gain_db=6, p=0.5) if 'Gain' in globals() else Gain(min_gain_db=-6, max_gain_db=6),\n",
    "    GainTransition(min_gain_db=-6, max_gain_db=6, min_duration=0.01, max_duration=0.3, duration_unit=\"fraction\", p=0.2),\n",
    "    ClippingDistortion(min_percentile_threshold=0, max_percentile_threshold=30, p=0.3),\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.2, p=0.3),\n",
    "    PitchShift(min_semitones=-4, max_semitones=4, p=0.3),\n",
    "], p=0.8, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_audio_with_transforms(batch):\n",
    "    print(batch)\n",
    "    wavs = [audio_augmentations(w[\"array\"], sample_rate=SAMPLING_RATE) for w in batch[\"audio\"]]\n",
    "    inputs = feature_extractor(wavs, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\")\n",
    "    return {model_input_name: inputs.get(model_input_name), \"labels\": batch[\"labels\"]}"
   ],
   "id": "6efe71fbc9496afa",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T22:14:04.048062Z",
     "start_time": "2025-11-19T22:14:04.040796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# split if 'test' not present\n",
    "if \"test\" not in dataset.features:\n",
    "    dataset = dataset.train_test_split(test_size=0.2, shuffle=True, seed=42, stratify_by_column=\"labels\")\n",
    "\n",
    "\n",
    "    print(dataset)"
   ],
   "id": "ce042adaf4f126b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['filename', 'fold', 'labels', 'category', 'esc10', 'src_file', 'take', 'audio'],\n",
      "        num_rows: 1600\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['filename', 'fold', 'labels', 'category', 'esc10', 'src_file', 'take', 'audio'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T22:31:51.136012Z",
     "start_time": "2025-11-19T22:31:51.130846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset[\"train\"].set_transform(preprocess_audio_with_transforms, output_all_columns=False)\n",
    "dataset[\"test\"].set_transform(preprocess_audio, output_all_columns=False)"
   ],
   "id": "1c76d001b8fa1674",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T22:31:53.549297Z",
     "start_time": "2025-11-19T22:31:53.529034Z"
    }
   },
   "cell_type": "code",
   "source": "print(dataset[\"train\"][0][\"input_values\"])",
   "id": "4bf8c8f9a934d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3840, -0.5803, -0.1875,  ..., -0.1889, -0.1511, -0.1185],\n",
      "        [-0.8791, -0.4846, -0.0918,  ..., -0.0967, -0.1159, -0.2865],\n",
      "        [-0.4386, -0.4456, -0.0529,  ..., -0.2288, -0.1986, -0.2741],\n",
      "        ...,\n",
      "        [ 0.3838,  0.3838,  0.3838,  ...,  0.3838,  0.3838,  0.3838],\n",
      "        [ 0.3838,  0.3838,  0.3838,  ...,  0.3838,  0.3838,  0.3838],\n",
      "        [ 0.3838,  0.3838,  0.3838,  ...,  0.3838,  0.3838,  0.3838]])\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T22:31:56.652234Z",
     "start_time": "2025-11-19T22:31:56.214655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config = ASTConfig.from_pretrained(pretrained_model)\n",
    "config.num_labels = num_labels\n",
    "config.label2id = label2id\n",
    "config.id2label = id2label\n",
    "\n",
    "\n",
    "model = ASTForAudioClassification.from_pretrained(pretrained_model, config=config, ignore_mismatched_sizes=True)\n",
    "# Initialize classifier weights (optional, some weights already re-initialized by ignore_mismatched_sizes)\n",
    "model.init_weights()\n",
    "model.to(device)"
   ],
   "id": "48081e110ed7e553",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([50]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([50, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ASTForAudioClassification(\n",
       "  (audio_spectrogram_transformer): ASTModel(\n",
       "    (embeddings): ASTEmbeddings(\n",
       "      (patch_embeddings): ASTPatchEmbeddings(\n",
       "        (projection): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ASTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ASTLayer(\n",
       "          (attention): ASTAttention(\n",
       "            (attention): ASTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): ASTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ASTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ASTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): ASTMLPHead(\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dense): Linear(in_features=768, out_features=50, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T22:32:19.226720Z",
     "start_time": "2025-11-19T22:32:16.570626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=str(OUT_DIR),\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=5e-5,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"no\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "\n",
    "# prepare metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "AVERAGE = \"macro\" if num_labels > 2 else \"binary\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    metrics = accuracy.compute(predictions=preds, references=labels)\n",
    "    metrics.update(precision.compute(predictions=preds, references=labels, average=AVERAGE))\n",
    "    metrics.update(recall.compute(predictions=preds, references=labels, average=AVERAGE))\n",
    "    metrics.update(f1.compute(predictions=preds, references=labels, average=AVERAGE))\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# The default data collator won't accept torch tensors returned in transforms; use a custom collator\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorWithPadding:\n",
    "    feature_extractor: ASTFeatureExtractor\n",
    "\n",
    "\n",
    "    def __call__(self, features: Any) -> Dict[str, torch.Tensor]:\n",
    "        print(\"HERRRE\")\n",
    "        # features is a list of dicts with keys: model_input_name and 'labels'\n",
    "        input_values = [f[model_input_name].squeeze(0) if isinstance(f[model_input_name], torch.Tensor) else torch.tensor(f[model_input_name]) for f in features]\n",
    "        # batch using the extractor's pad function\n",
    "        batch = self.feature_extractor.pad({model_input_name: input_values}, return_tensors=\"pt\")\n",
    "        labels = torch.tensor([f[\"labels\"] for f in features], dtype=torch.long)\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "\n",
    "collator = DataCollatorWithPadding(feature_extractor=feature_extractor)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collator,\n",
    ")"
   ],
   "id": "5fd29d29d631632c",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T22:42:33.908143Z",
     "start_time": "2025-11-19T22:42:33.458510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_result = trainer.train()\n",
    "metrics = train_result.metrics\n",
    "trainer.save_state()\n",
    "\n",
    "\n",
    "# final evaluation\n",
    "eval_metrics = trainer.evaluate(eval_dataset=dataset[\"test\"])\n",
    "print(\"Eval metrics:\")\n",
    "print(eval_metrics)"
   ],
   "id": "bb237bec581cd5dc",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'audio'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[38]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m train_result = \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      2\u001B[39m metrics = train_result.metrics\n\u001B[32m      3\u001B[39m trainer.save_state()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/transformers/trainer.py:2325\u001B[39m, in \u001B[36mTrainer.train\u001B[39m\u001B[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[39m\n\u001B[32m   2323\u001B[39m         hf_hub_utils.enable_progress_bars()\n\u001B[32m   2324\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2325\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2326\u001B[39m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2327\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2328\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2329\u001B[39m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2330\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/transformers/trainer.py:2618\u001B[39m, in \u001B[36mTrainer._inner_training_loop\u001B[39m\u001B[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[39m\n\u001B[32m   2616\u001B[39m update_step += \u001B[32m1\u001B[39m\n\u001B[32m   2617\u001B[39m num_batches = args.gradient_accumulation_steps \u001B[38;5;28;01mif\u001B[39;00m update_step != (total_updates - \u001B[32m1\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m remainder\n\u001B[32m-> \u001B[39m\u001B[32m2618\u001B[39m batch_samples, num_items_in_batch = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_batch_samples\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch_iterator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_batches\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2619\u001B[39m \u001B[38;5;66;03m# Store the number of batches for current gradient accumulation\u001B[39;00m\n\u001B[32m   2620\u001B[39m \u001B[38;5;66;03m# This is used to correctly scale the loss when the last accumulation step has fewer batches\u001B[39;00m\n\u001B[32m   2621\u001B[39m \u001B[38;5;28mself\u001B[39m.current_gradient_accumulation_steps = \u001B[38;5;28mlen\u001B[39m(batch_samples)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/transformers/trainer.py:5654\u001B[39m, in \u001B[36mTrainer.get_batch_samples\u001B[39m\u001B[34m(self, epoch_iterator, num_batches, device)\u001B[39m\n\u001B[32m   5652\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_batches):\n\u001B[32m   5653\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m5654\u001B[39m         batch_samples.append(\u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mepoch_iterator\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m   5655\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[32m   5656\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/accelerate/data_loader.py:567\u001B[39m, in \u001B[36mDataLoaderShard.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    565\u001B[39m \u001B[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001B[39;00m\n\u001B[32m    566\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m567\u001B[39m     current_batch = \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdataloader_iter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    568\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[32m    569\u001B[39m     \u001B[38;5;28mself\u001B[39m.end()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:732\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    729\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    730\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    731\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m732\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    733\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    734\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    735\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    736\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    737\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    738\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:788\u001B[39m, in \u001B[36m_SingleProcessDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    786\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    787\u001B[39m     index = \u001B[38;5;28mself\u001B[39m._next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m788\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m    789\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pin_memory:\n\u001B[32m    790\u001B[39m         data = _utils.pin_memory.pin_memory(data, \u001B[38;5;28mself\u001B[39m._pin_memory_device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:50\u001B[39m, in \u001B[36m_MapDatasetFetcher.fetch\u001B[39m\u001B[34m(self, possibly_batched_index)\u001B[39m\n\u001B[32m     48\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.auto_collation:\n\u001B[32m     49\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.dataset, \u001B[33m\"\u001B[39m\u001B[33m__getitems__\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.dataset.__getitems__:\n\u001B[32m---> \u001B[39m\u001B[32m50\u001B[39m         data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m.\u001B[49m\u001B[43m__getitems__\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     51\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     52\u001B[39m         data = [\u001B[38;5;28mself\u001B[39m.dataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/datasets/arrow_dataset.py:2863\u001B[39m, in \u001B[36mDataset.__getitems__\u001B[39m\u001B[34m(self, keys)\u001B[39m\n\u001B[32m   2861\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__getitems__\u001B[39m(\u001B[38;5;28mself\u001B[39m, keys: \u001B[38;5;28mlist\u001B[39m) -> \u001B[38;5;28mlist\u001B[39m:\n\u001B[32m   2862\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2863\u001B[39m     batch = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[34;43m__getitem__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mkeys\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2864\u001B[39m     n_examples = \u001B[38;5;28mlen\u001B[39m(batch[\u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28miter\u001B[39m(batch))])\n\u001B[32m   2865\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m [{col: array[i] \u001B[38;5;28;01mfor\u001B[39;00m col, array \u001B[38;5;129;01min\u001B[39;00m batch.items()} \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_examples)]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/datasets/arrow_dataset.py:2859\u001B[39m, in \u001B[36mDataset.__getitem__\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   2857\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._format_type \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._format_type \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m (\u001B[33m\"\u001B[39m\u001B[33marrow\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mpandas\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mpolars\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m   2858\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m Column(\u001B[38;5;28mself\u001B[39m, key)\n\u001B[32m-> \u001B[39m\u001B[32m2859\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_getitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/datasets/arrow_dataset.py:2841\u001B[39m, in \u001B[36mDataset._getitem\u001B[39m\u001B[34m(self, key, **kwargs)\u001B[39m\n\u001B[32m   2839\u001B[39m formatter = get_formatter(format_type, features=\u001B[38;5;28mself\u001B[39m._info.features, **format_kwargs)\n\u001B[32m   2840\u001B[39m pa_subtable = query_table(\u001B[38;5;28mself\u001B[39m._data, key, indices=\u001B[38;5;28mself\u001B[39m._indices)\n\u001B[32m-> \u001B[39m\u001B[32m2841\u001B[39m formatted_output = \u001B[43mformat_table\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2842\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpa_subtable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mformatter\u001B[49m\u001B[43m=\u001B[49m\u001B[43mformatter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mformat_columns\u001B[49m\u001B[43m=\u001B[49m\u001B[43mformat_columns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_all_columns\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_all_columns\u001B[49m\n\u001B[32m   2843\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2844\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m formatted_output\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/datasets/formatting/formatting.py:658\u001B[39m, in \u001B[36mformat_table\u001B[39m\u001B[34m(table, key, formatter, format_columns, output_all_columns)\u001B[39m\n\u001B[32m    656\u001B[39m python_formatter = PythonFormatter(features=formatter.features)\n\u001B[32m    657\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m format_columns \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m658\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mformatter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpa_table\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquery_type\u001B[49m\u001B[43m=\u001B[49m\u001B[43mquery_type\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    659\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m query_type == \u001B[33m\"\u001B[39m\u001B[33mcolumn\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    660\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m format_columns:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/datasets/formatting/formatting.py:415\u001B[39m, in \u001B[36mFormatter.__call__\u001B[39m\u001B[34m(self, pa_table, query_type)\u001B[39m\n\u001B[32m    413\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.format_column(pa_table)\n\u001B[32m    414\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m query_type == \u001B[33m\"\u001B[39m\u001B[33mbatch\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m415\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mformat_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpa_table\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/datasets/formatting/formatting.py:541\u001B[39m, in \u001B[36mCustomFormatter.format_batch\u001B[39m\u001B[34m(self, pa_table)\u001B[39m\n\u001B[32m    539\u001B[39m batch = \u001B[38;5;28mself\u001B[39m.python_arrow_extractor().extract_batch(pa_table)\n\u001B[32m    540\u001B[39m batch = \u001B[38;5;28mself\u001B[39m.python_features_decoder.decode_batch(batch)\n\u001B[32m--> \u001B[39m\u001B[32m541\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[30]\u001B[39m\u001B[32m, line 14\u001B[39m, in \u001B[36mpreprocess_audio_with_transforms\u001B[39m\u001B[34m(batch)\u001B[39m\n\u001B[32m     13\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpreprocess_audio_with_transforms\u001B[39m(batch):\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m     wavs = [audio_augmentations(w[\u001B[33m\"\u001B[39m\u001B[33marray\u001B[39m\u001B[33m\"\u001B[39m], sample_rate=SAMPLING_RATE) \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m \u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43maudio\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m]\n\u001B[32m     15\u001B[39m     inputs = feature_extractor(wavs, sampling_rate=SAMPLING_RATE, return_tensors=\u001B[33m\"\u001B[39m\u001B[33mpt\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     16\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m {model_input_name: inputs.get(model_input_name), \u001B[33m\"\u001B[39m\u001B[33mlabels\u001B[39m\u001B[33m\"\u001B[39m: batch[\u001B[33m\"\u001B[39m\u001B[33mlabels\u001B[39m\u001B[33m\"\u001B[39m]}\n",
      "\u001B[31mKeyError\u001B[39m: 'audio'"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5d2f73cf777298e3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
