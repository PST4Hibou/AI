{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-18T16:59:14.102085Z",
     "start_time": "2025-11-18T16:59:13.013580Z"
    }
   },
   "source": [
    "from datasets import load_from_disk\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchmetrics\n",
    "from tqdm import tqdm\n",
    "import torchaudio\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "SAMPLE_RATE = 16000\n",
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 24\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "print(f\"Using device: {DEVICE}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pierre/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T16:59:14.215745Z",
     "start_time": "2025-11-18T16:59:14.203840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# dataset = load_dataset(\"Hibou-Foundation/big_ds_preprocessed_specto_noaugment_1_50g\")\n",
    "dataset = load_from_disk(\"../dataset/ds_2_noaugment_test.hf\")"
   ],
   "id": "96e96b43be3120dc",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T16:59:14.828614Z",
     "start_time": "2025-11-18T16:59:14.824898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = dataset.with_format(\"torch\", columns=[\"audio\", \"label\"])\n",
    "print(\"Dataset splits:\", {k: v.shape for k, v in dataset.items()})"
   ],
   "id": "86b6eb8f8464a079",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits: {'train': (61543, 2), 'val': (7222, 2), 'test': (7220, 2)}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T16:59:15.290405Z",
     "start_time": "2025-11-18T16:59:15.288105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------\n",
    "# Collate function with Mel computation\n",
    "# -----------------------------\n",
    "def collate_fn(batch):\n",
    "    xs = [b[\"audio\"] for b in batch]\n",
    "    ys = [b[\"label\"] for b in batch]\n",
    "\n",
    "    xs = torch.stack(xs, dim=0).unsqueeze(1)\n",
    "    ys = torch.tensor(ys, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset[\"train\"], batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(dataset[\"val\"], batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(dataset[\"test\"], batch_size=BATCH_SIZE, shuffle=False,num_workers=NUM_WORKERS, collate_fn=collate_fn)"
   ],
   "id": "11db12420f3aa8e4",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T16:59:15.914167Z",
     "start_time": "2025-11-18T16:59:15.788146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------\n",
    "# Simplified CNN\n",
    "# -----------------------------\n",
    "class SimpleAudioCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.PReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.PReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.PReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.PReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleAudioCNN().to(DEVICE)"
   ],
   "id": "d3369f291e5d6dad",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T17:13:11.940065Z",
     "start_time": "2025-11-18T17:13:11.909591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# -----------------------------\n",
    "# Residual Block Definition\n",
    "# -----------------------------\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.prelu1 = nn.PReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Shortcut connection: Adjust for channel difference if needed\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        self.prelu2 = nn.PReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The main path\n",
    "        residual = self.prelu1(self.bn1(self.conv1(x)))\n",
    "        residual = self.bn2(self.conv2(residual))\n",
    "\n",
    "        # Add shortcut (identity)\n",
    "        shortcut = self.shortcut(x)\n",
    "\n",
    "        # Activation after addition\n",
    "        out = self.prelu2(residual + shortcut)\n",
    "        return out\n",
    "\n",
    "# -----------------------------\n",
    "# Advanced CNN for Drone Audio\n",
    "# -----------------------------\n",
    "class AdvancedAudioCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initial Conv Block: Larger kernel to capture basic acoustic patterns\n",
    "        self.conv_init = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, padding=2), # 5x5 kernel\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.PReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        # Feature extraction blocks with Residual Connections\n",
    "        self.res_block1 = nn.Sequential(\n",
    "            ResidualBlock(32, 64),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.res_block2 = nn.Sequential(\n",
    "            ResidualBlock(64, 128),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        # # Deeper layer\n",
    "        # self.res_block3 = nn.Sequential(\n",
    "        #     ResidualBlock(128, 256),\n",
    "        #     nn.MaxPool2d(2)\n",
    "        # )\n",
    "\n",
    "        # Global Pooling (Average and Max)\n",
    "        # This will be applied in the forward pass\n",
    "\n",
    "        # Fully Connected Layer (FC)\n",
    "        # Input features will be 256 (from GAP) + 256 (from GMP) = 512\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128 * 2, 128),\n",
    "            nn.PReLU(),\n",
    "            nn.Dropout(0.5), # Slightly increased dropout for better generalization\n",
    "            nn.Linear(128, 1),\n",
    "            # nn.Sigmoid() # Add this if you want the output to be a probability in [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: (Batch, 1, Freq, Time)\n",
    "        x = self.conv_init(x)\n",
    "        x = self.res_block1(x)\n",
    "        x = self.res_block2(x)\n",
    "        # x = self.res_block3(x)\n",
    "\n",
    "        # Apply Global Average Pooling (GAP) and Global Max Pooling (GMP)\n",
    "        gap = F.adaptive_avg_pool2d(x, (1, 1)).view(x.size(0), -1) # Output shape: (Batch, 256)\n",
    "        gmp = F.adaptive_max_pool2d(x, (1, 1)).view(x.size(0), -1) # Output shape: (Batch, 256)\n",
    "\n",
    "        # Concatenate both features before the FC layer\n",
    "        x = torch.cat([gap, gmp], dim=1) # Output shape: (Batch, 512)\n",
    "\n",
    "        return self.fc(x)\n",
    "\n",
    "# Assuming DEVICE is already defined\n",
    "model = AdvancedAudioCNN().to(DEVICE)"
   ],
   "id": "88c6bfd38d3487b8",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T17:13:12.273406Z",
     "start_time": "2025-11-18T17:13:12.272044Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b8f2f3d152fd133a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T17:13:12.482714Z",
     "start_time": "2025-11-18T17:13:12.479506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------\n",
    "# Loss, optimizer, metric\n",
    "# -----------------------------\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "metric_acc = torchmetrics.classification.BinaryAccuracy().to(DEVICE)"
   ],
   "id": "bdd4a5ec499d3c97",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-18T17:13:12.721839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------\n",
    "# Training loop\n",
    "# -----------------------------\n",
    "EPOCHS = 20\n",
    "best_val_acc = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\"):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "        train_acc += metric_acc(out, y) * x.size(0)\n",
    "\n",
    "    scheduler.step()\n",
    "    train_loss /= len(dataset[\"train\"])\n",
    "    train_acc /= len(dataset[\"train\"])\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss, val_acc = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(valid_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Valid]\"):\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            val_loss += loss.item() * x.size(0)\n",
    "            val_acc += metric_acc(out, y) * x.size(0)\n",
    "\n",
    "    val_loss /= len(dataset[\"val\"])\n",
    "    val_acc /= len(dataset[\"val\"])\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | \"\n",
    "          f\"Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_simple_cnn.pt\")\n",
    "        print(\"✅ Saved new best model!\")"
   ],
   "id": "f0d38993247be581",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [Train]: 100%|██████████| 481/481 [01:00<00:00,  7.91it/s]\n",
      "Epoch 1/20 [Valid]: 100%|██████████| 57/57 [00:05<00:00, 10.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Train Loss: 0.1347 | Train Acc: 0.9526 | Val Loss: 0.0763 | Val Acc: 0.9740\n",
      "✅ Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 [Train]:  69%|██████▊   | 330/481 [00:45<00:23,  6.41it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T16:48:30.061938Z",
     "start_time": "2025-11-18T16:48:30.019830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load model\n",
    "model = SimpleAudioCNN().to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"best_simple_cnn.pt\", map_location=DEVICE))\n",
    "model.eval()\n",
    "print(\"Loaded best model and set to eval mode\")"
   ],
   "id": "eed53dcd5f0fa21b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model and set to eval mode\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T16:48:38.245272Z",
     "start_time": "2025-11-18T16:48:32.435890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_preds = []\n",
    "all_probs = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in tqdm(test_loader, desc=\"Testing\"):\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        logits = model(x)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs > 0.5).float()\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "        all_labels.extend(y.cpu().numpy().flatten())"
   ],
   "id": "3aa71f4d58521398",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 29/29 [00:05<00:00,  5.00it/s]\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T16:46:57.942565Z",
     "start_time": "2025-11-18T16:46:57.926371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------\n",
    "# Metrics\n",
    "# -----------------------------\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "df_results = pd.DataFrame({\n",
    "    \"true_label\": all_labels,\n",
    "    \"pred_label\": all_preds,\n",
    "    \"confidence\": all_probs\n",
    "})\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(all_labels, all_preds, target_names=dataset[\"train\"].features[\"label\"].names, digits=3))\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(cm)\n"
   ],
   "id": "3c6e96e1efa78fca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 95.76%\n",
      "[[5171  162]\n",
      " [ 144 1743]]\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T16:46:57.998450Z",
     "start_time": "2025-11-18T16:46:57.997114Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e4e99c80422dae79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "eb9dfff20c773b88"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
