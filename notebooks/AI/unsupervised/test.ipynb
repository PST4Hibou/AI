{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-29T17:06:18.740245Z",
     "start_time": "2025-11-29T17:06:18.734615Z"
    }
   },
   "source": [
    "# byol_audio.py\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "# -------------------------\n",
    "# Configuration (edit as needed)\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 96\n",
    "NUM_WORKERS = 8\n",
    "SAMPLE_RATE = 16000\n",
    "N_MELS = 128\n",
    "N_FFT = 1024\n",
    "HOP_LENGTH = 256\n",
    "EPOCHS_BYOL = 5\n",
    "EPOCHS_PROBE = 2\n",
    "LR_BYOL = 3e-4\n",
    "LR_PROBE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "MODEL_DIR = \"./ssl_checkpoints\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "BACKBONE = \"resnet50\"  # or \"resnet18\" to speed up testing\n",
    "\n",
    "# reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T16:58:10.910182Z",
     "start_time": "2025-11-29T16:58:10.900298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -------------------------\n",
    "# Utilities: Mel Preproc + Augmentations\n",
    "# -------------------------\n",
    "def waveform_to_logmel(waveform: torch.Tensor,\n",
    "                       sample_rate=SAMPLE_RATE,\n",
    "                       n_fft=N_FFT,\n",
    "                       hop_length=HOP_LENGTH,\n",
    "                       n_mels=N_MELS,\n",
    "                       fmin=20,\n",
    "                       fmax=8000) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    waveform: 1D torch tensor (N,)\n",
    "    returns: torch.FloatTensor shape [3, n_mels, time]\n",
    "    \"\"\"\n",
    "    wav = waveform.cpu().numpy().astype(np.float32)\n",
    "    mel = librosa.feature.melspectrogram(\n",
    "        y=wav,\n",
    "        sr=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels,\n",
    "        fmin=fmin,\n",
    "        fmax=fmax,\n",
    "        power=2.0\n",
    "    )\n",
    "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "    # normalize per-sample\n",
    "    mel_db = (mel_db - mel_db.mean()) / (mel_db.std() + 1e-6)\n",
    "    mel_db = torch.from_numpy(mel_db).float().unsqueeze(0)  # [1, n_mels, T]\n",
    "    mel_db = mel_db.repeat(3, 1, 1)  # 3 channels for ResNet\n",
    "    return mel_db\n",
    "\n",
    "# Simple waveform-level augmentations\n",
    "def add_noise(wav: np.ndarray, snr_db: float = 20.0) -> np.ndarray:\n",
    "    if snr_db <= 0:\n",
    "        return wav\n",
    "    rms = np.sqrt(np.mean(wav ** 2) + 1e-9)\n",
    "    snr = 10 ** (snr_db / 20.0)\n",
    "    noise_rms = rms / snr\n",
    "    noise = np.random.normal(0, noise_rms, size=wav.shape)\n",
    "    return wav + noise\n",
    "\n",
    "def random_gain(wav: np.ndarray, min_gain_db=-6.0, max_gain_db=6.0) -> np.ndarray:\n",
    "    g = np.random.uniform(min_gain_db, max_gain_db)\n",
    "    factor = 10 ** (g / 20)\n",
    "    return wav * factor\n",
    "\n",
    "def random_time_shift(wav: np.ndarray, max_shift_seconds=0.05, sr=SAMPLE_RATE) -> np.ndarray:\n",
    "    max_shift = int(max_shift_seconds * sr)\n",
    "    if max_shift <= 0:\n",
    "        return wav\n",
    "    shift = np.random.randint(-max_shift, max_shift)\n",
    "    if shift == 0:\n",
    "        return wav\n",
    "    return np.roll(wav, shift)\n",
    "\n",
    "# SpecAugment-style (on Log-Mel)\n",
    "def spec_augment(mel: torch.Tensor,\n",
    "                 time_mask_max=10,\n",
    "                 freq_mask_max=15,\n",
    "                 n_time_masks=1,\n",
    "                 n_freq_masks=1) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    mel: [C, F, T] but we apply masks to last two dims; channels are identical anyway.\n",
    "    Returns same shape.\n",
    "    \"\"\"\n",
    "    _, F, T = mel.shape\n",
    "    mel = mel.clone()\n",
    "    for _ in range(n_freq_masks):\n",
    "        f = np.random.randint(0, freq_mask_max + 1)\n",
    "        if f == 0:\n",
    "            continue\n",
    "        f0 = np.random.randint(0, max(1, F - f))\n",
    "        mel[:, f0:f0 + f, :] = 0.0\n",
    "    for _ in range(n_time_masks):\n",
    "        t = np.random.randint(0, time_mask_max + 1)\n",
    "        if t == 0:\n",
    "            continue\n",
    "        t0 = np.random.randint(0, max(1, T - t))\n",
    "        mel[:, :, t0:t0 + t] = 0.0\n",
    "    return mel\n",
    "\n",
    "# Augmentation pipeline returning two views\n",
    "def make_two_views(waveform: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    waveform: torch tensor (N,)\n",
    "    Returns two log-mel tensors: view1, view2 shape [3, n_mels, T]\n",
    "    We'll apply different random augmentations to waveform, compute log-mel, then specaugment.\n",
    "    \"\"\"\n",
    "    wav_np = waveform.cpu().numpy().astype(np.float32)\n",
    "\n",
    "    # VIEW 1 (mild)\n",
    "    v1 = wav_np.copy()\n",
    "    v1 = add_noise(v1, snr_db=np.random.uniform(15, 30))\n",
    "    v1 = random_gain(v1, -3.0, 3.0)\n",
    "    v1 = random_time_shift(v1, max_shift_seconds=0.02)\n",
    "    mel1 = librosa.feature.melspectrogram(y=v1, sr=SAMPLE_RATE, n_fft=N_FFT,\n",
    "                                          hop_length=HOP_LENGTH, n_mels=N_MELS,\n",
    "                                          fmin=20, fmax=8000, power=2.0)\n",
    "    mel1 = librosa.power_to_db(mel1, ref=np.max)\n",
    "    mel1 = (mel1 - mel1.mean()) / (mel1.std() + 1e-6)\n",
    "    mel1 = torch.from_numpy(mel1).float().unsqueeze(0).repeat(3, 1, 1)\n",
    "    mel1 = spec_augment(mel1, time_mask_max=8, freq_mask_max=10, n_time_masks=1, n_freq_masks=1)\n",
    "\n",
    "    # VIEW 2 (stronger)\n",
    "    v2 = wav_np.copy()\n",
    "    v2 = add_noise(v2, snr_db=np.random.uniform(10, 25))\n",
    "    v2 = random_gain(v2, -6.0, 6.0)\n",
    "    v2 = random_time_shift(v2, max_shift_seconds=0.04)\n",
    "    mel2 = librosa.feature.melspectrogram(y=v2, sr=SAMPLE_RATE, n_fft=N_FFT,\n",
    "                                          hop_length=HOP_LENGTH, n_mels=N_MELS,\n",
    "                                          fmin=20, fmax=8000, power=2.0)\n",
    "    mel2 = librosa.power_to_db(mel2, ref=np.max)\n",
    "    mel2 = (mel2 - mel2.mean()) / (mel2.std() + 1e-6)\n",
    "    mel2 = torch.from_numpy(mel2).float().unsqueeze(0).repeat(3, 1, 1)\n",
    "    mel2 = spec_augment(mel2, time_mask_max=12, freq_mask_max=15, n_time_masks=2, n_freq_masks=2)\n",
    "\n",
    "    return mel1, mel2\n",
    "\n",
    "# -------------------------\n",
    "# Collate function: returns two augmented views + label (label optional)\n",
    "# -------------------------\n",
    "def collate_fn_byol(batch):\n",
    "    \"\"\"\n",
    "    batch: list of dataset items where item[\"audio\"] is a torch tensor shape (8000,)\n",
    "    returns: dict with\n",
    "       view1: tensor [B, 3, F, T]\n",
    "       view2: tensor [B, 3, F, T]\n",
    "       labels: tensor [B] (optional, used later for linear probe)\n",
    "    \"\"\"\n",
    "    view1_list = []\n",
    "    view2_list = []\n",
    "    labels = []\n",
    "\n",
    "    for item in batch:\n",
    "        wav = item[\"audio\"]  # HF dataset gives a torch tensor already\n",
    "        v1, v2 = make_two_views(wav)\n",
    "        view1_list.append(v1)\n",
    "        view2_list.append(v2)\n",
    "        labels.append(item[\"label\"])\n",
    "\n",
    "    # stack (they should all have same time dimension since audio fixed length)\n",
    "    view1 = torch.stack(view1_list, dim=0)\n",
    "    view2 = torch.stack(view2_list, dim=0)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return {\"view1\": view1, \"view2\": view2, \"label\": labels}"
   ],
   "id": "1cac341c7c037b2c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T16:58:28.903558Z",
     "start_time": "2025-11-29T16:58:28.897559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# -------------------------\n",
    "# BYOL model components\n",
    "# -------------------------\n",
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=2048, out_dim=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, in_dim]\n",
    "        return self.net(x)\n",
    "\n",
    "class BYOLAudio(nn.Module):\n",
    "    def __init__(self, backbone_name=\"resnet50\", projector_hidden=2048, projector_out=256, pretrained=False):\n",
    "        super().__init__()\n",
    "        # load backbone and adapt first conv to 3 channels (we already have 3 channels)\n",
    "        if backbone_name == \"resnet50\":\n",
    "            backbone = models.resnet50(weights=None if not pretrained else \"IMAGENET1K_V1\")\n",
    "        elif backbone_name == \"resnet18\":\n",
    "            backbone = models.resnet18(weights=None if not pretrained else \"IMAGENET1K_V1\")\n",
    "        else:\n",
    "            raise ValueError(\"unsupported backbone\")\n",
    "\n",
    "        # remove classifier\n",
    "        num_features = backbone.fc.in_features\n",
    "        backbone.fc = nn.Identity()\n",
    "        self.backbone = backbone\n",
    "        # projector\n",
    "        self.projector = MLPHead(num_features, hidden_dim=projector_hidden, out_dim=projector_out)\n",
    "        # predictor\n",
    "        self.predictor = MLPHead(projector_out, hidden_dim=projector_hidden // 2, out_dim=projector_out)\n",
    "\n",
    "    def forward_backbone(self, x):\n",
    "        # x: [B, 3, F, T]\n",
    "        out = self.backbone(x)  # [B, num_features]\n",
    "        return out\n",
    "\n",
    "    def online_forward(self, x):\n",
    "        feat = self.forward_backbone(x)\n",
    "        proj = self.projector(feat)\n",
    "        pred = self.predictor(proj)\n",
    "        return feat, proj, pred\n",
    "\n",
    "    def target_forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            feat = self.forward_backbone(x)\n",
    "            proj = self.projector(feat)\n",
    "        return feat, proj\n"
   ],
   "id": "8613a7807a80c48a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T16:58:38.248466Z",
     "start_time": "2025-11-29T16:58:38.245155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -------------------------\n",
    "# Helper: create target network as copy of online network\n",
    "# -------------------------\n",
    "def initialize_target_network(online_net: BYOLAudio) -> BYOLAudio:\n",
    "    target = BYOLAudio(backbone_name=BACKBONE)\n",
    "    target.load_state_dict(online_net.state_dict())\n",
    "    for p in target.parameters():\n",
    "        p.requires_grad = False\n",
    "    return target\n",
    "\n",
    "# momentum update\n",
    "@torch.no_grad()\n",
    "def momentum_update(online: nn.Module, target: nn.Module, m: float):\n",
    "    # target = m * target + (1 - m) * online\n",
    "    for param_o, param_t in zip(online.parameters(), target.parameters()):\n",
    "        param_t.data = param_t.data * m + param_o.data * (1.0 - m)\n",
    "\n",
    "# loss: negative cosine similarity (or l2 between normalized vectors)\n",
    "def byol_loss_fn(p, z):\n",
    "    # p: prediction from online net [B, D]\n",
    "    # z: projection from target net [B, D] (stop grad)\n",
    "    p = F.normalize(p, dim=1)\n",
    "    z = F.normalize(z, dim=1)\n",
    "    return 2 - 2 * (p * z).sum(dim=1)  # per-sample loss (2 - 2 cos) -> mean later\n"
   ],
   "id": "5e90d9cefaa70cbd",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T16:58:47.210808Z",
     "start_time": "2025-11-29T16:58:47.204899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -------------------------\n",
    "# Training BYOL\n",
    "# -------------------------\n",
    "def train_byol(dataset, epochs=EPOCHS_BYOL, save_every=5):\n",
    "    # prepare dataloader\n",
    "    train_loader = DataLoader(dataset[\"train\"], batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, num_workers=NUM_WORKERS,\n",
    "                              collate_fn=collate_fn_byol, pin_memory=True)\n",
    "\n",
    "    online_net = BYOLAudio(backbone_name=BACKBONE).to(DEVICE)\n",
    "    target_net = initialize_target_network(online_net).to(DEVICE)\n",
    "\n",
    "    optimizer = optim.AdamW(online_net.parameters(), lr=LR_BYOL, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    # momentum schedule params (cosine schedule)\n",
    "    base_m = 0.996\n",
    "    final_m = 1.0\n",
    "    total_steps = epochs * len(train_loader)\n",
    "    global_step = 0\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    for epoch in range(epochs):\n",
    "        online_net.train()\n",
    "        running_loss = 0.0\n",
    "        pbar = tqdm(train_loader, desc=f\"BYOL Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "        for batch in pbar:\n",
    "            v1 = batch[\"view1\"].to(DEVICE)  # [B, 3, F, T]\n",
    "            v2 = batch[\"view2\"].to(DEVICE)\n",
    "\n",
    "            # ONLINE forward: view1 -> pred1, view2 -> pred2\n",
    "            _, proj1, pred1 = online_net.online_forward(v1)\n",
    "            _, proj2, pred2 = online_net.online_forward(v2)\n",
    "\n",
    "            # TARGET forward (no grad): view1 -> z1, view2 -> z2\n",
    "            with torch.no_grad():\n",
    "                _, z1 = target_net.target_forward(v1)\n",
    "                _, z2 = target_net.target_forward(v2)\n",
    "\n",
    "            # BYOL loss: compare pred1 vs z2 and pred2 vs z1\n",
    "            loss1 = byol_loss_fn(pred1, z2).mean()\n",
    "            loss2 = byol_loss_fn(pred2, z1).mean()\n",
    "            loss = (loss1 + loss2) * 0.5\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update target network momentum\n",
    "            global_step += 1\n",
    "            # cosine schedule from base_m -> final_m\n",
    "            momentum = 1.0 - (1.0 - base_m) * (math.cos(math.pi * global_step / total_steps) + 1) / 2\n",
    "            momentum_update(online_net, target_net, momentum)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            pbar.set_postfix({\"loss\": f\"{running_loss / (global_step % len(train_loader) + 1):.4f}\",\n",
    "                              \"momentum\": f\"{momentum:.4f}\"})\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f\"BYOL Epoch {epoch+1}/{epochs} loss {epoch_loss:.4f}\")\n",
    "\n",
    "        # save checkpoint\n",
    "        if (epoch + 1) % save_every == 0 or epoch == epochs - 1:\n",
    "            ckpt_path = os.path.join(MODEL_DIR, f\"byol_epoch{epoch+1}.pth\")\n",
    "            torch.save({\"online_state\": online_net.state_dict(),\n",
    "                        \"target_state\": target_net.state_dict(),\n",
    "                        \"optimizer\": optimizer.state_dict(),\n",
    "                        \"epoch\": epoch+1}, ckpt_path)\n",
    "            print(f\"Saved {ckpt_path}\")\n",
    "\n",
    "    # final save\n",
    "    final_path = os.path.join(MODEL_DIR, \"byol_final.pth\")\n",
    "    torch.save({\"online_state\": online_net.state_dict()}, final_path)\n",
    "    print(\"BYOL training finished. Model saved to\", final_path)\n",
    "    return online_net"
   ],
   "id": "4b03cba3a51b1051",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T16:59:00.746874Z",
     "start_time": "2025-11-29T16:59:00.737196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -------------------------\n",
    "# Linear probe\n",
    "# -------------------------\n",
    "class LinearProbe(nn.Module):\n",
    "    def __init__(self, backbone: BYOLAudio, out_dim=1):\n",
    "        super().__init__()\n",
    "        # backbone.backbone is the actual CNN\n",
    "        self.backbone = backbone.backbone\n",
    "        # freeze backbone\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "        # classifier\n",
    "        num_features = self.backbone.fc.in_features if hasattr(self.backbone, \"fc\") else None\n",
    "        # if we replaced fc with Identity earlier, note num_features is known from BYOLAudio internals\n",
    "        # We'll infer by passing a dummy input during initialization if needed\n",
    "        self.classifier = nn.Linear(backbone.projector.net[-1].out_features if False else backbone.projector.net[-1].out_features, out_dim)\n",
    "        # but simpler: we will compute features size at runtime in train_probe\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.backbone(x)  # shape [B, feat_dim]\n",
    "        out = self.classifier(feat)\n",
    "        return out\n",
    "\n",
    "def extract_feature_dim(backbone_model: BYOLAudio):\n",
    "    # helper: run a dummy input through backbone to get feature dim\n",
    "    dummy = torch.randn(1, 3, N_MELS, (SAMPLE_RATE // HOP_LENGTH) + 1).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        feat = backbone_model.forward_backbone(dummy)\n",
    "    return feat.shape[1]\n",
    "\n",
    "def train_linear_probe(online_net: BYOLAudio, dataset, epochs=EPOCHS_PROBE):\n",
    "    # build dataloaders (no augmentation, just mel preprocessing)\n",
    "    def collate_probe(batch):\n",
    "        xs = []\n",
    "        ys = []\n",
    "        for item in batch:\n",
    "            wav = item[\"audio\"]\n",
    "            mel = waveform_to_logmel(wav)  # deterministic view\n",
    "            xs.append(mel)\n",
    "            ys.append(item[\"label\"])\n",
    "        x = torch.stack(xs, dim=0)\n",
    "        y = torch.tensor(ys, dtype=torch.float32)\n",
    "        return x, y\n",
    "\n",
    "    train_loader = DataLoader(dataset[\"train\"], batch_size=BATCH_SIZE, shuffle=True,\n",
    "                              num_workers=NUM_WORKERS, collate_fn=collate_probe, pin_memory=True)\n",
    "    val_loader = DataLoader(dataset[\"val\"], batch_size=BATCH_SIZE, shuffle=False,\n",
    "                            num_workers=NUM_WORKERS, collate_fn=collate_probe, pin_memory=True)\n",
    "\n",
    "    feat_dim = extract_feature_dim(online_net)  # get dim\n",
    "    # freeze backbone\n",
    "    backbone_cnn = online_net.backbone\n",
    "    for p in backbone_cnn.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # classifier head for binary\n",
    "    clf = nn.Linear(feat_dim, 1).to(DEVICE)\n",
    "    optimizer = optim.Adam(clf.parameters(), lr=LR_PROBE, weight_decay=WEIGHT_DECAY)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_auc = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        # train\n",
    "        clf.train()\n",
    "        running_loss = 0.0\n",
    "        preds_all = []\n",
    "        labels_all = []\n",
    "        pbar = tqdm(train_loader, desc=f\"Probe Train {epoch+1}/{epochs}\", leave=False)\n",
    "        for x, y in pbar:\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                feats = backbone_cnn(x)\n",
    "\n",
    "            logits = clf(feats).squeeze(1)\n",
    "            loss = criterion(logits, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "            preds_all.extend(probs.tolist())\n",
    "            labels_all.extend(y.cpu().numpy().tolist())\n",
    "\n",
    "            # partial metrics\n",
    "            if len(labels_all) >= 1:\n",
    "                auc = roc_auc_score(labels_all, preds_all) if len(set(labels_all)) > 1 else 0.5\n",
    "                acc = accuracy_score(np.array(labels_all) > 0.5, np.array(preds_all) > 0.5)\n",
    "                pbar.set_postfix({\"loss\": f\"{running_loss / (len(preds_all) // BATCH_SIZE + 1):.4f}\",\n",
    "                                  \"auc\": f\"{auc:.4f}\", \"acc\": f\"{acc:.4f}\"})\n",
    "\n",
    "        # validate\n",
    "        clf.eval()\n",
    "        v_preds = []\n",
    "        v_labels = []\n",
    "        v_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x = x.to(DEVICE)\n",
    "                y = y.to(DEVICE)\n",
    "                feats = backbone_cnn(x)\n",
    "                logits = clf(feats).squeeze(1)\n",
    "                loss = criterion(logits, y)\n",
    "                v_loss += loss.item()\n",
    "                probs = torch.sigmoid(logits).cpu().numpy()\n",
    "                v_preds.extend(probs.tolist())\n",
    "                v_labels.extend(y.cpu().numpy().tolist())\n",
    "\n",
    "        auc_val = roc_auc_score(v_labels, v_preds) if len(set(v_labels)) > 1 else 0.5\n",
    "        acc_val = accuracy_score(np.array(v_labels) > 0.5, np.array(v_preds) > 0.5)\n",
    "        print(f\"Probe Epoch {epoch+1}/{epochs}  Train loss: {running_loss/len(train_loader):.4f}  Val loss: {v_loss/len(val_loader):.4f}  Val AUC: {auc_val:.4f}  Val Acc: {acc_val:.4f}\")\n",
    "\n",
    "        # save best\n",
    "        if auc_val > best_auc:\n",
    "            best_auc = auc_val\n",
    "            torch.save({\"clf_state\": clf.state_dict(), \"epoch\": epoch+1}, os.path.join(MODEL_DIR, \"best_probe.pth\"))\n",
    "            print(\"Saved best linear probe checkpoint (AUC improved)\")\n",
    "\n",
    "    print(\"Linear probe training finished. Best Val AUC:\", best_auc)\n"
   ],
   "id": "93748c5fee7e3759",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T17:02:58.821879Z",
     "start_time": "2025-11-29T17:02:03.101211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"Hibou-Foundation/big_ds_4_raw_wav_balanced\")\n",
    "dataset = dataset.with_format(\"torch\", columns=[\"audio\", \"label\"])"
   ],
   "id": "b85e5f818c8e37d3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 23/23 [00:00<00:00, 4191.21files/s]\n",
      "Generating train split: 100%|██████████| 352116/352116 [00:44<00:00, 7950.49 examples/s] \n",
      "Generating val split: 100%|██████████| 43580/43580 [00:04<00:00, 9607.53 examples/s] \n",
      "Generating test split: 100%|██████████| 43478/43478 [00:05<00:00, 8691.22 examples/s] \n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T22:39:55.443194Z",
     "start_time": "2025-11-29T17:06:34.069502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train BYOL (self-supervised)\n",
    "ssl_model = train_byol(dataset, epochs=EPOCHS_BYOL, save_every=5)\n",
    "\n",
    "# Run linear probe\n",
    "train_linear_probe(ssl_model, dataset, epochs=EPOCHS_PROBE)"
   ],
   "id": "6337246729ba7cc6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BYOL Epoch 1/5 loss 0.1178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BYOL Epoch 2/5 loss 0.0344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BYOL Epoch 3/5 loss 0.0296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BYOL Epoch 4/5 loss 0.0290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BYOL Epoch 5/5 loss 0.0279\n",
      "Saved ./ssl_checkpoints/byol_epoch5.pth\n",
      "BYOL training finished. Model saved to ./ssl_checkpoints/byol_final.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probe Epoch 1/2  Train loss: 0.1298  Val loss: 0.1276  Val AUC: 0.9891  Val Acc: 0.9580\n",
      "Saved best linear probe checkpoint (AUC improved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probe Epoch 2/2  Train loss: 0.0974  Val loss: 0.1029  Val AUC: 0.9906  Val Acc: 0.9683\n",
      "Saved best linear probe checkpoint (AUC improved)\n",
      "Linear probe training finished. Best Val AUC: 0.9906252831430898\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T22:48:14.260762Z",
     "start_time": "2025-11-29T22:48:08.745396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sampling_rate = 16000\n",
    "ds_test_online = load_dataset(\"Usernameeeeee/drone_test\", split=\"test\")\n",
    "ds_test_online2 = load_dataset(\"Usernameeeeee/drone_test_2\", split=\"test\")\n",
    "\n",
    "def swap_labels(example):\n",
    "    example[\"label\"] = 1 - example[\"label\"]   # flips 0 ↔ 1\n",
    "    return example\n",
    "\n",
    "ds_test_online = ds_test_online.map(swap_labels)\n",
    "ds_test_online2 = ds_test_online2.map(swap_labels)"
   ],
   "id": "d8b02ab26ffe633",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 100%|██████████| 893/893 [00:00<00:00, 6283.64 examples/s]\n",
      "Generating test split: 100%|██████████| 2805/2805 [00:00<00:00, 3591.47 examples/s]\n",
      "Map: 100%|██████████| 893/893 [00:00<00:00, 8501.19 examples/s]\n",
      "Map: 100%|██████████| 2805/2805 [00:00<00:00, 9393.58 examples/s] \n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T22:47:22.473182Z",
     "start_time": "2025-11-29T22:47:22.469276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def convert_to_mel_spectrogram(data):\n",
    "    mel = librosa.feature.melspectrogram(\n",
    "        y=data,\n",
    "        sr=sampling_rate,\n",
    "        n_fft=1025,\n",
    "        hop_length=256,\n",
    "        n_mels=128,\n",
    "        fmin=20,\n",
    "        fmax=8000,\n",
    "        power=2.0\n",
    "    )\n",
    "\n",
    "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "    mel_db = (mel_db - mel_db.mean()) / (mel_db.std() + 1e-6)\n",
    "\n",
    "    mel_db = torch.tensor(mel_db).float().unsqueeze(0)  # [1, 128, T]\n",
    "\n",
    "    return mel_db"
   ],
   "id": "5bd0347860d00cda",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T22:47:46.240997Z",
     "start_time": "2025-11-29T22:47:46.235735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def collate_fn_librosa_test(batch):\n",
    "    inputs, labels = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        wav = item[\"audio\"][\"array\"]\n",
    "        mel = convert_to_mel_spectrogram(wav)   # [1, 128, T]\n",
    "        mel = mel.repeat(3, 1, 1)               # → [3, 128, T]\n",
    "        inputs.append(mel)\n",
    "        labels.append(item[\"label\"])\n",
    "\n",
    "    # Pad to max T in batch\n",
    "    max_T = max(x.shape[-1] for x in inputs)\n",
    "\n",
    "    padded = []\n",
    "    for x in inputs:\n",
    "        pad_T = max_T - x.shape[-1]\n",
    "        padded.append(\n",
    "            torch.nn.functional.pad(x, (0, pad_T))  # pad last dimension only\n",
    "        )\n",
    "\n",
    "    inputs = torch.stack(padded)   # [B, 3, 128, max_T]\n",
    "    labels = torch.tensor(labels).float()\n",
    "\n",
    "    return inputs, labels"
   ],
   "id": "cc543cde17679b47",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_loader_1 = DataLoader(\n",
    "    ds_test_online,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    collate_fn=collate_fn_librosa_test,\n",
    ")\n",
    "\n",
    "test_loader_2 = DataLoader(\n",
    "    ds_test_online2,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    collate_fn=collate_fn_librosa_test,\n",
    ")"
   ],
   "id": "560e964218a42b85"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T22:48:33.489915Z",
     "start_time": "2025-11-29T22:48:33.486086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def run_test(model, loader):\n",
    "    y_true, y_pred = [], []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(loader, desc=\"Testing\"):\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "\n",
    "            logits = model(x).squeeze(1)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > 0.5).float()\n",
    "\n",
    "            y_true.extend(y.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    return y_true, y_pred"
   ],
   "id": "53dc146296950ac8",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T22:48:39.597545Z",
     "start_time": "2025-11-29T22:48:39.593560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    y_true = np.array(y_true).astype(int)\n",
    "    y_pred = np.array(y_pred).astype(int)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_pred)\n",
    "    except:\n",
    "        auc = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": auc,\n",
    "        \"confusion_matrix\": cm,\n",
    "    }\n"
   ],
   "id": "3726db88d30f262e",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T22:54:28.621666Z",
     "start_time": "2025-11-29T22:54:28.618401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "CLASS_NAMES = [\"no_drone\", \"drone\"]\n",
    "\n",
    "def plot_confusion(cm, title):\n",
    "    plt.figure(figsize=(4,4))\n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "        xticklabels=CLASS_NAMES,\n",
    "        yticklabels=CLASS_NAMES\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n"
   ],
   "id": "bcb60cbb51115143",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T22:54:38.559969Z",
     "start_time": "2025-11-29T22:54:38.555982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ResNet50Audio(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = models.resnet50(weights=None)   # No pretrained weights? → set pretrained=True if needed\n",
    "\n",
    "        # Modify first conv to accept 3-channel mel (shape [3,128,T])\n",
    "        self.backbone.conv1 = nn.Conv2d(\n",
    "            3, 64, kernel_size=7, stride=2, padding=3, bias=False\n",
    "        )\n",
    "\n",
    "        # Replace classifier head\n",
    "        in_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Linear(in_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n"
   ],
   "id": "c1dbc13aefd9ef81",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T22:54:48.130942Z",
     "start_time": "2025-11-29T22:54:47.690949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = ResNet50Audio().to(DEVICE)\n",
    "\n",
    "checkpoint_path = \"./models_resnet.pt\"\n",
    "\n",
    "state = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "model.load_state_dict(state)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"Loaded model from:\", checkpoint_path)\n"
   ],
   "id": "cc9ccac2ce8a729",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './models_resnet.pt'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[21]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      3\u001B[39m model = ResNet50Audio().to(DEVICE)\n\u001B[32m      5\u001B[39m checkpoint_path = \u001B[33m\"\u001B[39m\u001B[33m./models_resnet.pt\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m state = \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcheckpoint_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[43m=\u001B[49m\u001B[43mDEVICE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      8\u001B[39m model.load_state_dict(state)\n\u001B[32m     10\u001B[39m model.eval()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/serialization.py:1484\u001B[39m, in \u001B[36mload\u001B[39m\u001B[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[39m\n\u001B[32m   1481\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mencoding\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args.keys():\n\u001B[32m   1482\u001B[39m     pickle_load_args[\u001B[33m\"\u001B[39m\u001B[33mencoding\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[33m\"\u001B[39m\u001B[33mutf-8\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1484\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_file_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrb\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[32m   1485\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[32m   1486\u001B[39m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[32m   1487\u001B[39m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[32m   1488\u001B[39m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[32m   1489\u001B[39m         orig_position = opened_file.tell()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/serialization.py:759\u001B[39m, in \u001B[36m_open_file_like\u001B[39m\u001B[34m(name_or_buffer, mode)\u001B[39m\n\u001B[32m    757\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_open_file_like\u001B[39m(name_or_buffer: FileLike, mode: \u001B[38;5;28mstr\u001B[39m) -> _opener[IO[\u001B[38;5;28mbytes\u001B[39m]]:\n\u001B[32m    758\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[32m--> \u001B[39m\u001B[32m759\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    760\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    761\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mw\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/PST4/AI/.venv/lib/python3.13/site-packages/torch/serialization.py:740\u001B[39m, in \u001B[36m_open_file.__init__\u001B[39m\u001B[34m(self, name, mode)\u001B[39m\n\u001B[32m    739\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name: Union[\u001B[38;5;28mstr\u001B[39m, os.PathLike[\u001B[38;5;28mstr\u001B[39m]], mode: \u001B[38;5;28mstr\u001B[39m) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m740\u001B[39m     \u001B[38;5;28msuper\u001B[39m().\u001B[34m__init__\u001B[39m(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: './models_resnet.pt'"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f20ef80fcfdb3f16"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
