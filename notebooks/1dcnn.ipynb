{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-24T14:37:22.601626Z",
     "start_time": "2025-09-24T14:37:22.599276Z"
    }
   },
   "source": [
    "from datasets import load_from_disk, DatasetDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Accuracy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "\n",
    "\n",
    "SEED=42\n",
    "SAMPLE_RATE=16000\n",
    "BATCH_SIZE=16\n",
    "PIN_MEMORY=False\n",
    "NUM_WORKERS = 24\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T14:37:23.897681Z",
     "start_time": "2025-09-24T14:37:23.041777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ds = load_from_disk(\"../data/datasets/ds_462700\")\n",
    "LABELS = ds.features[\"label\"]\n",
    "\n",
    "# When the dataset is not a datrasetDict, we split manually\n",
    "if not isinstance(ds, DatasetDict):\n",
    "    ds = ds.train_test_split(test_size=0.3, seed=SEED)\n",
    "    test_and_valid = ds[\"test\"].train_test_split(test_size=0.5, seed=SEED)\n",
    "\n",
    "    ds = DatasetDict({\n",
    "        \"train\": ds[\"train\"],\n",
    "        \"valid\": test_and_valid[\"train\"],\n",
    "        \"test\": test_and_valid[\"test\"],\n",
    "    })\n",
    "\n",
    "\n",
    "ds[\"train\"].shape"
   ],
   "id": "579e08770bc5c4f2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(323890, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T14:37:24.596027Z",
     "start_time": "2025-09-24T14:37:24.593962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def collate_fn(batch):\n",
    "    audios = [torch.tensor(x[\"audio\"], dtype=torch.float32) for x in batch]\n",
    "    labels = torch.tensor([x[\"label\"] for x in batch], dtype=torch.long)\n",
    "\n",
    "    max_len = max(a.shape[0] for a in audios)\n",
    "    audios = torch.stack([F.pad(a, (0, max_len - a.shape[0])) for a in audios])\n",
    "    return audios, labels"
   ],
   "id": "4475a78c4d7eb1f2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T14:37:25.402002Z",
     "start_time": "2025-09-24T14:37:25.400762Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "de8ff4457ff413b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T14:37:26.051268Z",
     "start_time": "2025-09-24T14:37:26.048961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(dataset=ds[\"train\"], batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, pin_memory=PIN_MEMORY, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(dataset=ds[\"test\"], batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, pin_memory=PIN_MEMORY, num_workers=NUM_WORKERS)\n",
    "valid_loader = DataLoader(dataset=ds[\"valid\"], batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, pin_memory=PIN_MEMORY, num_workers=NUM_WORKERS)"
   ],
   "id": "76b7b3eb2cebbc21",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T14:37:26.673954Z",
     "start_time": "2025-09-24T14:37:26.669433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AudioCNN(nn.Module):\n",
    "    def __init__(self, n_classes: int = len(LABELS.names)):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=9, stride=2, padding=4),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 32, kernel_size=9, stride=2, padding=4),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 64, kernel_size=9, stride=2, padding=4),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "        )\n",
    "        self.fc = nn.Linear(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.net(x).squeeze(-1)\n",
    "\n",
    "        return self.fc(x)\n",
    "\n",
    "model = AudioCNN()"
   ],
   "id": "6b1973a998df5119",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T14:47:10.835390Z",
     "start_time": "2025-09-24T14:47:10.833040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "906c7859ac590f8",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T14:47:13.864204Z",
     "start_time": "2025-09-24T14:47:13.063762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    acc_metric.reset()\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [train]\", leave=False)\n",
    "    for x, y in pbar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "        acc_metric.update(out, y)\n",
    "\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    train_loss /= len(ds[\"train\"])\n",
    "    train_acc = acc_metric.compute()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    acc_metric.reset()\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(valid_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [valid]\", leave=False)\n",
    "        for x, y in pbar:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            val_loss += loss.item() * x.size(0)\n",
    "            acc_metric.update(out, y)\n",
    "\n",
    "    val_loss /= len(ds[\"valid\"])\n",
    "    val_acc = acc_metric.compute()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n"
   ],
   "id": "77426fa389a9ff68",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "If `preds` have one dimension more than `target`, `preds.shape[1]` should be equal to number of classes.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 20\u001B[39m\n\u001B[32m     17\u001B[39m     optimizer.step()\n\u001B[32m     19\u001B[39m     train_loss += loss.item() * x.size(\u001B[32m0\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m20\u001B[39m     \u001B[43macc_metric\u001B[49m\u001B[43m.\u001B[49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     22\u001B[39m     pbar.set_postfix({\u001B[33m\"\u001B[39m\u001B[33mloss\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss.item()\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m})\n\u001B[32m     24\u001B[39m train_loss /= \u001B[38;5;28mlen\u001B[39m(ds[\u001B[33m\"\u001B[39m\u001B[33mtrain\u001B[39m\u001B[33m\"\u001B[39m])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/lib/python3.13/site-packages/torchmetrics/metric.py:549\u001B[39m, in \u001B[36mMetric._wrap_update.<locals>.wrapped_func\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    547\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.set_grad_enabled(\u001B[38;5;28mself\u001B[39m._enable_grad):\n\u001B[32m    548\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m549\u001B[39m         \u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    550\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[32m    551\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mExpected all tensors to be on\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(err):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/lib/python3.13/site-packages/torchmetrics/classification/stat_scores.py:339\u001B[39m, in \u001B[36mMulticlassStatScores.update\u001B[39m\u001B[34m(self, preds, target)\u001B[39m\n\u001B[32m    337\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Update state with predictions and targets.\"\"\"\u001B[39;00m\n\u001B[32m    338\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.validate_args:\n\u001B[32m--> \u001B[39m\u001B[32m339\u001B[39m     \u001B[43m_multiclass_stat_scores_tensor_validation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    340\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpreds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mnum_classes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmultidim_average\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mignore_index\u001B[49m\n\u001B[32m    341\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    342\u001B[39m preds, target = _multiclass_stat_scores_format(preds, target, \u001B[38;5;28mself\u001B[39m.top_k)\n\u001B[32m    343\u001B[39m num_classes = \u001B[38;5;28mself\u001B[39m.num_classes \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.num_classes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[32m1\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/lib/python3.13/site-packages/torchmetrics/functional/classification/stat_scores.py:286\u001B[39m, in \u001B[36m_multiclass_stat_scores_tensor_validation\u001B[39m\u001B[34m(preds, target, num_classes, multidim_average, ignore_index)\u001B[39m\n\u001B[32m    284\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mIf `preds` have one dimension more than `target`, `preds` should be a float tensor.\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    285\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m num_classes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m preds.shape[\u001B[32m1\u001B[39m] != num_classes:\n\u001B[32m--> \u001B[39m\u001B[32m286\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    287\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mIf `preds` have one dimension more than `target`, `preds.shape[1]` should be\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    288\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m equal to number of classes.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    289\u001B[39m     )\n\u001B[32m    290\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m preds.shape[\u001B[32m2\u001B[39m:] != target.shape[\u001B[32m1\u001B[39m:]:\n\u001B[32m    291\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    292\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mIf `preds` have one dimension more than `target`, the shape of `preds` should be\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    293\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m (N, C, ...), and the shape of `target` should be (N, ...).\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    294\u001B[39m     )\n",
      "\u001B[31mValueError\u001B[39m: If `preds` have one dimension more than `target`, `preds.shape[1]` should be equal to number of classes."
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T12:17:53.850801Z",
     "start_time": "2025-09-24T12:10:12.188355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    acc_metric.reset()\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [train]\", leave=False)\n",
    "    for x, y in pbar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "        acc_metric.update(out, y)\n",
    "\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    train_loss /= len(ds[\"train\"])\n",
    "    train_acc = acc_metric.compute()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    acc_metric.reset()\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(valid_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [valid]\", leave=False)\n",
    "        for x, y in pbar:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            val_loss += loss.item() * x.size(0)\n",
    "            acc_metric.update(out, y)\n",
    "\n",
    "    val_loss /= len(ds[\"valid\"])\n",
    "    val_acc = acc_metric.compute()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n"
   ],
   "id": "bcbbaa6145c4e8e4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Train Loss: 0.1963, Train Acc: 0.9235, Val Loss: 0.1607, Val Acc: 0.9378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 | Train Loss: 0.1513, Train Acc: 0.9447, Val Loss: 0.2034, Val Acc: 0.9141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 | Train Loss: 0.1357, Train Acc: 0.9513, Val Loss: 0.1236, Val Acc: 0.9591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 | Train Loss: 0.1253, Train Acc: 0.9551, Val Loss: 0.1483, Val Acc: 0.9552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 | Train Loss: 0.1188, Train Acc: 0.9579, Val Loss: 0.1274, Val Acc: 0.9543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T12:28:48.531213Z",
     "start_time": "2025-09-24T12:28:33.863450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "acc_metric.reset()\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    pbar = tqdm(test_loader, desc=\"Testing\", leave=False)\n",
    "    for x, y in pbar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        test_loss += loss.item() * x.size(0)\n",
    "        acc_metric.update(out, y)\n",
    "\n",
    "test_loss /= len(ds[\"test\"])\n",
    "test_acc = acc_metric.compute()\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}\")"
   ],
   "id": "e421a5ee250afbc1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1263 | Test Acc: 0.9549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T12:32:00.950298Z",
     "start_time": "2025-09-24T12:32:00.903253Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), \"checkpoints/audio_cnn.pth\")",
   "id": "f3d4a4d4278658e9",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T14:19:15.498649Z",
     "start_time": "2025-09-24T14:19:15.493711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def predict_wav(path, model, labels=LABELS, sample_rate=SAMPLE_RATE, duration=0.5, device='cpu'):\n",
    "    waveform, sr = torchaudio.load(path)  # [channels, T]\n",
    "\n",
    "    # Convert to mono\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    #\n",
    "    # # Resample if needed\n",
    "    if sr != sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(sr, sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    # normalize\n",
    "    # waveform = (waveform - waveform.mean()) / (waveform.std() + 1e-6)\n",
    "\n",
    "    waveform = waveform.squeeze(0)  # now [T]\n",
    "    num_samples = int(sample_rate * duration)\n",
    "    # Truncate or pad\n",
    "    if waveform.shape[0] > num_samples:\n",
    "        waveform = waveform[:num_samples]\n",
    "    elif waveform.shape[0] < num_samples:\n",
    "        waveform = F.pad(waveform, (0, num_samples - waveform.shape[0]))\n",
    "\n",
    "    waveform = waveform.unsqueeze(0).to(device)\n",
    "\n",
    "    # Predict\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(waveform)\n",
    "        probs = torch.softmax(out, dim=1).cpu().numpy()[0]\n",
    "        pred_idx = out.argmax(dim=1).item()\n",
    "        pred_label = labels.names[pred_idx]\n",
    "\n",
    "    return pred_label, probs\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# predict_wav(\"/home/pierre/Downloads/B_S2_D1_092-bebop_000_.wav\", model, device=device)\n",
    "# label, probs = predict_wav(\"/home/pierre/Documents/Projects/PST4/AI/data/raw/test/drone/10.wav\", model, device=device)\n",
    "# print(\"Predicted label:\", label)\n",
    "# print(\"Probabilities:\", probs)\n",
    "\n",
    "# label, probs = predict_wav(\"/home/pierre/Downloads/B_S2_D1_092-bebop_000_.wav\", model, device=device)\n",
    "# label, probs = predict_wav(\"/home/pierre/Downloads/clean-trap-loop-131bpm-136738.mp3\", model, device=device)\n",
    "# label, probs = predict_wav(\"/home/pierre/Downloads/B_S2_D1_067-bebop_000_.wav\", model, device=device)\n",
    "# label, probs = predict_wav(\"/home/pierre/Downloads/WINGInsc_Abeilles solitaires ou mouches (ID 0101)_LS.wav\", model, device=device)\n",
    "# label, probs = predict_wav(\"/home/pierre/Downloads/audio.wav\", model, device=device)\n",
    "# label, probs = predict_wav(\"/home/pierre/Documents/Projects/PST4/AI/data/raw/test/drone/1.wav\", model, device=device)\n",
    "# label, probs = predict_wav(\"/home/pierre/Documents/Projects/PST4/AI/data/raw/test/drone/2.wav\", model, device=device)\n",
    "# label, probs = predict_wav(\"/home/pierre/Documents/Projects/PST4/AI/data/raw/test/drone/3.wav\", model, device=device)\n",
    "# label, probs = predict_wav(\"/home/pierre/Documents/Projects/PST4/AI/data/raw/archive(1)/DREGON_clean_recordings_whitenoise/DREGON_clean_recordings_whitenoise/60_-15_1.2.wav\", model, device=device)\n",
    "# label, probs = predict_wav(\"/home/pierre/Documents/Projects/PST4/AI/data/raw/archive(1)/DREGON_clean_recordings_whitenoise/DREGON_clean_recordings_whitenoise/75_-15_2.4.wav\", model, device=device)\n",
    "# label, probs = predict_wav(\"/home/pierre/Documents/Projects/PST4/AI/data/raw/archive(1)/DREGON_clean_recordings_speech/DREGON_clean_recordings_speech/45_0_1.2__3.wav\", model, device=device)\n",
    "# label, probs = predict_wav(\"/home/pierre/Documents/Projects/PST4/AI/data/raw/test/other/1.wav\", model, device=device)\n",
    "# label, probs = predict_wav(\"/home/pierre/Documents/Projects/PST4/AI/data/raw/hibou_dataset/drone/2997-2997.wav\", model, device=device)\n",
    "# label, probs = predict_wav(\"/home/pierre/Documents/Projects/PST4/AI/data/raw/hibou_dataset/drone/2997-2997.wav\", model, device=device)"
   ],
   "id": "d646bf79981eb037",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AudioCNN(\n",
       "  (net): Sequential(\n",
       "    (0): Conv1d(1, 16, kernel_size=(9,), stride=(2,), padding=(4,))\n",
       "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv1d(16, 32, kernel_size=(9,), stride=(2,), padding=(4,))\n",
       "    (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Conv1d(32, 64, kernel_size=(9,), stride=(2,), padding=(4,))\n",
       "    (7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "    (9): AdaptiveAvgPool1d(output_size=1)\n",
       "  )\n",
       "  (fc): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 126
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T14:19:18.274450Z",
     "start_time": "2025-09-24T14:19:18.164750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "drone_dir = \"/home/pierre/Documents/Projects/PST4/AI/data/raw/test/drone\"\n",
    "import os\n",
    "# List all .wav files in the directory\n",
    "wav_files = [f for f in os.listdir(drone_dir) if f.endswith('.wav')]\n",
    "\n",
    "# Predict for each file\n",
    "for wav_file in wav_files:\n",
    "    path = os.path.join(drone_dir, wav_file)\n",
    "    label, probs = predict_wav(path, model, device=device)\n",
    "    print(f\"File: {wav_file} {label}\")"
   ],
   "id": "115f7714d2953342",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 1_1.wav drone\n",
      "File: 1_2.wav drone\n",
      "File: 1_3.wav drone\n",
      "File: 0.wav other\n",
      "File: 1.wav other\n",
      "File: 2.wav other\n",
      "File: 3.wav other\n",
      "File: 4.wav other\n",
      "File: 5.wav other\n",
      "File: 6.wav other\n",
      "File: 7.wav other\n",
      "File: 8.wav other\n",
      "File: 9.wav other\n",
      "File: 10.wav other\n",
      "File: 11.wav other\n",
      "File: 12.wav other\n",
      "File: 13.wav other\n",
      "File: 14.wav other\n",
      "File: 15.wav other\n",
      "File: 16.wav other\n",
      "File: 17.wav other\n",
      "File: 18.wav other\n",
      "File: 19.wav other\n",
      "File: 20.wav other\n",
      "File: 21.wav other\n",
      "File: 22.wav other\n",
      "File: 23.wav other\n",
      "File: 24.wav other\n",
      "File: 25.wav other\n",
      "File: 26.wav other\n",
      "File: 27.wav other\n",
      "File: 28.wav other\n",
      "File: 29.wav other\n",
      "File: 30.wav other\n",
      "File: 31.wav other\n",
      "File: 32.wav other\n",
      "File: 33.wav other\n",
      "File: 34.wav other\n",
      "File: 35.wav drone\n",
      "File: 36.wav other\n",
      "File: 37.wav other\n",
      "File: 38.wav other\n",
      "File: 39.wav other\n",
      "File: 40.wav other\n",
      "File: 41.wav other\n",
      "File: 42.wav other\n",
      "File: 43.wav other\n",
      "File: 44.wav other\n",
      "File: 45.wav other\n",
      "File: 46.wav other\n",
      "File: 47.wav other\n",
      "File: 48.wav other\n",
      "File: 49.wav other\n",
      "File: 50.wav other\n",
      "File: 51.wav other\n",
      "File: 52.wav other\n",
      "File: 53.wav other\n",
      "File: 54.wav other\n",
      "File: 55.wav other\n",
      "File: 56.wav other\n",
      "File: 57.wav other\n",
      "File: 58.wav other\n",
      "File: 59.wav other\n",
      "File: 60.wav other\n",
      "File: 61.wav other\n",
      "File: 62.wav other\n",
      "File: 63.wav other\n",
      "File: 64.wav other\n",
      "File: 65.wav other\n",
      "File: 66.wav other\n",
      "File: 67.wav other\n",
      "File: 68.wav other\n",
      "File: 69.wav other\n",
      "File: 70.wav other\n",
      "File: 71.wav other\n",
      "File: 72.wav other\n",
      "File: 73.wav other\n",
      "File: 74.wav drone\n",
      "File: 75.wav other\n",
      "File: 76.wav other\n",
      "File: 77.wav other\n",
      "File: 78.wav other\n",
      "File: 79.wav other\n",
      "File: 80.wav other\n",
      "File: 81.wav other\n",
      "File: 82.wav other\n",
      "File: 83.wav other\n",
      "File: 84.wav other\n",
      "File: 85.wav other\n",
      "File: 86.wav other\n",
      "File: 87.wav other\n",
      "File: 88.wav other\n",
      "File: 89.wav other\n"
     ]
    }
   ],
   "execution_count": 127
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5b9d093703857743"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3ce1a7a088d8403a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
