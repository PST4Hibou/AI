{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-25T15:40:25.429579Z",
     "start_time": "2025-09-25T15:40:23.282840Z"
    }
   },
   "source": [
    "from datasets import load_from_disk, DatasetDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Accuracy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "\n",
    "\n",
    "SEED=42\n",
    "SAMPLE_RATE=16000\n",
    "BATCH_SIZE=16\n",
    "PIN_MEMORY=False\n",
    "NUM_WORKERS = 24\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pierre/.local/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T15:40:26.791501Z",
     "start_time": "2025-09-25T15:40:25.935736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ds = load_from_disk(\"../data/datasets/ds_462700\")\n",
    "LABELS = ds.features[\"label\"]\n",
    "\n",
    "# When the dataset is not a datrasetDict, we split manually\n",
    "if not isinstance(ds, DatasetDict):\n",
    "    ds = ds.train_test_split(test_size=0.3, seed=SEED)\n",
    "    test_and_valid = ds[\"test\"].train_test_split(test_size=0.5, seed=SEED)\n",
    "\n",
    "    ds = DatasetDict({\n",
    "        \"train\": ds[\"train\"],\n",
    "        \"valid\": test_and_valid[\"train\"],\n",
    "        \"test\": test_and_valid[\"test\"],\n",
    "    })\n",
    "\n",
    "\n",
    "ds[\"train\"].shape"
   ],
   "id": "579e08770bc5c4f2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(323890, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T15:40:28.087612Z",
     "start_time": "2025-09-25T15:40:28.085481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def collate_fn(batch):\n",
    "    audios = [torch.tensor(x[\"audio\"], dtype=torch.float32) for x in batch]\n",
    "    labels = torch.tensor([x[\"label\"] for x in batch], dtype=torch.long)\n",
    "\n",
    "    max_len = max(a.shape[0] for a in audios)\n",
    "    audios = torch.stack([F.pad(a, (0, max_len - a.shape[0])) for a in audios])\n",
    "    return audios, labels"
   ],
   "id": "4475a78c4d7eb1f2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:10:34.653808Z",
     "start_time": "2025-09-24T15:10:34.652541Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "de8ff4457ff413b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T15:40:29.917065Z",
     "start_time": "2025-09-25T15:40:29.914659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(dataset=ds[\"train\"], batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, pin_memory=PIN_MEMORY, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(dataset=ds[\"test\"], batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, pin_memory=PIN_MEMORY, num_workers=NUM_WORKERS)\n",
    "valid_loader = DataLoader(dataset=ds[\"valid\"], batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, pin_memory=PIN_MEMORY, num_workers=NUM_WORKERS)"
   ],
   "id": "76b7b3eb2cebbc21",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T09:14:46.301397Z",
     "start_time": "2025-09-25T09:14:46.297241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AudioCNN(nn.Module):\n",
    "    def __init__(self, n_classes: int = len(LABELS.names)):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=9, stride=2, padding=4),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 32, kernel_size=9, stride=2, padding=4),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 64, kernel_size=9, stride=2, padding=4),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "        )\n",
    "        # PRELU, sigmoid, reduire nb couche\n",
    "        self.fc = nn.Linear(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.net(x).squeeze(-1)\n",
    "\n",
    "        return self.fc(x)\n",
    "\n",
    "model = AudioCNN()"
   ],
   "id": "6b1973a998df5119",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T15:47:37.657442Z",
     "start_time": "2025-09-25T15:47:37.654104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AudioCNN(nn.Module):\n",
    "    def __init__(self, n_classes: int = len(LABELS.names)):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=9, stride=2, padding=4),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.PReLU(),\n",
    "        )\n",
    "        self.fc = nn.Linear(16, n_classes)  # only 16 channels left after pooling\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)          # [B, 1, T]\n",
    "        x = self.net(x)             # [B, 16, T/2]\n",
    "        x = x.mean(dim=-1)          # [B, 16]   (global average pooling)\n",
    "        return self.fc(x)           # [B, n_classes]\n",
    "\n",
    "\n",
    "model = AudioCNN()"
   ],
   "id": "1af815934d0e67e4",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T15:47:39.795701Z",
     "start_time": "2025-09-25T15:47:39.792845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# adamW,\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)"
   ],
   "id": "906c7859ac590f8",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T15:55:29.895848Z",
     "start_time": "2025-09-25T15:47:41.363451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "acc_metric = Accuracy(task='multiclass', num_classes=len(LABELS.names)).to(device)\n",
    "\n",
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    acc_metric.reset()\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [train]\", leave=False)\n",
    "    for x, y in pbar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "        acc_metric.update(out, y)\n",
    "\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    train_loss /= len(ds[\"train\"])\n",
    "    train_acc = acc_metric.compute()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    acc_metric.reset()\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(valid_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [valid]\", leave=False)\n",
    "        for x, y in pbar:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            val_loss += loss.item() * x.size(0)\n",
    "            acc_metric.update(out, y)\n",
    "\n",
    "    val_loss /= len(ds[\"valid\"])\n",
    "    val_acc = acc_metric.compute()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n"
   ],
   "id": "bcbbaa6145c4e8e4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Train Loss: 0.3207, Train Acc: 0.8825, Val Loss: 0.2947, Val Acc: 0.8892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 | Train Loss: 0.2879, Train Acc: 0.8899, Val Loss: 0.2692, Val Acc: 0.8977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 | Train Loss: 0.2757, Train Acc: 0.8938, Val Loss: 0.2730, Val Acc: 0.8929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 | Train Loss: 0.2737, Train Acc: 0.8948, Val Loss: 0.2713, Val Acc: 0.8989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 | Train Loss: 0.2725, Train Acc: 0.8950, Val Loss: 0.2582, Val Acc: 0.9043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T16:04:20.475437Z",
     "start_time": "2025-09-25T16:04:06.177186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "acc_metric.reset()\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    pbar = tqdm(test_loader, desc=\"Testing\", leave=False)\n",
    "    for x, y in pbar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        test_loss += loss.item() * x.size(0)\n",
    "        acc_metric.update(out, y)\n",
    "\n",
    "test_loss /= len(ds[\"test\"])\n",
    "test_acc = acc_metric.compute()\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}\")"
   ],
   "id": "e421a5ee250afbc1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2550 | Test Acc: 0.9061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T12:32:00.950298Z",
     "start_time": "2025-09-24T12:32:00.903253Z"
    }
   },
   "cell_type": "code",
   "source": "# torch.save(model.state_dict(), \"checkpoints/audio_cnn.pth\")",
   "id": "f3d4a4d4278658e9",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T16:04:51.412436Z",
     "start_time": "2025-09-25T16:04:51.243856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def predict_wav(path, model, labels=LABELS, sample_rate=SAMPLE_RATE, duration=0.5, device='cpu'):\n",
    "    waveform, sr = torchaudio.load(path)  # [channels, T]\n",
    "\n",
    "    # Convert to mono\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    #\n",
    "    # # Resample if needed\n",
    "    if sr != sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(sr, sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    # normalize\n",
    "    # waveform = (waveform - waveform.mean()) / (waveform.std() + 1e-6)\n",
    "\n",
    "    waveform = waveform.squeeze(0)  # now [T]\n",
    "    num_samples = int(sample_rate * duration)\n",
    "    # Truncate or pad\n",
    "    if waveform.shape[0] > num_samples:\n",
    "        waveform = waveform[:num_samples]\n",
    "    elif waveform.shape[0] < num_samples:\n",
    "        waveform = F.pad(waveform, (0, num_samples - waveform.shape[0]))\n",
    "\n",
    "    waveform = waveform.unsqueeze(0).to(device)\n",
    "\n",
    "    # Predict\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(waveform)\n",
    "        probs = torch.softmax(out, dim=1).cpu().numpy()[0]\n",
    "        pred_idx = out.argmax(dim=1).item()\n",
    "        pred_label = labels.names[pred_idx]\n",
    "\n",
    "    return pred_label, probs\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "# predict_wav(\"/home/pierre/Downloads/B_S2_D1_092-bebop_000_.wav\", model, device=device)\n",
    "# label, probs = predict_wav(\"/home/pierre/Documents/Projects/PST4/AI/data/raw/test/drone/10.wav\", model, device=device)\n",
    "# print(\"Predicted label:\", label)\n",
    "# print(\"Probabilities:\", probs)\n",
    "\n",
    "# label, probs = predict_wav(\"/home/pierre/Downloads/audio.wav\", model, device=device)\n",
    "# label, probs = predict_wav(\"/home/pierre/Documents/Projects/PST4/AI/data/raw/test/drone/1.wav\", model, device=device)\n",
    "# label, probs = predict_wav(\"/home/pierre/Documents/Projects/PST4/AI/data/raw/test/drone/2.wav\", model, device=device)\n",
    "# label, probs = predict_wav(\"/home/pierre/Documents/Projects/PST4/AI/data/raw/test/drone/3.wav\", model, device=device)\n",
    "# label, probs = predict_wav(\"/home/pierre/Documents/Projects/PST4/AI/data/raw/archive(1)/DREGON_clean_recordings_whitenoise/DREGON_clean_recordings_whitenoise/60_-15_1.2.wav\", model, device=device)\n",
    "# label, probs = predict_wav(\"/home/pierre/Documents/Projects/PST4/AI/data/raw/archive(1)/DREGON_clean_recordings_whitenoise/DREGON_clean_recordings_whitenoise/75_-15_2.4.wav\", model, device=device)\n",
    "# label, probs = predict_wav(\"/home/pierre/Documents/Projects/PST4/AI/data/raw/archive(1)/DREGON_clean_recordings_speech/DREGON_clean_recordings_speech/45_0_1.2__3.wav\", model, device=device)\n",
    "# label, probs = predict_wav(\"/home/pierre/Documents/Projects/PST4/AI/data/raw/test/other/1.wav\", model, device=device)\n",
    "# label, probs = predict_wav(\"/home/pierre/Documents/Projects/PST4/AI/data/raw/hibou_dataset/drone/2997-2997.wav\", model, device=device)\n",
    "# label, probs = predict_wav(\"/home/pierre/Documents/Projects/PST4/AI/data/raw/hibou_dataset/drone/2997-2997.wav\", model, device=device)\n",
    "\n",
    "\n",
    "recordings = [\n",
    "    (\"/home/pierre/Downloads/B_S2_D1_092-bebop_000_.wav\", \"drone\"),\n",
    "    (\"/home/pierre/Downloads/B_S2_D1_067-bebop_000_.wav\", \"drone\"),\n",
    "    (\"/home/pierre/Documents/Projects/PST4/AI/data/raw/test/drone/1.wav\", \"drone\"),\n",
    "    (\"/home/pierre/Documents/Projects/PST4/AI/data/raw/test/drone/2.wav\", \"drone\"),\n",
    "    (\"/home/pierre/Documents/Projects/PST4/AI/data/raw/test/drone/3.wav\", \"drone\"),\n",
    "    (\"/home/pierre/Downloads/audio.wav\", \"drone\"),\n",
    "    (\"/home/pierre/Documents/Projects/PST4/AI/data/raw/archive(1)/DREGON_clean_recordings_whitenoise/DREGON_clean_recordings_whitenoise/60_-15_1.2.wav\", \"other\"),\n",
    "    (\"/home/pierre/Documents/Projects/PST4/AI/data/raw/archive(1)/DREGON_clean_recordings_whitenoise/DREGON_clean_recordings_whitenoise/75_-15_2.4.wav\", \"other\"),\n",
    "    (\"/home/pierre/Documents/Projects/PST4/AI/data/raw/archive(1)/DREGON_clean_recordings_speech/DREGON_clean_recordings_speech/45_0_1.2__3.wav\", \"other\"),\n",
    "    (\"/home/pierre/Documents/Projects/PST4/AI/data/raw/test/other/1.wav\", \"other\"),\n",
    "    (\"/home/pierre/Documents/Projects/PST4/AI/data/raw/hibou_dataset/drone/2997-2997.wav\", \"drone\"),\n",
    "]\n",
    "\n",
    "for path, label in recordings:\n",
    "    pred_label, probs = predict_wav(path, model, device=device)\n",
    "    print(f\"BON={label == pred_label} - Expected Label: {label}, Predicted Label: {pred_label}, Probabilities: {probs}\")\n",
    "\n"
   ],
   "id": "d646bf79981eb037",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BON=True - Expected Label: drone, Predicted Label: drone, Probabilities: [0.17857537 0.82142466]\n",
      "BON=True - Expected Label: drone, Predicted Label: drone, Probabilities: [0.14354423 0.8564558 ]\n",
      "BON=False - Expected Label: drone, Predicted Label: other, Probabilities: [0.90347457 0.09652542]\n",
      "BON=False - Expected Label: drone, Predicted Label: other, Probabilities: [0.8523274  0.14767258]\n",
      "BON=False - Expected Label: drone, Predicted Label: other, Probabilities: [0.8732327  0.12676731]\n",
      "BON=True - Expected Label: drone, Predicted Label: drone, Probabilities: [0.00333356 0.9966665 ]\n",
      "BON=True - Expected Label: other, Predicted Label: other, Probabilities: [0.8701127  0.12988731]\n",
      "BON=True - Expected Label: other, Predicted Label: other, Probabilities: [0.8603066  0.13969342]\n",
      "BON=True - Expected Label: other, Predicted Label: other, Probabilities: [0.8845821  0.11541785]\n",
      "BON=True - Expected Label: other, Predicted Label: other, Probabilities: [0.9128914  0.08710863]\n",
      "BON=False - Expected Label: drone, Predicted Label: other, Probabilities: [0.72830534 0.2716947 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pierre/.local/lib/python3.13/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T14:42:54.806747Z",
     "start_time": "2025-09-25T14:42:54.628381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "drone_dir = \"/home/pierre/Documents/Projects/PST4/AI/data/raw/test/drone\"\n",
    "import os\n",
    "# List all .wav files in the directory\n",
    "wav_files = [f for f in os.listdir(drone_dir) if f.endswith('.wav')]\n",
    "\n",
    "# Predict for each file\n",
    "for wav_file in wav_files:\n",
    "    path = os.path.join(drone_dir, wav_file)\n",
    "    label, probs = predict_wav(path, model, device=device)\n",
    "    print(f\"File: {wav_file} {label}\")"
   ],
   "id": "115f7714d2953342",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 1_1.wav drone\n",
      "File: 1_2.wav drone\n",
      "File: 1_3.wav drone\n",
      "File: 0.wav other\n",
      "File: 1.wav other\n",
      "File: 2.wav other\n",
      "File: 3.wav other\n",
      "File: 4.wav other\n",
      "File: 5.wav other\n",
      "File: 6.wav other\n",
      "File: 7.wav other\n",
      "File: 8.wav other\n",
      "File: 9.wav other\n",
      "File: 10.wav other\n",
      "File: 11.wav other\n",
      "File: 12.wav other\n",
      "File: 13.wav other\n",
      "File: 14.wav other\n",
      "File: 15.wav other\n",
      "File: 16.wav other\n",
      "File: 17.wav other\n",
      "File: 18.wav other\n",
      "File: 19.wav other\n",
      "File: 20.wav other\n",
      "File: 21.wav other\n",
      "File: 22.wav other\n",
      "File: 23.wav other\n",
      "File: 24.wav other\n",
      "File: 25.wav other\n",
      "File: 26.wav other\n",
      "File: 27.wav other\n",
      "File: 28.wav other\n",
      "File: 29.wav other\n",
      "File: 30.wav other\n",
      "File: 31.wav other\n",
      "File: 32.wav other\n",
      "File: 33.wav other\n",
      "File: 34.wav other\n",
      "File: 35.wav drone\n",
      "File: 36.wav other\n",
      "File: 37.wav other\n",
      "File: 38.wav other\n",
      "File: 39.wav other\n",
      "File: 40.wav other\n",
      "File: 41.wav other\n",
      "File: 42.wav other\n",
      "File: 43.wav other\n",
      "File: 44.wav other\n",
      "File: 45.wav other\n",
      "File: 46.wav other\n",
      "File: 47.wav other\n",
      "File: 48.wav other\n",
      "File: 49.wav other\n",
      "File: 50.wav other\n",
      "File: 51.wav other\n",
      "File: 52.wav other\n",
      "File: 53.wav other\n",
      "File: 54.wav drone\n",
      "File: 55.wav other\n",
      "File: 56.wav other\n",
      "File: 57.wav other\n",
      "File: 58.wav other\n",
      "File: 59.wav other\n",
      "File: 60.wav other\n",
      "File: 61.wav other\n",
      "File: 62.wav other\n",
      "File: 63.wav other\n",
      "File: 64.wav other\n",
      "File: 65.wav other\n",
      "File: 66.wav other\n",
      "File: 67.wav other\n",
      "File: 68.wav other\n",
      "File: 69.wav other\n",
      "File: 70.wav other\n",
      "File: 71.wav other\n",
      "File: 72.wav other\n",
      "File: 73.wav other\n",
      "File: 74.wav drone\n",
      "File: 75.wav other\n",
      "File: 76.wav other\n",
      "File: 77.wav other\n",
      "File: 78.wav other\n",
      "File: 79.wav other\n",
      "File: 80.wav other\n",
      "File: 81.wav other\n",
      "File: 82.wav other\n",
      "File: 83.wav other\n",
      "File: 84.wav other\n",
      "File: 85.wav other\n",
      "File: 86.wav other\n",
      "File: 87.wav other\n",
      "File: 88.wav other\n",
      "File: 89.wav other\n",
      "File: 90.wav other\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:17:47.176586Z",
     "start_time": "2025-09-24T15:17:47.136902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = AudioCNN()\n",
    "model.load_state_dict(torch.load(\"checkpoints/audio_cnn.pth\"))"
   ],
   "id": "5b9d093703857743",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:18:17.021397Z",
     "start_time": "2025-09-24T15:18:17.005491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ],
   "id": "3ce1a7a088d8403a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AudioCNN(\n",
       "  (net): Sequential(\n",
       "    (0): Conv1d(1, 16, kernel_size=(9,), stride=(2,), padding=(4,))\n",
       "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv1d(16, 32, kernel_size=(9,), stride=(2,), padding=(4,))\n",
       "    (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Conv1d(32, 64, kernel_size=(9,), stride=(2,), padding=(4,))\n",
       "    (7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "    (9): AdaptiveAvgPool1d(output_size=1)\n",
       "  )\n",
       "  (fc): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:19:44.654461Z",
     "start_time": "2025-09-24T15:19:44.648479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "label, probs = predict_wav(\"/home/pierre/Documents/Projects/PST4/AI/data/raw/test/drone/1.wav\", model, device=device)\n",
    "print(f\"File: {label}\")"
   ],
   "id": "7ec49a968d9b7e6f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: other\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bfff751d6ac2573c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
